[{"id": "chatcmpl-BUneYerJdmGTQdqZ0cucWyaY2eGhs", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "utils.py  \u2013  Logic Analysis and Intended Functionality\n======================================================\n\nOverview  \n--------\nThis helper module centralises generic, non-model specific utilities that are required across virtually every other component in the SEQA reproduction code-base.  It contains only *stateless* functions so that importing it never has side-effects beyond (optional) logging configuration and deterministic seeding.  All behaviour must comply with the Design\u2019s \u201cData structures and interfaces\u201d section and must *not* introduce public interfaces that are absent from the diagram.\n\nThe four public helpers foreseen by the Design are:\n\n1. seed_everything(seed:int\u2502None)  \n2. flatten_dict(nested:dict, parent_key:str = \"\", sep:str = \"/\") \u2192 dict  \n3. length_limit(tokenizer, text:str, max_tokens:int) \u2192 str  \n4. setup_logging(cfg:Config, name:str\u2502None = None) \u2192 logging.Logger  \n\nOnly these should be imported by the rest of the code.  Internal/private helpers can be declared with a leading underscore, but must not be relied upon by other modules directly.\n\n--------------------------------------------------------------------\n1. seed_everything\n--------------------------------------------------------------------\nPurpose  \n\u2022 Ensure that results are reproducible across runs and machines.  \n\u2022 Covers Python\u2019s `random`, NumPy, and PyTorch PRNGs, and also sets `PYTHONHASHSEED` so that dictionary iteration order stays deterministic.  \n\u2022 Must respect `experiment.random_seed` field from `config.yaml`.  \n\u2022 Called once, **very early** in `main.py` **before** any heavy-weight library (HF transformers, datasets) initialises GPU/cuda contexts, because some of those libs read global seeds at import time.\n\nImplementation logic  \na. If `seed is None`, attempt to read from `Config.random_seed`; otherwise raise `ValueError` \u2013 the project never runs with an unspecified seed.  \nb. Convert to `int` and do boundary checking (0 \u2264 seed \u2264 2\u00b3\u00b2-1).  \nc. Set:  \n\n    \u2022 `os.environ[\"PYTHONHASHSEED\"]`  \n    \u2022 `random.seed(seed)`  \n    \u2022 `np.random.seed(seed)`  \n    \u2022 `torch.manual_seed(seed)`  \n    \u2022 `torch.cuda.manual_seed_all(seed)`  (guard on CUDA availability)\n\nd. Enable full determinism for CUDA kernels:  \n   `torch.backends.cudnn.deterministic = True`,  \n   `torch.backends.cudnn.benchmark     = False`  \n   (important because tokenisation + embedding are GPU heavy).  \ne. Return the seed so callers can log it.\n\nEdge cases  \n\u2022 If torch is compiled without CUDA, the cuda calls should be skipped.  \n\u2022 No heavy imports (e.g., transformers) must be triggered inside this function; they come later in the stack.\n\n--------------------------------------------------------------------\n2. flatten_dict\n--------------------------------------------------------------------\nPurpose  \nNested dictionaries coming from metrics (`evaluation.py`) or hyper-parameters (`Config.as_dict()`) must be serialised for JSON or CSV logging.  `flatten_dict` converts:\n\n```\n{\n  \"generator\": { \"num_beams\": 4 },\n  \"bayesian_filter\": { \"mu0\": 0.5, \"beta\": 1.0 }\n}\n```\ninto\n```\n{\n  \"generator/num_beams\": 4,\n  \"bayesian_filter/mu0\": 0.5,\n  \"bayesian_filter/beta\": 1.0\n}\n```\n\nImplementation logic  \n\u2022 Depth-first traversal (recursive) while concatenating keys with a separator (default `/`).  \n\u2022 Leaves values that are themselves dicts or list of dicts? \u2013 only flatten dicts; lists/tuples keep their position index, e.g., `\"metrics/0\": \"BLEU\"`.  \n\u2022 Non-serialisable objects (e.g., torch tensors) should be converted to `str()` so that logging or YAML dumping never crashes.\n\nComplexity: O(total_number_of_items) time and memory.\n\n--------------------------------------------------------------------\n3. length_limit\n--------------------------------------------------------------------\nPurpose  \nGiven an assembled prompt string, ensure it does **not** exceed the encoder token budget specified in `config.generator.encoder_max_tokens` (512 or 1024).  This is critical because BART\u2019s tokenizer will otherwise truncate or error out, breaking positional alignment and reproducibility.\n\nSignature  \n`length_limit(tokenizer, text, max_tokens) -> str`\n\nArguments  \n\u2022 `tokenizer`  \u2013 a Hugging-Face tokenizer that provides a fast `__call__` returning `input_ids` length without constructing tensors (`with_padding=False, add_special_tokens=False`).  \n\u2022 `text`       \u2013 plain string (unicode).  \n\u2022 `max_tokens` \u2013 integer budget.\n\nImplementation logic  \n1. Tokenise `text`; if `len(ids) \u2264 max_tokens`, return original `text`.  \n2. If overflow:  \n   a. Compute a *safe* truncation index.  Because prompt is sequence of accepted chunks (already separated by newline), simplest is to iteratively drop **whole chunks from the end** until the token count fits.  But `length_limit` should stay generic, so default behaviour is to cut at whitespace boundary:  \n      \u2022 While len(ids) > budget: remove last *N* characters, where *N* is proportional (~500 chars) to accelerate convergence, then re-tokenise.  \n      \u2022 Alternative strategy: binary search on character index.  \n   b. Guarantee returned text is non-empty; if inevitably empty, raise `RuntimeError` (should never happen given sensible chunk acceptance rates).  \n3. Return truncated string.\n\nNote: Actual chunk pruning logic (e.g., dropping lowest novelty chunks) lives in `PromptBuilder`.  `length_limit` merely enforces the hard cap.\n\n--------------------------------------------------------------------\n4. setup_logging\n--------------------------------------------------------------------\nPurpose  \nUniform logging across all modules using the standard library\u2019s `logging` module:\n\n\u2022 Root level defined once so that *all* subsequent `logging.getLogger(__name__)` calls propagate to the same handlers.  \n\u2022 Log file path configurable: `logs/seqa_{timestamp}.log` by default.  \n\u2022 Console handler with colour is **not** permitted because `colorlog` or `rich` are *not* in requirements.  Stick to simple formatter.  \n\u2022 Log level:  \n  \u2013 DEBUG if `Config.debug` exists and is `True`;  \n  \u2013 else INFO.\n\nImplementation logic  \na. Only create handlers if not already configured (`logging.getLogger().handlers`).  \nb. Format string: `'%(asctime)s | %(levelname)-8s | %(name)s | %(message)s'` (UTC time to avoid TZ drift).  \nc. File handler writes same format; make sure directory exists (`mkdir -p logs`).  \nd. Return a module-level logger for convenience.\n\nUsage pattern  \n`logger = utils.setup_logging(cfg, __name__)` inside every module.\n\n--------------------------------------------------------------------\nInternal / Private helpers\n--------------------------------------------------------------------\n\u2022 `_ensure_dir(path:str)` \u2013 mkdir on demand; used by setup_logging.  \n\u2022 `_is_jsonable(obj)` \u2013 validate values before flattening (optional).\n\n--------------------------------------------------------------------\nInteraction with Config Singleton\n--------------------------------------------------------------------\n`Config` is loaded inside `main.py` and imported in downstream modules; however, utilities should *not* import Config at module import time (circular dependency!).  Instead, functions take explicit arguments (`cfg:Config` or scalar params).  `setup_logging` expects a *fully initialised* Config, so `main.py` must call it **after** Config.load().\n\n--------------------------------------------------------------------\nEdge-case Guarantees\n--------------------------------------------------------------------\n\u2022 All utilities must run on *both* Python 3.9 and 3.12 (per requirements).  \n\u2022 No heavy dependencies; only `os`, `sys`, `random`, `logging`, `pathlib`, `typing`, `time`, `datetime`, plus optional `torch` and `numpy` for seeding.  \n\u2022 Functions avoid mutable default arguments; no global state except the logger and PRNGs modified by seed_everything.  \n\u2022 Thread safety: logging already serialises writes; other utils are stateless.  \n\u2022 `length_limit` must not allocate GPU memory.\n\n--------------------------------------------------------------------\nUnit Testing Checklist\n--------------------------------------------------------------------\n1. seed_everything  \n   \u2022 Call with seed=123, then generate torch/np/random numbers; repeat \u2192 identical sequences.  \n   \u2022 Validate `os.environ[\"PYTHONHASHSEED\"]` == \"123\".\n\n2. flatten_dict  \n   \u2022 Deep nested dict (depth 3) flattened keys count matches original leaves count.  \n   \u2022 Separator change works.\n\n3. length_limit  \n   \u2022 Prompt that is under, equal to, and over the budget produce expected results.  \n   \u2022 Returned string length (in tokens) \u2264 `max_tokens`.  \n   \u2022 Stress test with 10 000-token string to ensure O(N log N) or better behaviour.\n\n4. setup_logging  \n   \u2022 Subsequent calls do not duplicate handlers.  \n   \u2022 Log file is created and receives entries.\n\n--------------------------------------------------------------------\nResulting Public API in utils.py\n--------------------------------------------------------------------\n```\nseed_everything(seed: int) -> int\nflatten_dict(d: dict, parent_key: str = '', sep: str = '/') -> dict\nlength_limit(tokenizer: PreTrainedTokenizerBase, text: str, max_tokens: int) -> str\nsetup_logging(cfg: Config, name: str | None = None) -> logging.Logger\n```\n\nNo other top-level public symbols are to be exported; any additional helper must be prefixed with an underscore.\n\n--------------------------------------------------------------------\nAlignment with Paper & Config\n--------------------------------------------------------------------\n\u2022 The seed value comes directly from `config.yaml` (`experiment.random_seed`).  \n\u2022 Token budget (`max_tokens`) must honour `config.generator.encoder_max_tokens`; do *not* hard-code 512/1024.  \n\u2022 Logging of seed and other hyper-parameters will utilise `flatten_dict` so experiments can be traced, a key requirement for scientific reproducibility emphasised by the paper.  \n\nThis logic analysis ensures that when we later implement `utils.py`, every other module (chunker, bayesian_filter, ... ) gets deterministic behaviour, robust logging, and convenient helper functionality\u2014without deviating from either the Design diagram or the configuration file.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680406, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 2770, "prompt_tokens": 15786, "total_tokens": 18556, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 640, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 10624}}}]