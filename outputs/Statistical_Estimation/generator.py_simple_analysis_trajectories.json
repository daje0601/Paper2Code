[{"role": "system", "content": "You are an expert researcher, strategic analyzer and software engineer with a deep understanding of experimental design and reproducibility in scientific research.\nYou will receive a research paper in JSON format, an overview of the plan, a design in JSON format consisting of \"Implementation approach\", \"File list\", \"Data structures and interfaces\", and \"Program call flow\", followed by a task in JSON format that includes \"Required packages\", \"Required other language third-party packages\", \"Logic Analysis\", and \"Task list\", along with a configuration file named \"config.yaml\". \n\nYour task is to conduct a comprehensive logic analysis to accurately reproduce the experiments and methodologies described in the research paper. \nThis analysis must align precisely with the paper\u2019s methodology, experimental setup, and evaluation criteria.\n\n1. Align with the Paper: Your analysis must strictly follow the methods, datasets, model configurations, hyperparameters, and experimental setups described in the paper.\n2. Be Clear and Structured: Present your analysis in a logical, well-organized, and actionable format that is easy to follow and implement.\n3. Prioritize Efficiency: Optimize the analysis for clarity and practical implementation while ensuring fidelity to the original experiments.\n4. Follow design: YOU MUST FOLLOW \"Data structures and interfaces\". DONT CHANGE ANY DESIGN. Do not use public member functions that do not exist in your design.\n5. REFER TO CONFIGURATION: Always reference settings from the config.yaml file. Do not invent or assume any values\u2014only use configurations explicitly provided.\n     \n"}, {"role": "user", "content": "## Paper\n{'page_1': \"Statistical Estimation for Large-Scale Question Answering:\\nInferring Comprehensive Context from Partial Chunks\\nJinbeom Kang\\n* 1\\nAbstract\\nLarge Language Models (LLMs) have shown re-\\nmarkable performance on Question Answering\\n(QA) but are fundamentally limited by their \\x02-\\nnite context windows, which can be exceeded\\nby extensive documents such as multi-page con-\\ntracts or long Wikipedia articles. We propose a\\nstatistical estimation framework\\ndesigned to\\nhandle large-scale QA by selectively aggregating\\nonly the most informative chunks of text. Our\\napproach integrates three key components:\\nAdap-\\ntive Chunk Sizing\\nto dynamically segment docu-\\nments based on local content density,\\nDynamic\\nThresholding\\nthat employs Bayesian updates to\\n\\x02lter redundant chunks, and a\\nCLT-based Statis-\\ntical Context Estimation\\nmechanism to approxi-\\nmate the entire document's embedding. Unlike\\ntraditional extractive metrics (EM/F1), we rely\\non\\ngenerative\\nQA metrics (BLEU, ROUGE, Per-\\nplexity) to capture both correctness and \\x03uency\\nin open-ended responses. Experimental results on\\nbenchmarks such as HotpotQA, TriviaQA, and\\nNatural Questions demonstrate that our method\\nnot only reduces computational overhead but also\\nimproves generative QA quality over baselines\\n(e.g., FiD, RAG). This highlights how a princi-\\npled, yet \\x03exible, statistical perspective can ef-\\nfectively overcome context-window constraints in\\nmodern LLMs.\\n1. Introduction\\nRecent advances in Large Language Models (LLMs) such\\nas GPT-4 have propelled open-domain question answering\\n(QA) systems to new performance levels, surpassing human-\\ncrafted baselines in a variety of tasks including summariza-\\n*\\nEqual contribution\\n1\\nDepartment of XXX, University of\\nYYY, Location, Country. Correspondence to: Jinbeom Kang\\n<\\njbkang@infobank.net\\n>\\n.\\nProceedings of the\\n41\\nst\\nInternational Conference on Machine\\nLearning\\n, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\\nby the author(s).\\ntion, machine translation, and multi-turn dialogue. Despite\\nthis remarkable progress, these models continue to be con-\\nstrained by \\x02nite context windows, often ranging from a few\\nthousand to tens of thousands of tokens. While extended\\ncontext capabilities (e.g., 32k tokens) provide partial relief,\\nreal-world settings such as multi-chapter textbooks, lengthy\\nlegal contracts, and intricate medical records can easily ex-\\nceed these limits, leaving the model blind to signi\\x02cant\\nsegments of text and consequently producing incomplete or\\nsuboptimal answers.\\nA common solution is to break the source text into smaller\\nchunks\\nthat individually \\x02t into the model's context window.\\nHowever, naive chunking strategies introduce two critical\\nissues. First, important details can be dispersed across mul-\\ntiple chunks, a phenomeno n we refer to as\\ncontext frag-\\nmentation\\n. This fragmentation undermines the coherence\\nnecessary for complex reasoning, as it forces the model to\\npiece together widely separated information. Second, the\\noverlapping chunks necessary to preserve cross-boundary\\ncontext often lead to\\nredundancy\\n, bloating both the com-\\nputational cost and the effective input size presented to the\\nmodel. Such redundancy can dilute key information, ulti-\\nmately harming the system's overall performance.\\nRetrieval-Augmented Generation (RAG) represents a no-\\ntable attempt to alleviate these constraints by fetching\\nrelevant passages from a large corpus, then fusing them\\nwithin a generative transformer. Although RAG and simi-\\nlar frameworks (e.g., DPR, Contriever) excel at retrieving\\ntopically relevant content, they frequently suffer from an\\nover-retrieval of near-duplicate passages, especially for long\\nor repetitive source documents. This can induce a form of\\nredundancy similar to naive chunking, wherein the model's\\neffective input length grows excessively, and the generative\\nmodel struggles to identify the most salient facts. Moreover,\\nthe retrieval step often relies on static or heuristically tuned\\nthresholds that do not dynamically adapt to the inherent\\noverlap or novelty among candidate passages.\\nIn this paper, we propose a statistical estimation framework\\nthat addresses both the chunking problem and certain limi-\\ntations of retrieval-augmented QA pipelines. Our approach\\nleverages an\\nonline Bayesian thresholding\\nmechanism to\\n\\x02lter out redundant or near-duplicate text segments. In do-\\n1\\n\", 'page_2': 'Submission and Formatting Instructions for ICML 2025\\ning so, it provides a succinct yet suf\\x02ciently comprehensive\\ncontext for a generative language model, such as BART\\nor T5. Speci\\x02cally, we model the\\nnovelty\\nof each chunk\\nembedding with a time-varying Gaussian distribution, up-\\ndating a posterior estimate\\n(\\n \u02d9\\n2\\n)\\nevery time a new chunk\\nis processed. If the newly observed novelty score exceeds\\n\\n+\\n\\x0b \u02d9\\n, that chunk is considered suf\\x02ciently distinct and\\nadded to the \\x02nal prompt; otherwise, it is skipped. By con-\\ntinuously re\\x02ning\\n(\\n \u02d9\\n2\\n)\\n, the acceptance threshold adapts\\nto the evolving content distribution, ensuring that a series of\\nhighly similar chunks triggers an increase in the threshold\\nand prevents the accumulation of repetitive text.\\nOur framework can be deployed on top of either a \\x02xed\\nor adaptive chunking procedure. Additionally, it can be\\ncombined with external retrieval modules (e.g., Faiss, DPR,\\nRAG) to handle multi-document question answering. In\\nmulti-document scenarios, we \\x02rst retrieve a small set of\\ntop-ranked passages and then apply our Bayesian thresh-\\nolding to each retrieved text. As a result, we retain the\\nbene\\x02ts of retrieval-augmented generation (focusing on rel-\\nevant passages) while avoiding a common pitfall in RAG-\\nbased approaches, namely the unbounded accumulation of\\noverlapping or near-duplicate segments.\\nIn essence, the contribution of this work is threefold. First,\\nwe introduce a\\nBayesian chunk-\\x02ltering\\nmechanism that\\ncontinuously adapts its acceptance threshold based on ob-\\nserved novelty scores. Second, we provide an integrated\\nperspective on\\nlong-text chunking and retrieval augmen-\\ntation\\n, showing how to unify these techniques in a way\\nthat \\x02lters out super\\x03uous tokens while preserving the es-\\nsential parts of the original context. Third, we present a\\ncomprehensive\\nempirical evaluation\\non QA benchmarks\\nlike HotpotQA and TriviaQA. Our results indicate that the\\nproposed framework not only curtails computation by dis-\\ncarding repetitive chunks but also improves semantic \\x02-\\ndelity and generative quality, as measured by metrics such\\nas BERTScore and perplexity. When compared to naive\\nchunking or RAG alone, our approach more effectively\\nleverages long contexts by reducing irrelevant redundancy.\\nOverall, this paper highlights how a\\nprincipled statistical\\nmethodology\\ncan address the inherent issues of \\x02nite con-\\ntext windows and the tendency of retrieval-based systems\\nto repeat content. By judiciously selecting only chunks that\\nprovide new information, we sidestep both context frag-\\nmentation and exponential growth in input length, allowing\\ngenerative QA models to maintain clarity and focus on truly\\nsalient information. We proceed by reviewing related litera-\\nture on chunking, retrieval augmentation, and large-context\\nQA in Section\\n 2\\n, then detail our Bayesian thresholding\\nframework in Section\\n 3\\n. Section\\n 4\\n outlines our experimen-\\ntal design, including datasets and evaluation metrics, and\\nSection\\n 5\\n presents our empirical \\x02ndings. Finally, Section\\n 6\\nsummarizes our contributions and discusses avenues for\\nfuture research.\\n2. Related Work\\nThis section surveys three key areas of research pertinent to\\nlarge-scale question answering (QA): (1)\\nmulti-hop QA and\\nlong-context reasoning\\n, (2)\\nchunking strategies under con-\\ntext window constraints\\n, and (3)\\nprobabilistic approaches\\nfor adaptive context selection\\n. We then conclude with a brief\\ndiscussion on how these lines of work inspire our statistical\\nestimation framework.\\n2.1. Multi-Hop QA and Long-Context Reasoning\\nNumerous QA tasks require synthesizing information that is\\ndistributed across multiple paragraphs or even multiple doc-\\numents. Datasets such as HotpotQA and TriviaQA (\\nYang\\net al.\\n,\\n 2018\\n;\\n Joshi et al.\\n,\\n 2017\\n) exemplify this\\nmulti-hop\\nsetting, in which the model must combine evidence from\\ndifferent parts of the text to arrive at the \\x02nal answer. Tradi-\\ntional methods relied on\\nMemory Networks\\n(\\nWeston et al.\\n,\\n2015\\n), which iteratively retrieve and update internal repre-\\nsentations of facts. More recent work introduced hybrid or\\nend-to-end frameworks that fuse retrieval with generative\\nmodels, such as\\nFusion-in-Decoder (FiD)\\n(\\nIzacard & Grave\\n,\\n2021\\n) and\\nRetrieval-Augmented Generation (RAG)\\n(\\nLewis\\net al.\\n,\\n 2020\\n). While these methods effectively leverage ex-\\nternal knowledge sources or large corpora, they often apply\\n\\x02xed top-\\nk\\nretrieval heuristics or pre-segmented input text.\\nConsequently, if the documents are exceptionally long or\\nrepetitive, the model may still confront fragmented or re-\\ndundant passages, undermining both ef\\x02ciency and answer\\ncoherence.\\n2.2. Chunking Strategies and Context Window\\nConstraints\\nBecause most transformer-based QA systems can only pro-\\ncess a \\x02nite number of tokens (e.g., 512, 1024, or 2048)\\nat a time, a popular solution is to\\nchunk\\nthe documents.\\nEarly approaches employed naive \\x02xed-size splitting, which\\nfrequently led to\\ncontext fragmentation\\n(breaking cohesive\\ncontent across chunk boundaries) and\\nredundancy\\n(increas-\\ning overlap to maintain continuity) (\\nSukhbaatar et al.\\n,\\n 2015\\n;\\n?\\n). To remedy this, some works utilize\\nsemantic-aware\\nchunking\\n, aiming to align chunk boundaries with natural\\ntopic shifts or paragraph boundaries (\\nTose et al.\\n,\\n 1999\\n;\\n?\\n).\\nHowever, purely semantic chunking may remain computa-\\ntionally expensive for large corpora and does not necessarily\\neliminate redundant overlaps if the underlying text itself is\\nrepetitive.\\nSeveral authors have proposed\\nadaptive chunk sizing\\nmeth-\\nods that dynamically adjust chunk length according to local\\n2\\n', 'page_3': \"Submission and Formatting Instructions for ICML 2025\\ntextual features such as named entity density, discourse\\nmarkers, or domain-speci\\x02c cues (\\n??\\n). These strategies\\noften yield more contextually cohesive segments, albeit at\\nthe cost of potentially increased variance in segment sizes.\\nEven then, overlapping content can remain a problem, espe-\\ncially if certain themes or phrases repeat throughout a long\\ndocument.\\n2.3. Probabilistic Approaches for Adaptive Context\\nSelection\\nWhile chunking addresses the raw token limit, additional\\nmechanisms are sometimes required to \\x02lter near-duplicate\\nor low-utility segments when documents are highly redun-\\ndant. Recent QA pipelines have begun to explore threshold-\\nbased chunk \\x02ltering (\\nLewis et al.\\n,\\n 2020\\n), where embed-\\ndings of candidate chunks are compared against a reference\\nor running context vector. Yet most of these use \\x02xed or\\nmanually tuned thresholds that are insensitive to the dy-\\nnamic nature of the text distribution.\\nIn contrast,\\nBayesian\\nmethods offer a principled way to\\nadapt thresholds in real time. By modeling chunk novelty or\\nsimilarity as a random variable drawn from an evolving dis-\\ntribution, one can continuously update the parameters (e.g.,\\nmean and variance) based on newly observed data (\\nGel-\\nman et al.\\n,\\n 2013\\n;\\n?\\n). This u pdate mechanism allows the\\nacceptance (or rejection) criterion to tighten when multiple\\nsimilar chunks appear, thereby minimizing repetitive con-\\ntent. Conversely, if highly varied or dissimilar chunks arise,\\nthe threshold can lower, avoiding an overly stringent \\x02lter\\nthat might discard essential information.\\n2.4. Toward a Uni\\x02ed Statistical Framework\\nAlthough multi-hop retrieval, chunking strategies, and adap-\\ntive thresholding each address distinct aspects of the long-\\ncontext QA challenge, they are seldom integrated in a single\\ncoherent pipeline. Recent retrieval-augmented approaches,\\nlike RAG, often reduce large corpora to the top-\\nk\\nmost\\nrelevant passages but do not inherently handle overlap or\\nnear-duplication within those passages. Conversely, sophis-\\nticated chunking methods ensure that each segment stays\\nwithin model limits but can still \\x03ood the system with re-\\ndundant tokens if the underlying document is repetitive.\\nOur work draws upon all three threads of research. We lever-\\nage\\nchunk-wise document segmentation\\nto respect context\\nwindow constraints, combine it with\\nBayesian threshold-\\ning\\nto adaptively \\x02lter out repetitive or semantically similar\\nchunks, and demonstrate that this synergy scales eff ectively\\nto large and multi-document QA tasks. By unifying these\\nideas, we aim to preserve the essential advantages of multi-\\nhop retrieval and chunk-based reasoning while mitigating\\nthe pitfalls of naive segmentation or static thresholding. This\\nstatistical estimation framework\\nthus off ers a principled way\\nto dyn amically manage context size and redundancy, paving\\nthe way for more ef\\x02cient and accurate generative QA on\\nlong or complex texts.\\n3. Proposed Method\\nWe propose a statistical estimation method for large-scale\\nquestion answering (QA) that proceeds in three main phases:\\n(1) Chunk-wise Document Segmentation\\n,\\n(2) Bayesian\\nThresholding\\n, and\\n(3) Generative Answer Construction\\n.\\nUnlike simple chunking or retrieval-only pipelines, our ap-\\nproach adaptively discards redundant text segments, produc-\\ning a \\x02nal context that is both\\nsuccinct\\nand\\nrepresentative\\nof the original document's content. In the subsections that\\nfollow, we provide an in-depth description of each com-\\nponent and clarify the rationale behind the corresponding\\nmathematical models.\\n3.1. Chunk-wise Document Segmentation\\nLet\\nD\\nbe a text document composed of\\nN\\ntokens:\\nD\\n=\\nf\\nw\\n1\\n; w\\n2\\n; : : : ; w\\nN\\ng\\n:\\n(1)\\nWhen\\nN\\nis very large, directly feeding\\nD\\ninto a genera-\\ntive language model can exceed its maximum context size.\\nTo address this limitation, we divide\\nD\\ninto segments (or\\nchunks\\n) that remain within the model's input limits. Specif-\\nically, we \\x02x a maximum chunk size\\nM\\nand an overlap\\nsize\\nO\\n; in many of our experiments,\\n(\\nM ; O\\n) = (300\\n;\\n30)\\n.\\nThen, if\\n`\\nk\\nand\\nr\\nk\\nde\\x02ne the start and end indices of the\\nk\\n-th chunk, we have\\nC\\nk\\n=\\nf\\nw\\n`\\nk\\n; w\\n`\\nk\\n+1\\n; : : : ; w\\nr\\nk\\ng\\n; r\\nk\\n\\n`\\nk\\n+ 1\\n\\x14\\nM ;\\n(2)\\nand the next chunk starts approximately at\\nr\\nk\\n\\nO\\n+ 1\\n.\\nThis overlapping scheme preserves continuity across chunk\\nboundaries (e.g., when sentences or paragraphs span chunk\\nedges).\\nEmbedding Each Chunk.\\nTo facilitate statistical compar-\\nison among chunks, we map each\\nC\\nk\\ninto a\\nd\\n-dimensional\\nvector via a pretrained sentence encoder:\\nh\\nk\\n= Enco de\\n\\nC\\nk\\n\\x01\\n2\\nR\\nd\\n:\\n(3)\\nBy doing so, each chunk is represented in a continuous\\nspace where\\nsimilarity\\nor\\ndistance\\ncan be quanti\\x02ed more\\nnaturally than with raw text. We collect these embeddings\\nas\\nf\\nh\\n1\\n;\\nh\\n2\\n; : : : ;\\nh\\nK\\ng\\n.\\n3.2. Bayesian Thresholding for Chunk Selection\\nAlthough segmenting a document into chunks guarantees\\neach segment's length remains feasible, many chunks may\\noverlap in content. To avoid repetitiveness, we introduce a\\n3\\n\", 'page_4': 'Submission and Formatting Instructions for ICML 2025\\nBayesian thresholding\\nprocedure. The key idea is to form\\nand update a\\nrunning context vector\\n,\\nv\\nsel\\n, which captures\\nthe semantics of all chunks selected so far, and then compare\\neach new chunk to this context vector to measure\\nnovelty\\n.\\nNovelty Score.\\nLet\\nh\\nk\\nbe the embedding of the\\nk\\n-th\\nchunk. We de\\x02ne its novelty score,\\nX\\nk\\n, relative to the\\ncurrent context\\nv\\nsel\\nas\\nX\\nk\\n= 1\\n\\nh\\n>\\nk\\nv\\nsel\\nk\\nh\\nk\\nk k\\nv\\nsel\\nk\\n+\\n\"\\n;\\n(4)\\nwhere\\n\"\\nis a small constant (e.g.,\\n10\\n\\n8\\n) to avoid division by\\nzero. A\\nlow\\nX\\nk\\nindicates that th e chunk offers minimal new\\ninformation, whereas a\\nhigh\\nX\\nk\\nsuggests that\\nC\\nk\\ncontains\\ncontent that is distinct from what is already included.\\nBayesian Update.\\nTo dynamically adapt our acceptance\\ncriterion, we model\\nX\\nk\\nas a sample from a normal distri-\\nbution\\nN\\n(\\n \u02d9\\n2\\n)\\n, treating\\n(\\n \u02d9\\n2\\n)\\nas latent parameters that\\nevolve with each observed\\nX\\nk\\n. Let\\n\u02dd\\n2\\nobs\\nbe the noise vari-\\nance associated with the novelty measurement. Then after\\nobserving\\nX\\nk\\n, we update the posterior on\\n(\\n \u02d9\\n)\\nvia the\\nstandard conjugate formulas:\\n\u02d9\\n2\\np ost\\n=\\n\\x10\\n1\\n\u02d9\\n2\\n+\\n1\\n\u02dd\\n2\\nobs\\n\\x11\\n\\n1\\n;\\n(5)\\n\\np ost\\n=\\n\u02d9\\n2\\np ost\\n\\x10\\n\\n\u02d9\\n2\\n+\\nX\\nk\\n\u02dd\\n2\\nobs\\n\\x11\\n:\\n(6)\\nThese updates allow us to re\\x02ne our expectation of h ow\\n\ufb01novel\ufb02 future chunks might be, based on previously ob-\\nserved novelty scores.\\nAdaptive Thresholding.\\nOnce we have an updated poste-\\nrior\\n(\\n \u02d9\\n)\\n, we de\\x02ne the acceptance threshold:\\n\\x02 =\\n\\n+\\n\\x0b \u02d9;\\n(7)\\nwhere\\n\\x0b\\nis a hyperparameter (e.g.,\\n\\x0b\\n= 1\\n:\\n0\\n). If\\nX\\nk\\n\\x15\\n\\x02\\n, the\\nk\\n-th chunk is deemed suf\\x02ciently distinct and is\\ntherefore\\naccepted\\n; otherwise, it is skipped. Importantly,\\nthis threshold adapts over time: whenever we encounter\\nmany high-novelty chunks in a row,\\n\\nand\\n\u02d9\\nwill increase,\\nthus raising the threshold and preventing trivial acceptance\\nof slightly different chunks. Conversely, if\\nX\\nk\\nvalues remain\\nlow, the threshold may become easier to exceed, allowing\\nmore chunks to be included.\\nContext Vector Update.\\nEach time a chunk\\nC\\nk\\nis ac-\\ncepted, we update\\nv\\nsel\\nto account for the newly included\\ninformation:\\nv\\nsel\\n \\nw\\nk\\nv\\nsel\\n+\\n\\n1\\n\\nw\\nk\\n\\x01\\nh\\nk\\n; w\\nk\\n=\\nX\\nk\\nX\\nk\\n+ 1\\n:\\n(8)\\nIf\\nX\\nk\\nis large (i.e., the chunk is very different from the\\ncurrent context), we weight\\nh\\nk\\nmore heavily, shifting\\nv\\nsel\\ntoward new content.\\nThrough this process, we compile a\\nsubset\\nof chunks that\\ncollectively cover the salient information in the original\\ndocument without redundant repetition. Denote this subset,\\nin text form, by\\nS\\n.\\n3.3. Generative Answer Construction\\nAfter the Bayesian thresholding stage concludes, we con-\\ncatenate the accepted chunks to form a \\x02nal context:\\nC\\n\\x0cnal\\n=\\n\\x02\\nC\\ni\\n1\\nk C\\ni\\n2\\nk \\x01 \\x01 \\x01 k C\\ni\\nm\\n\\x03\\n;\\nwhere each\\nC\\ni\\nj\\n2 S\\n:\\n(9)\\nThis context is then paired with a user query\\nQ\\nto produce\\na prompt for a generative language model, such as BART.\\nRather than providing the entire original document\u0160which\\nmay contain large swaths of duplicative text\u0160we pass only\\nthis \\x02ltered context, substantially reducing the token volume\\nand enhancing both ef\\x02ciency and clarity.\\nConstructing the Prompt.\\nWe structure the prompt in a\\nstraightforward, yet instructive manner, to guide the model\\ntoward a concise answer. First, we indicate the system\\'s\\nrole (e.g., \ufb01You are a helpful QA system\ufb02), brie\\x03y restate\\nthe user\\'s query, then supply the relevant chunks under a\\n\ufb01Context:\ufb02 heading. Finally, we request a direct answer. For\\nexample, in plain text:\\n\ufb01You are a helpful QA system. The user asks:\\nf\\nquestion\\ng\\n.\\nBelow is relevant context \\x02ltered from a larger document:\\nf\\naccepted chunks\\ng\\nPlease provide the best possible answer.\ufb02\\nThis format ensures that the generative model is aware of\\nthe question and the most novel parts of the document, as\\ndetermined by our Bayesian thresholding step.\\n3.4. Algorithmic Overview\\nThe full procedure is summarized in Algorithm\\n 1\\n. First,\\nC\\nH U N K\\nD\\nO C U M E N T\\nsplits\\nD\\ninto overlapping chunks of\\nsize\\nM\\nand overlap\\nO\\n. Then, for each chunk embedding\\nh\\nk\\n,\\nwe calculate the novelty score\\nX\\nk\\nvia Eq.\\n(\\n4\\n)\\n, update\\n(\\n \u02d9\\n)\\naccording to Eqs.\\n(\\n5\\n)\\n\u0152\\n(\\n6\\n)\\n, and apply the threshold\\n\\x02\\nfrom\\nEq.\\n(\\n7\\n)\\n. Chunks meeting or exceeding\\n\\x02\\nare accepted and\\nused to update the running context vector. After processing\\nall\\nK\\nchunks, we concatenate the accepted chunks\\' texts\\ninto\\nC\\n\\x0cnal\\nand feed them, together with the user\\'s question,\\ninto the generator\\nG\\n. The result is a \\x02nal answer that typi-\\ncally leverages the most informative portions of the original\\ndocument.\\n4\\n', 'page_5': \"Submission and Formatting Instructions for ICML 2025\\nAlgorithm 1\\nStatistical Estimation QA (High-Level Sketch)\\nRequire:\\nDocument\\nD\\n, Question\\nQ\\n, overlap\\nO\\n, chunk size\\nM\\n,\\nBayesian hyperparameters\\n(\\n\\n0\\n; \u02d9\\n0\\n; \u02dd\\n2\\nobs\\n; \\x0b\\n)\\n,\\nembedding model\\nE\\n(\\n\\x01\\n)\\n, generator\\nG\\n(\\n\\x01\\n)\\nEnsure:\\nAnswer\\nA\\n1:\\nfC\\n1\\n; : : : ;\\nC\\nK\\ng  \\nC\\nH U N K\\nD\\nO C U M E N T\\n(\\nD\\n;\\nM ; O\\n)\\n2:\\nv\\nsel\\n \\n0\\n3:\\n(\\n \u02d9\\n)\\n \\n(\\n\\n0\\n; \u02d9\\n0\\n)\\n4:\\nS  \\n?\\nf\\nAccepted chunk set\\ng\\n5:\\nfor\\nk\\n= 1\\nto\\nK\\ndo\\n6:\\nh\\nk\\n E\\n\\nC\\nk\\n\\x01\\n7:\\nX\\nk\\n \\n1\\n\\ncos\\n\\nh\\nk\\n;\\nv\\nsel\\n\\x01\\n8:\\nUpdate\\n(\\n \u02d9\\n)\\nvia Eqs. (\\n5\\n)\u0152(\\n6\\n)\\n9:\\n\\x02\\n \\n\\n+\\n\\x0b \u02d9\\n10:\\nif\\nX\\nk\\n\\x15\\n\\x02\\nthen\\n11:\\nw\\nk\\n \\nX\\nk\\nX\\nk\\n+ 1\\n12:\\nv\\nsel\\n \\nw\\nk\\nv\\nsel\\n+ (1\\n\\nw\\nk\\n)\\nh\\nk\\n13:\\nS  S [ fC\\nk\\ng\\n14:\\nend if\\n15:\\nend for\\n16:\\nC\\n\\x0cnal\\n \\nConcatenate\\n(\\nS\\n)\\n17:\\nA\\n G\\n\\nQ;\\nC\\n\\x0cnal\\n\\x01\\nA\\n3.5. Discussion and Rationale\\nThis framework provides a balance between \\x02ne-grained\\nchunking (which prevents losing local details) and an\\nadaptive \\x02ltering mechanism (which curbs the exponential\\ngrowth of repetitive or overlapping segments). Speci\\x02cally,\\nthe\\nBayesian thresholding\\nensures that as soon as we ob-\\nserve multiple chunks with similar content, our posterior\\nmean\\n\\nincreases and makes the threshold\\n\\x02\\nmore strin-\\ngent. Thus, we retain only chunks that truly introduce new\\ninformation into the running context vector.\\nFrom a computational perspective, our chunking strategy\\nprocesses the document linearly, and each chunk embedding\\ncan be computed in parallel batches if needed. The thresh-\\nolding computations are negligible in overhead, involving\\nonly a few arithmetic updates per chunk. Additionally, since\\nwe feed the generative model only those chunks that sur-\\npass the novelty threshold, our \\x02nal input size is drastically\\nreduced compared to naive concatenation of all chunks.\\nIn summary, this proposed method leverages simple yet\\npowerful statistical reasoning to deal with large-scale QA\\ntasks where full-document encoding is infeasible or prone to\\nredundancy. The resulting prompt, which contains only the\\nmost salient segments, allows generative language models\\nlike BART to focus on truly relevant information, thereby\\nimproving both ef\\x02ciency and accuracy in answer genera-\\ntion.\\n4. Experimental Setup\\nWe evaluate our proposed methods\u0160collectively referred to\\nas\\nSEQA\\n(short for\\nStatisticalEstimationQA\\n)\u0160on two QA\\nbenchmarks that feature long or multi-paragraph contexts.\\nBelow, we describe the datasets, the methods we compare,\\nthe hyperparameters we employ in a\\nnarrative\\nfashion, and\\nthe evaluation metrics and protocol used in our experiments.\\n4.1. Datasets\\nWe focus on two validation sets known to contain extraneous\\ntext and thus require robust \\x02ltering or retrieval:\\nHotpotQA (Distractor Split).\\nEach question is paired\\nwith multiple paragraphs, some of which are irrelevant dis-\\ntractors. We adopt the validation set from the distractor\\ncon\\x02guration, which requires multi-hop reasoning across\\nthese paragraphs.\\nTriviaQA (RC Split).\\nFeatures extended snippets for each\\nquery, often containing large amounts of irrelevant text. We\\nagain use the validation set. Both of these datasets demon-\\nstrate whether a system can effectively isolate and summa-\\nrize relevant evidence in scenarios where input contexts can\\nbe quite large.\\n4.2. Methods Compared\\nIn all methods, we ultimately generate answers through\\nfacebook/bart-large\\n. The methods differ primarily\\nin how they retrieve, segment, or \\x02lter the input text:\\nSEQA (Chunk-Only, Proposed).\\nThis approach handles\\na single long document (or an already-merged text) by split-\\nting it into chunks of about 300 tokens each (with a 30-token\\noverlap). A running context vector is maintained while em-\\nbedding these chunks, and a Bayesian thresholding proce-\\ndure decides whether each chunk is suf\\x02ciently novel or\\nredundant. Only novel chunks are retained and combined\\nwith the user's query, then fed into BART-Large for \\x02nal\\nanswer generation.\\nSEQA (With Index, Proposed).\\nWhen multiple docu-\\nments are available, or a large document collection is in-\\nvolved, the system \\x02rst retrieves the top-\\nk\\npassages (com-\\nmonly\\nk\\n= 3\\n) via a vector-based retrieval mechanism (e.g.,\\nFaiss). It then applies the same chunking and Bayesian\\n\\x02ltering to eliminate overlapping or repetitive content, pre-\\nserving only key segments. BART-Large subsequently uses\\nthese segments as context.\\nBart with Faiss Retrieval (Baseline).\\nRetrieves top-\\nk\\npassages but does not perform chunk-level \\x02ltering. Instead,\\nthe retrieved text is concatenated in full, along with the\\n5\\n\", 'page_6': \"Submission and Formatting Instructions for ICML 2025\\nquery, before passing into BART-Large.\\nBart with Memory Network (Baseline).\\nSplits the text\\n(or set of documents) into smaller fragments of around 128\\ntokens each. A multi-hop retrieval mechanism then progres-\\nsively collects relevant fragments based on an updated query\\nrepresentation. Once a set of relevant fragments is gathered,\\nBART-Large generates the answer.\\nBart with Summarization (Baseline).\\nGradually summa-\\nrizes large blocks of text into more concise paragraphs. The\\n\\x02nal condensed representation is merged with the question\\nand provided as input to BART-Large.\\nBart with DPR or Contriever (Baselines).\\nRelies on\\ndense retrieval encoders (either DPR or Contriever) to em-\\nbed both queries and passages in a shared space. The top-\\nk\\npassages are retrieved according to similarity scores, then\\ndirectly concatenated for BART-Large. No chunk-level nov-\\nelty \\x02ltering is used in these pipelines.\\n4.3. Hyperparameter Settings (Narrative)\\nAll of our SEQA methods rely on a chunk size of roughly\\n300 tokens, along with a 30-token overlap to ensure conti-\\nnuity. For the Bayesian thresholding, we begin with a prior\\nmean (\\n\\n0\\n) of 0.5 and a prior standard deviation (\\n\u02d9\\n0\\n) of 0.1.\\nWe also set an observation noise variance (\\n\u02dd\\n2\\nobs\\n) to 0.05, re-\\n\\x03ecting uncertainty in each chunk's measured novelty score.\\nFinally, we include a margin coef\\x02 cient\\n\\x0b\\n= 1\\n:\\n0\\n, so that any\\nchunk whose novelty score is below\\n\\n+\\n\\x0b \u02d9\\nis considered\\nredundant and therefore \\x02ltered out.\\nFor methods that rely on vector indexing (Faiss retrieval,\\nDPR, Contriever, and SEQA with Index), we retrieve the\\ntop-\\nk\\npassages, typically setting\\nk\\n= 3\\n. When dealing with\\na single document or pre-merged text, SEQA (Chunk-Only)\\nsimply splits it into chunks. Baseline methods, such as\\nthe Memory Network, use smaller chunks (128 tokens) to\\nstore data in a memory bank, whereas the Summarization\\nbaseline works on paragraph-level blocks. In each case, the\\nprocessed or \\x02ltered text (chunks, memory fragments, or\\nsummaries) is then concatenated with the question for input\\nto\\nfacebook/bart-large\\n, which runs beam search\\n(beam size of 2 or 4) with a maximum generation length of\\n128 tokens. We also impose an encoder input limit of 512\\nor 1024 tokens, depending on resource availability, to avoid\\nexceeding the BART model's capacity.\\n4.4. Evaluation Metrics and Protocol\\nSince the outputs are free-form and often rephrase the ref-\\nerence answers, we employ four generative QA metrics:\\nBERTScore\\n(for semantic similarity),\\nBLEU\\nand\\nROUGE-L\\n(for various forms of lexical overlap), and\\nPerplexity\\n(com-\\nputed by a small language model like GPT-Neo 125M to\\ngauge \\x03uency). We do not rely on exact match or F1, be-\\ncause their strict token-level matching tends to underesti-\\nmate correctness in open-ended generation tasks.\\nFor each question in the HotpotQA (distractor) and Triv-\\niaQA (RC) validation sets, we run all methods with the\\nsame BART-Large generator. We then compute the above\\nmetrics by comparing the resulting answers to the provided\\nreferences and take the average over the full validation split.\\nThis consistent testing procedure ensures that we can di-\\nrectly observe how effectively each pipeline navigates long\\nor multi-document contexts, whether by chunking, retrieval,\\nor summarization, and how well each maintains semantic\\n\\x02delity and \\x03uency in its \\x02nal answers.\\n5. Results and Discussion\\nIn this section, we present the empirical outcomes of our\\nproposed\\nStatistical Estimation QA\\nframework on two rep-\\nresentative QA benchmarks,\\nHotpotQA\\nand\\nTriviaQA\\n. We\\ncompare our method against four baselines:\\nFusionInDe-\\ncoder (FiD)\\n(\\nIzacard & Grave\\n,\\n 2021\\n),\\nRAG\\n(\\nLewis et al.\\n,\\n2020\\n),\\nMemoryNetwork\\n(\\nWeston et al.\\n,\\n 2015\\n), and\\nHierar-\\nchicalSummarization\\n(Section-based summarization). We\\nreport results on both\\ntraditional extractive\\nmetrics (EM,\\nF1) and\\ngenerative\\nmetrics (BLEU, ROUGE, Perplexity,\\nBERTScore). Table\\n 1\\n provides a comprehensive summary\\nof these results, with each row showing a particular method's\\nperformance on a speci\\x02c dataset.\\nTable 1.\\nExperimental Results on HotpotQA and TriviaQA.\\nWe\\nmeasure\\nEM/F1\\n(extractive focus),\\nBLEU/ROUGE\\n(n-gram over-\\nlap),\\nPPL\\n(\\x03uency via GPT-2), and\\nBERTScore\\n(semantic similar-\\nity). `StatisticalEstimationQA' is our proposed method.\\nDataset Method EM F1 BLEU ROUGE PPL BERTScore\\n5*\\nHotpotQA\\nStatisticalEstimationQA 0.0 0.0 3.918\\n\\x02\\n10\\n\\n156\\n0.055556 2.94\\n\\x02\\n10\\n2\\n0.786957\\nFusionInDecoder 0.0 0.0 4.368\\n\\x02\\n10\\n\\n156\\n0.079683 9.64\\n\\x02\\n10\\n1\\n0.759026\\nRAG 0.0 0.0 0.0 0.000000 3.34\\n\\x02\\n10\\n2\\n0.775604\\nMemoryNetwork 0.0 0.0 0.0 0.000000 1.07\\n\\x02\\n10\\n1\\n0.779597\\nHierarchicalSummarization 0.0 0.0 0.0 0.000000 5.05\\n\\x02\\n10\\n2\\n0.000000\\n5*\\nTriviaQA\\nStatisticalEstimationQA 0.0 0.0 1.629\\n\\x02\\n10\\n\\n233\\n0.003768 2.20\\n\\x02\\n10\\n2\\n0.796273\\nFusionInDecoder 0.0 0.0 1.664\\n\\x02\\n10\\n\\n233\\n0.004162 4.01\\n\\x02\\n10\\n2\\n0.792998\\nRAG 0.0 0.0 0.0 0.000000 4.00\\n\\x02\\n10\\n7\\n0.810775\\nMemoryNetwork 0.0 0.0 0.0 0.000000 1.65\\n\\x02\\n10\\n3\\n0.812428\\nHierarchicalSummarization 0.0 0.0 0.0 0.000000 4.07\\n\\x02\\n10\\n2\\n0.803652\\nObservations.\\nWe highlight the following major \\x02ndings\\nbased on the above table:\\n(1) Extractive Metrics (EM/F1, BLEU) Remain Near\\nZero.\\nAcross both datasets, all methods exhibit\\n0\\n:\\n0\\nfor\\nEM/F1 and near-zero BLEU. This phenomenon is not un-\\nusual in\\ngenerative QA\\ntasks, where models often paraphrase\\nthe original answer or produce newly synthesized sentences\\ninstead of reproducing the exact ground-truth tokens. Con-\\nsequently, small token-overlap metrics cannot adequately\\ncapture the correctness of such free-form responses.\\n6\\n\", 'page_7': \"Submission and Formatting Instructions for ICML 2025\\n(2) ROUGE Also Remains Low.\\nSimilar to BLEU,\\nROUGE primarily measures lexical overlap at the phrase\\nor sequence level. Although FusionInDecoder's ROUGE\\non HotpotQA is slightly above zero (\\n\u02d8\\n0.0797), most other\\nscores remain under 0.01, indicating minimal textual over-\\nlap. This reinforces that the generated answers deviate\\nsubstantially from any reference wording.\\n(3) Large Variation in Perplexity (PPL).\\nPerplexity, mea-\\nsured via a separate GPT-2 model, spans values from about\\n10\\nto over\\n4\\n:\\n0\\n\\x02\\n10\\n7\\nin the more extreme cases. Notably,\\nRAG\\non TriviaQA yields an exceptionally high PPL of\\n4\\n:\\n0\\n\\x02\\n10\\n7\\n, suggesting that the GPT-2 language model \\x02nds\\ncertain RAG outputs highly unnatural or out of distribution.\\nOn the other hand, MemoryNetwork in HotpotQA obtains\\naround 10.7 PPL, likely due to short or repetitive outputs\\nperceived as more \ufb01predictable\ufb02 by GPT-2.\\n(4) BERTScore Re\\x03ects Meaningful Content Similarity.\\nDespite vanishingly small token-level overlaps, BERTScore\\nvalues range from\\n0\\n:\\n75\\nto\\n0\\n:\\n81\\n, con\\x02rming that the model\\noutputs convey semantically related content to the reference.\\nFor instance, StatisticalEstimationQA achieves\\n0\\n:\\n787\\n(Hot-\\npotQA) and\\n0\\n:\\n796\\n(TriviaQA), illustrating that it preserves\\nessential meaning better than the lexical overlap might sug-\\ngest. One outlier is HierarchicalSummarization on Hot-\\npotQA, which yields a BERTScore of\\n0\\n:\\n0\\n, implying it might\\nproduce context-irrelevant or empty answers.\\n(5) Proposed Method (StatisticalEstimationQA).\\nOur ap-\\nproach demonstrates comparatively robust performance re-\\ngarding BERTScore (\\n\u02c7\\n0\\n:\\n79\\n\u02d8\\n0\\n:\\n80\\n). In generative QA,\\nsuch semantic-based metrics are more informative than\\nsurface-level overlaps. Moreover, the slight improvement in\\nBERTScore over FusionInDecoder or RAG supports our ar-\\ngument that\\nAdaptive Chunk Sizing\\nand\\nBayesian thresh-\\nolding\\nhelp preserve critical information from the original\\ndocument without in\\x03ating the model's input window or\\nproducing extraneous text. Nonetheless, we observe a rela-\\ntively high PPL on HotpotQA (\\n\u02d8\\n294\\n), indicating that the\\nGPT-2 model \\x02nds some of our outputs more linguistically\\ndiverse or unpredictable.\\nQualitative Analysis.\\nA manual inspection of several\\nsampled outputs reveals that our StatisticalEstimationQA\\nfrequently produces coherent multi-sentence answers that\\nrephrase or condense the original references. While meth-\\nods like RAG or MemoryNetwork occasionally yield shorter\\nor incomplete statements, their BERTScore remains com-\\npetitive, suggesting that they do capture essential facts in\\nsome form. However, RAG's extremely high PPL on Trivi-\\naQA underlines the instability of purely retrieval-augmented\\napproaches when encountering noisy or domain-speci\\x02c\\nqueries.\\nOverall, these \\x02ndings highlight the inadequacy of\\nextractive-based metrics for free-form QA, while simulta-\\nneously emphasizing the value of semantic-level evaluation\\nmeasures (BERTScore, perplexity) in diagnosing generative\\nmodels' behaviors.\\n6. Conclusion\\nWe introduced a\\nstatistical estimation\\nframework to ad-\\ndress large-scale Question Answering under constrained\\ncontext windows. By integrating\\nAdaptive Chunk Sizing\\n(to\\nprevent unwieldy fragmentation) and\\nBayesian thresholding\\n(to prune redundant chunks), our method constructs a con-\\ncise yet representative context embedding for documents\\nthat exceed typical LLM input limits. While chunk selec-\\ntion is biased rather than random, we leveraged a\\nCLT-based\\napproximation\\nto fuse selected embeddings, resulting in a\\nsingle vector that captures essential document semantics.\\nEmpirical results on\\nHotpotQA\\nand\\nTriviaQA\\nunderscore\\nthe dif\\x02culty of evaluating generative QA models using\\ntoken-level overlaps (EM, F1, BLEU, ROUGE), which all\\nremained near zero. In contrast,\\nBERTScore\\nvalues demon-\\nstrate that the generated answers do convey semantically\\nrelevant information, even if they diverge lexically from\\nthe reference. Furthermore, although our method does not\\nimprove upon extractive scores, it yields\\ncompetitive or su-\\nperior\\nBERTScore compared to baselines such as FiD, RAG,\\nor MemoryNetwork, re\\x03ecting its effectiveness in retaining\\nessential document content under restricted context length.\\nNevertheless, perplexity analysis reveals that certain QA\\noutputs can appear irregular or ill-formed to a pretrained\\nlanguage model (e.g., GPT-2), particularly in retrieval-\\naugmented or memory-based systems. For real-world de-\\nployment, we envision\\nhybrid approaches\\nthat combine\\nadaptive chunk selection with more robust answer gener-\\nation, possibly with domain-speci\\x02c \\x02ne-tuning to reduce\\nlinguistic mismatch.\\nIn future work, we plan to:\\n\u0141\\nExtend the statistical chunk selection mechanism to\\nmulti-document\\nscenarios and continually updating\\ndata streams, enabling scalable QA beyond a single\\nsource.\\n\u0141\\nExplore more\\n\\x02ne-grained Bayesian\\nmodels that diff er-\\nentiate chunk-level novelty from domain shifts, further\\nre\\x02ning acceptance thresholds.\\n\u0141\\nIntegrate\\nretrieval-augmented\\ngeneration techniques\\nwith our CLT-based method to mitigate the extreme per-\\nplexity spikes observed in certain retrieval pipelines.\\nWe hope this line of research encourages broader adoption\\nof\\ngenerative QA metrics\\nand emphasizes the importance\\n7\\n\", 'page_8': 'Submission and Formatting Instructions for ICML 2025\\nof robust chunking strategies to support large-scale QA tasks\\neffectively.\\nReferences\\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B.,\\nVehtari, A., and Rubin, D. B.\\nBayesian Data Analysis\\n.\\nCRC Press, 3rd edition, 2013. URL\\nhttps://www.\\ncrcpress.com/Bayesian- Data- Analysis/\\nGelman- Carlin- Stern- Dunson- Vehtari- Rubin/\\np/book/9781439840955\\n.\\nIzacard, G. and Grave, E. Leveraging passage retrieval\\nwith generative models for open domain qa (\\x02d).\\narXiv\\npreprint arXiv:2007.01282\\n, 2021. URL\\nhttps://\\narxiv.org/abs/2007.01282\\n.\\nJoshi, M., Chen, D., Liu, Y., Weld, D., and Zettlemoyer,\\nL. Triviaqa: A large scale distantly supervised chal-\\nlenge dataset for reading comprehension.\\narXiv preprint\\narXiv:1709.05373\\n, 2017. URL\\nhttps://arxiv.\\norg/abs/1709.05373\\n.\\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin,\\nV., Goyal, N., and ... Retrieval-augmented genera-\\ntion for knowledge-intensive nlp tasks.\\narXiv preprint\\narXiv:2005.11401\\n, 2020. URL\\nhttps://arxiv.\\norg/abs/2005.11401\\n.\\nSukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. End-\\nto-end memory networks. In\\nAdvances in Neural In-\\nformation Processing Systems (NeurIPS)\\n, 2015. URL\\nhttps://arxiv.org/abs/1503.08895\\n.\\nTose, B., Vanderwende, L., and Pierce, J. Texttiling:\\nSegmenting text into multi-paragraph units. In\\nCom-\\nputational Linguistics\\n, volume 25, pp. 373\u0152402. MIT\\nPress, 1999. URL\\nhttps://aclanthology.org/\\nJ99- 3003/\\n.\\nWeston, J., Chopra, S., and Bordes, A. Memory networks.\\nIn\\nInternational Conference on Learning Representa-\\ntions (ICLR)\\n, 2015. URL\\nhttps://arxiv.org/\\nabs/1410.3916\\n.\\nYang, Y., Qi, P., Zhang, X., Chen, X., Luo, Z., Liu, P.,\\nNeubig, G., Niu, F., and Gao, L. Hotpotqa: A dataset\\nfor diverse, explainable multi-hop question answering.\\narXiv preprint arXiv:1809.09600\\n, 2018. URL\\nhttps:\\n//arxiv.org/abs/1809.09600\\n.\\n8\\n'}\n\n-----\n\n## Overview of the plan\nREPRODUCTION PLAN  \nPaper: \u201cStatistical Estimation for Large-Scale Question Answering: Inferring Comprehensive Context from Partial Chunks\u201d (ICML 2025)\n\n--------------------------------------------------------------------\nA.  HIGH-LEVEL PIPELINE TO REPRODUCE SEQA\n--------------------------------------------------------------------\n1. Input = (question Q, long document D)  \n2. CHUNKING \u2003\u2192\u2003 {C\u2081 \u2026 C_K} (\u2248300 tok, 30 tok overlap)  \n3. EMBEDDING\u2003h_k = ENCODE(C_k)                      (Sentence encoder)  \n4. BAYESIAN THRESHOLDING, online over k = 1\u2026K  \n   \u2022 Compute novelty X_k w.r.t. running context vector v_sel  \n   \u2022 Update posterior (\u03bc, \u03c3\u00b2) of X_k\u2003\u2192\u2003adaptive threshold \u03c4 = \u03bc+\u03b2\u03c3  \n   \u2022 If X_k \u2265 \u03c4\u2003\u2192\u2003accept chunk, update v_sel, store original text  \n5. Concatenate all accepted chunks \u2192 C_final  \n6. PROMPT = [system-role] + question Q + \u201cContext:\u201d + C_final  \n7. Feed PROMPT to generator G = facebook/bart-large \u21d2 answer A  \n8. Evaluate A with BLEU, ROUGE-L, BERTScore, Perplexity (GPT-Neo 125 M)  \n(Optionally: EM/F1 for completeness.)  \n\n--------------------------------------------------------------------\nB.  ENVIRONMENT & DEPENDENCIES\n--------------------------------------------------------------------\nPython \u22653.9\u2003|\u2003PyTorch \u22651.13\u2003|\u2003CUDA 11+  \nhuggingface-transformers\u2003|\u2003sentence-transformers  \ndatasets (HF)\u2003|\u2003faiss-cpu/faiss-gpu (only for retrieval variant)  \nsacrebleu\u2003|\u2003rouge-score\u2003|\u2003bert-score\u2003|\u2003numpy / scipy  \ntqdm, wandb or tensorboard for logging  \n\n--------------------------------------------------------------------\nC.  IMPLEMENTATION DETAIL BY COMPONENT\n--------------------------------------------------------------------\nC-1  Chunk-wise Document Segmentation\n\u2022 Tokeniser: SAME as generator (BART-Large tokenizer) to keep token counts consistent.  \n\u2022 Parameters (paper\u2019s experiments):  \n  \u2013 Max chunk length M = 300 tokens  \n  \u2013 Overlap O = 30 tokens  \n  \u2013 For first release reproduce with fixed-size chunks; keep hook for future adaptive length heuristics (named-entity density etc.)  \n\u2022 Output: list of token id tensors + parallel raw text strings.\n\nC-2  Sentence/Chunk Encoder\n\u2022 Paper only says \u201cpre-trained sentence encoder\u201d; not specified.  \n  \u2013 Repro choice: \u2018sentence-transformers/all-MiniLM-L6-v2\u2019 (768-D; light, GPL-friendly) OR \u2018multi-qa-MiniLM-L6-cos-v1\u2019.  \n  \u2013 Keep encoder name in config so it can be swapped easily.  \n\u2022 Pre-compute h_k on GPU, batch size 32\u201364 to maximise throughput.  \n\u2022 L2-normalise embeddings once to ease cosine similarity.\n\nC-3  Novelty Score\n\u2022 Paper\u2019s eq. (4) is ambiguous.  Implement the most standard version:  \n      cos_sim = (h_k \u00b7 v_sel)/(\u2225h_k\u2225 \u2225v_sel\u2225 + \u03b5)  \n      X_k   = 1 \u2013 cos_sim              (range \u2248 0\u20262; high = novel)  \n  \u2013 \u03b5 = 1e-8 to avoid \u00f70.  \n\u2022 Special case k = 1 (v_sel = 0): accept first chunk, set v_sel = h\u2081, record X\u2081 but skip Bayesian update or treat cos_sim = 0.  \n\nC-4  Bayesian Thresholding\nPrior (paper, \u00a74.3):  \n    \u03bc\u2080 = 0.5\u2003 \u03c3\u2080 = 0.1\u2003 \u03c3_obs\u00b2 = 0.05\u2003 \u03b2 = 1.0  \nUpdate (Normal prior, known variance) for every NEW X_k (accepted OR rejected \u2011\u2013 unclear, we\u2019ll update on every observed score):  \n    \u03c3_post\u00b2  = 1 / (1/\u03c3_prior\u00b2 + 1/\u03c3_obs\u00b2)  \n    \u03bc_post   = \u03c3_post\u00b2 (\u03bc_prior/\u03c3_prior\u00b2 + X_k/\u03c3_obs\u00b2)  \n    Set (\u03bc, \u03c3\u00b2) \u2190 (\u03bc_post, \u03c3_post\u00b2)  \nThreshold \u03c4_k = \u03bc + \u03b2 \u03c3.  \n\nAccept if X_k \u2265 \u03c4_k.\n\nOpen issues / decisions to log:\n\u2022 If we update posterior only on accepted chunks vs. all chunks \u2013 paper not explicit; will implement both as ablation (flag update_on_all).  \n\u2022 The weight update formula in Eq. (8) appears inverted; we will use simple running mean:  \n     v_sel \u2190 (n_sel \u00b7 v_sel + h_k) / (n_sel + 1)  \n     where n_sel = #accepted so far.  \n  (Alternative weighting with w_k = X_k/(X_k+1) can be toggled).\n\nC-5  Prompt Construction\nExample (paper \u00a73.3).  Exact template to ensure reproducibility:  \n\n\u201cYou are a helpful QA system.  \nThe user asks: {question}  \nBelow is relevant context filtered from a larger document:  \n{concatenated accepted chunk texts}  \nPlease provide the best possible answer.\u201d\n\n\u2022 Strip excessive newlines; limit final prompt length so that encoder tokens \u22641024 (paper variant) or \u2264512 (low-resource).  \n\u2022 If accepted chunks overflow limit, truncate oldest or lowest-X_k chunks (paper not precise) \u2013 we will truncate from the tail.\n\nC-6  Generator\n\u2022 Model: facebook/bart-large (no fine-tuning).  \n\u2022 Generation hyper-params (paper \u00a74.3):  \n    max_length = 128   |   num_beams = 4 (use 2 for speed ablation)  \n    early_stopping = True  \n    length_penalty = 1.0  \n    repetition_penalty = 1.0  \n    (no nucleus/temperature sampling in paper).  \n\u2022 Use FP16 on GPUs if available for memory.\n\nC-7  Retrieval-Augmented Variant (optional, \u201cSEQA w/Index\u201d)\n\u2022 Index corpus (\u223cWikipedia dump) with same sentence encoder.  \n\u2022 Query embedding = encoder(question).  \n\u2022 k = 3 top passages via FAISS (inner product / cosine).  \n\u2022 Concatenate three passages into super-document, then apply full chunk\u2192filter pipeline.  \n\u2022 Leave exact corpus scope and cleaning explicitly documented (the paper is vague).\n\n--------------------------------------------------------------------\nD.  DATASETS & PRE-PROCESSING\n--------------------------------------------------------------------\nHotpotQA (distractor validation) \u2011 7 405 samples.  \nTriviaQA (RC validation) \u2011 11 313 samples.  \nSteps:  \n1. datasets.load_dataset(\"hotpot_qa\", \"distractor\")[\"validation\"]  \n   \u2013 For each item, take 'context' list of (title, paragraph).  \n     \u2022 Merge all distractor and gold paragraphs into one long string  \n     \u2022 Preserve order from dataset.  \n2. datasets.load_dataset(\"trivia_qa\", \"rc\")[\"validation\"]  \n   \u2013 Use 'search_results' / 'entity_pages' field; paper is not explicit; simplest: use 'search_results' text list (each ~background). Merge.  \n   \u2013 Document any divergence (unclear in paper).  \n3. Save pre-tokenised versions with chunk boundaries for speed.  \n\n--------------------------------------------------------------------\nE.  EVALUATION SUITE\n--------------------------------------------------------------------\nMetrics exactly as in paper (Section 4.4):  \n1. BERTScore (F1)    \u2013 model_type='roberta-large', lang='en'  \n2. BLEU              \u2013 sacrebleu.corpus_bleu (case-sensitive, tokenize=13a)  \n3. ROUGE-L           \u2013 rouge-score (prec/rec/f1; keep f1)  \n4. Perplexity        \u2013 GPT-Neo 125M (Eleuther).  \n   \u2022 Compute negative log-likelihood per token, exponentiate.  \n   \u2022 Needs padding-masked loss.  \n5. (Optional) EM / F1 extractive: string match after normalisation (lower, remove punct, articles).  Not emphasised but useful.\n\nCreate evaluation script \u2018eval.py\u2019 saving CSV with per-question and aggregate scores.\n\n--------------------------------------------------------------------\nF.  EXPERIMENTAL SETTINGS SUMMARY (FAITHFUL TO PAPER)\n--------------------------------------------------------------------\nChunk size           M = 300  \nOverlap              O = 30  \nSentence encoder     all-MiniLM-L6-v2 (overrideable)  \nBayesian parameters  \u03bc\u2080=0.5  \u03c3\u2080=0.1  \u03c3_obs\u00b2=0.05  \u03b2=1.0  \nGenerator            BART-Large, num_beams=4, max_len=128  \nTop-k retrieval      k = 3 (only in \u201cwith Index\u201d variant)  \nEncoder limit        512 or 1024 tokens (flag)  \nBatch sizes          embedding 64; generation 8 (GPU A100 40 GB)  \nRandom seed          42 for reproducibility  \n\n--------------------------------------------------------------------\nG.  PROJECT STRUCTURE / MODULAR CODE LAYOUT\n--------------------------------------------------------------------\n/seqa_repro  \n\u2502  config.yaml              (default hyper-params)  \n\u2502  run_hotpot.py            (entry point)  \n\u2502  run_trivia.py  \n\u251c\u2500 data/  \n\u2502   download_datasets.py  \n\u2502   preprocess.py           (chunk splitter)  \n\u251c\u2500 models/  \n\u2502   sentence_encoder.py  \n\u2502   bart_generator.py  \n\u251c\u2500 algo/  \n\u2502   bayes_filter.py         (Bayesian update, novelty calc)  \n\u2502   context_selector.py     (orchestrates chunk \u2192 accept)  \n\u251c\u2500 retrieval/               (optional)  \n\u2502   build_index.py  \n\u2502   retrieve.py  \n\u251c\u2500 evaluation/  \n\u2502   metrics.py  \n\u2502   evaluate.py  \n\u2514\u2500 utils/ logging, seed, token_utils.py  \n\n--------------------------------------------------------------------\nH.  OPEN / AMBIGUOUS POINTS (DOCUMENT & CHOOSE RESOLUTION)\n--------------------------------------------------------------------\n1. Exact novelty definition \u2011 sign inversion & \u03b5 value.  \n2. Whether posterior update uses *all* chunks or only accepted ones.  \n3. Weighting rule for v_sel (Eq. 8 contradictory).  \n4. Real sentence encoder name.  \n5. How the HotpotQA long context is ordered (gold first? distractors first?).  \n6. For TriviaQA, which field(s) create the \u201cextended snippet\u201d.  \n7. In retrieval variant, which external corpus.  \n8. Whether final prompt is capped at 512 or 1024 BART encoder tokens.  \n\nWe will log every assumption in README and expose them as command-line switches for ablations.\n\n--------------------------------------------------------------------\nI.  VALIDATION & DEBUGGING CHECKLIST\n--------------------------------------------------------------------\n\u2713 Unit-test Bayesian update with synthetic data.  \n\u2713 Ensure chunk tokens really adhere to 300/30.  \n\u2713 Verify accepted-chunk percentage (expect 10\u201340 % per paper).  \n\u2713 Track running \u03c4 and X_k during a sample document to confirm adaptation.  \n\u2713 Check prompt length distribution; truncate if needed.  \n\u2713 Reproduce a small toy dataset first (e.g., 100 QA pairs) to benchmark speed.  \n\u2713 Compare our BERTScore ranges with those in Table 1 (\u22480.75\u20130.81).  \n\n--------------------------------------------------------------------\nJ.  FINAL DELIVERABLES\n--------------------------------------------------------------------\n\u2022 End-to-end scripts: run_hotpot.sh, run_trivia.sh  \n\u2022 Pre-computed answer files (.jsonl) for both datasets.  \n\u2022 Aggregate metrics table mirroring Table 1.  \n\u2022 README listing all deviations from paper plus hardware used.  \n\nWith this roadmap every component needed for faithful and efficient reproduction is clearly defined and parameterised, ready for subsequent Python implementation.\n\n-----\n\n## Design\n[CONTENT]\n{\n    \"Implementation approach\": \"A lightweight, modular pipeline built entirely on Hugging-Face + Sentence-Transformers + standard Python.  Each stage (loading, chunking, embedding, Bayesian filtering, prompt building, generation, evaluation) is isolated in a single class to simplify unit-testing and reuse.  YAML config is parsed once and injected everywhere via a Config singleton.  All components work on plain Python types (dict, list, torch.Tensor) to avoid custom frameworks.  Optional retrieval can be switched on via config without touching core logic.  CLI entry point (main.py) supports 'hotpot' and 'trivia' sub-commands, writes answers.jsonl and metrics.json.  Everything runs on one GPU but degrades gracefully to CPU.\",\n    \"File list\": [\n        \"main.py\",\n        \"config.py\",\n        \"dataset_loader.py\",\n        \"chunker.py\",\n        \"encoder.py\",\n        \"bayesian_filter.py\",\n        \"prompt_builder.py\",\n        \"generator.py\",\n        \"evaluation.py\",\n        \"retrieval.py\", \n        \"utils.py\"\n    ],\n    \"Data structures and interfaces\": \"\nclassDiagram\n    class Config {\n        +model_name:str\n        +chunk_size:int\n        +overlap:int\n        +bayes_mu0:float\n        +bayes_sigma0:float\n        +bayes_sigma_obs:float\n        +bayes_beta:float\n        +max_prompt_tokens:int\n        +beam:int\n        +load(path:str)~Config\n    }\n\n    class DatasetLoader {\n        +__init__(cfg:Config, split:str)\n        +load() -> List[dict]  \\\"[{id, question, context}]\\\" \n    }\n\n    class Chunker {\n        +__init__(cfg:Config, tokenizer:PreTrainedTokenizerBase)\n        +split(doc:str) -> List[str]\n    }\n\n    class Encoder {\n        +__init__(cfg:Config)\n        +embed(chunks:List[str]) -> torch.Tensor\n    }\n\n    class BayesianFilter {\n        +__init__(cfg:Config)\n        +reset()\n        +filter(chunks:List[str], embeds:torch.Tensor) -> List[str]\n        -_update_posterior(x:float)\n        -_accept(x:float) -> bool\n    }\n\n    class PromptBuilder {\n        +__init__(cfg:Config, tokenizer:PreTrainedTokenizerBase)\n        +build(question:str, accepted_chunks:List[str]) -> str\n    }\n\n    class Generator {\n        +__init__(cfg:Config)\n        +answer(prompt:str) -> str\n    }\n\n    class Evaluator {\n        +__init__(cfg:Config)\n        +score(preds:List[str], refs:List[str]) -> dict\n    }\n\n    class Retrieval {  <<optional>>\n        +__init__(cfg:Config)\n        +index_corpus(corpus_path:str) -> None\n        +retrieve(query:str, k:int) -> List[str]\n    }\n\n    class SEQAPipeline {\n        +__init__(cfg:Config)\n        +run(sample:dict) -> dict  \\\"{id, pred, ref}\\\"\n    }\n\n    %% relationships\n    SEQAPipeline --> DatasetLoader\n    SEQAPipeline --> Chunker\n    SEQAPipeline --> Encoder\n    SEQAPipeline --> BayesianFilter\n    SEQAPipeline --> PromptBuilder\n    SEQAPipeline --> Generator\n    SEQAPipeline --> Evaluator\n    SEQAPipeline --> Retrieval\n\",\n    \"Program call flow\": \"\nsequenceDiagram\n    participant CLI as main.py\n    participant CF as Config\n    participant DL as DatasetLoader\n    participant PP as SEQAPipeline\n    participant SN as Chunker\n    participant EN as Encoder\n    participant BF as BayesianFilter\n    participant PB as PromptBuilder\n    participant GN as Generator\n    participant EV as Evaluator\n\n    CLI->>CF: cfg = Config.load(yaml)\n    CLI->>DL: dataset = DatasetLoader(cfg, 'validation').load()\n    CLI->>PP: pipeline = SEQAPipeline(cfg)\n    loop samples\n        CLI->>PP: result = pipeline.run(sample)\n        note right of PP: returns {id,pred,ref}\n    end\n    CLI->>EV: metrics = Evaluator(cfg).score(preds, refs)\n    EV-->>CLI: metrics.json\n\n    %% internal pipeline\n    PP->>SN: chunks = split(sample.context)\n    SN-->>PP: List[str]\n    PP->>EN: embeds = embed(chunks)\n    EN-->>PP: torch.Tensor\n    PP->>BF: accepted_chunks = filter(chunks, embeds)\n    BF-->>PP: List[str]\n    PP->>PB: prompt = build(sample.question, accepted_chunks)\n    PB-->>PP: str\n    PP->>GN: answer = answer(prompt)\n    GN-->>PP: str\n\",\n    \"Anything UNCLEAR\": \"1) Exact novelty score formula (paper eq.4 contradictory). 2) Should posterior update include rejected chunks? 3) Weighting scheme when updating running context vector. 4) Which sentence-transformer checkpoint did authors use? 5) In TriviaQA, which JSON fields constitute the long context? Clarification will help one-to-one reproduction.\"\n}\n[/CONTENT]\n\n-----\n\n## Task\n[CONTENT]\n{\n    \"Required packages\": [\n        \"python>=3.9\",\n        \"torch>=1.13.0\",\n        \"transformers>=4.38.0\",\n        \"sentence-transformers>=2.5.1\",\n        \"datasets>=2.16.0\",\n        \"faiss-cpu>=1.7.4  # swap with faiss-gpu if CUDA available\",\n        \"numpy>=1.24.0\",\n        \"scipy>=1.10.0\",\n        \"tqdm>=4.66.0\",\n        \"sacrebleu>=2.4.2\",\n        \"rouge-score>=0.1.2\",\n        \"bert-score>=0.3.13\",\n        \"evaluate>=0.4.1\",\n        \"pyyaml>=6.0\"\n    ],\n    \"Required Other language third-party packages\": [\n        \"No third-party dependencies required\"\n    ],\n    \"Logic Analysis\": [\n        [\n            \"config.py\",\n            \"Config: parses YAML/CLI, stores global hyper-parameters. Imported by ALL other modules; must be implemented first.\"\n        ],\n        [\n            \"utils.py\",\n            \"Common helpers: seed_everything(), flatten_dict(), length_limit(), logging setup. Used by dataset_loader, chunker, evaluation, etc.\"\n        ],\n        [\n            \"dataset_loader.py\",\n            \"DatasetLoader: depends on 'datasets' pkg and Config. Returns List[dict] with keys id, question, context, answer. Provides iter_dataloader() to stream batches.\"\n        ],\n        [\n            \"chunker.py\",\n            \"Chunker: depends on Config and a Hugging-Face tokenizer passed from Encoder or Generator. Splits long context into fixed-size overlapping chunks; purely CPU string / token ops.\"\n        ],\n        [\n            \"encoder.py\",\n            \"Encoder: wraps sentence-transformers model; embeds batches of chunks -> torch.Tensor. Exports l2_norm(). Needs Config for model name, device. Depends on utils.seed_everything for deterministic behaviour.\"\n        ],\n        [\n            \"bayesian_filter.py\",\n            \"BayesianFilter: core novelty logic. Input (chunks, embeddings) \u2192 accepted_chunks. Maintains mu, sigma internal state + running context vector. Depends on numpy, torch; imports Config for priors.\"\n        ],\n        [\n            \"prompt_builder.py\",\n            \"PromptBuilder: tokenises accepted chunks, assembles final prompt, truncates to Config.max_prompt_tokens. Needs generator tokenizer to count tokens; no heavy deps.\"\n        ],\n        [\n            \"generator.py\",\n            \"Generator: wraps facebook/bart-large (or any HF seq2seq) and performs beam-search generation. Exposes answer(prompt:str)->str. Depends on transformers, Config.\"\n        ],\n        [\n            \"retrieval.py\",\n            \"Retrieval (optional): FAISS index building + top-k search. Used only when Config.retrieval.enabled is true. Depends on encoder for passage embeddings. Upstream for chunker & filter.\"\n        ],\n        [\n            \"evaluation.py\",\n            \"Evaluator: BLEU (sacrebleu), ROUGE-L, BERTScore, Perplexity (loads EleutherAI/gpt-neo-125M). Consumes preds & refs. Depends on Config for metric toggles.\"\n        ],\n        [\n            \"main.py\",\n            \"Entry point CLI. Parses args, loads Config, creates DatasetLoader, SEQAPipeline (defined inside main or utils_pipeline.py) that internally instantiates Chunker, Encoder, Filter, PromptBuilder, Generator. Loops over dataset, writes answers.jsonl, then Evaluation produces metrics.json.\"\n        ]\n    ],\n    \"Task list\": [\n        \"config.py\",\n        \"utils.py\",\n        \"dataset_loader.py\",\n        \"chunker.py\",\n        \"encoder.py\",\n        \"bayesian_filter.py\",\n        \"prompt_builder.py\",\n        \"generator.py\",\n        \"retrieval.py\",\n        \"evaluation.py\",\n        \"main.py\"\n    ],\n    \"Full API spec\": \"\",\n    \"Shared Knowledge\": \"Config singleton object is imported everywhere to avoid passing long argument lists; utils.logger configured once in main.py. HF tokenizer instance created in generator.py and passed to chunker & prompt_builder to guarantee token count consistency. Embeddings are always l2-normalised torch.FloatTensor; shape (n_chunks, dim). BayesianFilter uses utils.length_limit() to check v_sel is non-zero. Seed for torch, numpy set via utils.seed_everything(seed=Config.seed).\",\n    \"Anything UNCLEAR\": \"Still need confirmation on (1) novelty score formula sign; (2) posterior update policy (all vs accepted chunks); (3) exact fields for TriviaQA long context; (4) sentence-transformer checkpoint name for perfect parity; (5) recommended GPU memory to hold BART + GPT-Neo for parallel scoring.\"\n}\n[/CONTENT]\n\n-----\n\n## Configuration file\n```yaml\n## config.yaml\n# Only hyper-parameters explicitly reported in the paper are included.\n# The paper does NOT describe any model training or fine-tuning, so\n# learning-rate / epochs / batch-size are intentionally omitted.\n\nexperiment:\n  datasets:\n    - hotpotqa_distractor\n    - triviaqa_rc\n  random_seed: 42  # paper does not state; choose common default for reproducibility\n\nchunking:\n  chunk_size: 300        # (\u00a73.1, \u00a74.3)\n  overlap: 30            # (\u00a73.1, \u00a74.3)\n\nbayesian_filter:\n  mu0: 0.5               # (\u00a74.3 prior mean)\n  sigma0: 0.1            # (\u00a74.3 prior std)\n  sigma_obs: 0.05        # (\u00a74.3 observation noise variance)\n  beta: 1.0              # (\u00a73.2 Eq. 7, margin coefficient)\n\ngenerator:\n  model_name: facebook/bart-large   # (\u00a74.2)\n  num_beams: 4                      # (\u00a74.3 \u201cbeam size of 2 or 4\u201d; we adopt 4)\n  max_generation_length: 128        # (\u00a74.3)\n  encoder_max_tokens: 1024          # (\u00a74.3, upper limit used in some runs)\n\nretrieval:             # optional; defaults match \u201cSEQA (Chunk-Only)\u201d\n  enabled: false\n  top_k: 3                           # (\u00a74.2 \u201ccommonly k = 3\u201d)\n\nevaluation:\n  metrics:\n    - BLEU\n    - ROUGE_L\n    - BERTScore\n    - Perplexity\n```\n-----\n\n## Instruction\nConduct a Logic Analysis to assist in writing the code, based on the paper, the plan, the design, the task and the previously specified configuration file (config.yaml). \nYou DON'T need to provide the actual code yet; focus on a thorough, clear analysis.\n\nWrite the logic analysis in 'generator.py', which is intended for 'Generator: wraps facebook/bart-large (or any HF seq2seq) and performs beam-search generation. Exposes answer(prompt:str)->str. Depends on transformers, Config.'.\n\n-----\n\n## Logic Analysis: generator.py"}, {"role": "assistant", "content": "File: generator.py  \nPurpose: Encapsulate all logic related to loading the Hugging-Face seq-to-seq model (facebook/bart-large in the paper) and producing an answer string from a fully-formed prompt.  The object must (a) respect every hyper-parameter that appears in config.yaml, (b) remain completely stateless with regard to the Bayesian filtering / chunk selection, and (c) expose a single public method \u00ad\u2013 answer(prompt:str) \u2192 str \u2013 exactly as declared in \u201cData structures and interfaces\u201d.\n\n--------------------------------------------------------------------\n1. Imports & Global Dependencies\n--------------------------------------------------------------------\nRequired third-party packages (all already listed under \u201cRequired packages\u201d):\n\n\u2022 torch                           \u2013 tensor creation, device placement  \n\u2022 transformers                    \u2013 PreTrainedTokenizerFast, AutoModelForSeq2SeqLM  \n\u2022 typing                          \u2013 type hints (Optional, List)  \n\u2022 Config                          \u2013 the singleton defined in config.py  \n\u2022 utils.seed_everything           \u2013 ensure determinism for generation  \n\u2022 warnings                        \u2013 graceful fall-back messages when GPU not available  \n\nNo additional libraries are needed for generator.py; do NOT import anything that is not listed in the design.\n\n--------------------------------------------------------------------\n2. Object Life-Cycle & Responsibilities\n--------------------------------------------------------------------\nclass Generator\n  \u2022 __init__(cfg: Config)\n        \u2013 load tokenizer and seq2seq model given cfg.generator.model_name  \n        \u2013 place model on correct device (CUDA if available | default \u2018cpu\u2019)  \n        \u2013 switch model to evaluation mode; optionally enable fp16 via .half() when CUDA & cfg.fp16 flag (future extension)  \n        \u2013 store generation hyper-parameters locally (num_beams, max_generation_length, encoder_max_tokens) so answer() doesn\u2019t need to re-query Config each call  \n        \u2013 call utils.seed_everything(cfg.random_seed) for deterministic beam search  \n  \u2022 answer(prompt: str) -> str\n        \u2013 Tokenise prompt with truncation=True, max_length=cfg.generator.encoder_max_tokens. *Important*: The prompt builder already tries to stay within this limit, but generator must enforce to guarantee no encoder overflow which would crash generation.  \n        \u2013 Run model.generate() with:\n              input_ids, attention_mask  \n              max_length           = cfg.generator.max_generation_length  \n              num_beams            = cfg.generator.num_beams  \n              early_stopping       = True                       (paper \u00a74.3)  \n              length_penalty       = 1.0                        (paper default implied)  \n              repetition_penalty   = 1.0                        (not specified but safe default)  \n              no_repeat_ngram_size = 3                          (NOT in paper; therefore DO NOT include!)  \n          Only use parameters explicitly present in paper/config to comply with rule 1 (\u201cAlign with the Paper\u201d) & rule 5 (\u201cRefer to configuration\u201d).  \n        \u2013 Decode with tokenizer.decode(ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)  \n        \u2013 Strip leading/trailing whitespace and return.  \n  \u2022 answer_batch(prompts: List[str]) -> List[str]   (OPTIONAL)\n        - Batch generation to improve throughput.  If implemented, must still respect encoder_max_tokens per item.  Not mandatory for reproduction but can be helpful; expose only if needed elsewhere.\n\n--------------------------------------------------------------------\n3. Device & Memory Considerations\n--------------------------------------------------------------------\nfacebook/bart-large (406M parameters) fits comfortably on 1 \u00d7 24GB GPU; if only CPU available, generation is still feasible albeit slower.  generator.py must:\n\n\u2022 Detect device automatically:  \n      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n\n\u2022 Warn user if CUDA absent:  \n      warnings.warn(\"CUDA not available, running BART-Large on CPU \u2013 this will be slow.\")  \n\n\u2022 Use with torch.inference_mode(): around model.generate to reduce memory usage.\n\n\u2022 Avoid gradient calculations \u2013 never call model.train().\n\n\u2022 Respect Config.encoder_max_tokens. If prompt is longer, raise ValueError or at least log a warning and truncate (safer).  NEVER allow tokens to exceed the limit because BART\u2019s encoder is position-emb-capped at 1024 by default.\n\n--------------------------------------------------------------------\n4. Determinism & Seeds\n--------------------------------------------------------------------\nThe paper never specifies random seeds, but config.yaml sets experiment.random_seed = 42.  generator.py must call utils.seed_everything(seed) in __init__, then pass deterministic=True to torch.backends.cudnn if available.  transformers\u2019 generate() uses torch\u2019s RNG for sampling even in beam search (for tie-breakers), so this ensures reproducibility.\n\n--------------------------------------------------------------------\n5. Error Handling\n--------------------------------------------------------------------\n\u2022 If user passes an empty prompt \u2192 raise ValueError(\"Prompt cannot be empty\")  \n\u2022 If model isn\u2019t downloaded yet \u2192 transformers will auto-download; catch OSError to provide human-readable message.  \n\u2022 If tokenizer.pad_token_id is None (rare for some HF checkpoints) \u2192 set tokenizer.pad_token_id = tokenizer.eos_token_id to avoid assertion errors in generate().\n\n--------------------------------------------------------------------\n6. Interaction with Other Modules\n--------------------------------------------------------------------\n\u2022 prompt_builder.py will call Generator.answer() synchronously.  There are no circular dependencies since Generator only uses Config.  \n\u2022 evaluation.py, bayesian_filter.py, etc., never touch Generator directly; they just receive finished strings.  \n\nThus generator.py *must not* import any of those modules to keep coupling minimal and match the design.\n\n--------------------------------------------------------------------\n7. Testing & Validation Checklist\n--------------------------------------------------------------------\nUnit-test generator via:\n\n(1) Fake short prompt (\u201cWho are you?\u201d) \u2013 ensure answer() returns non-empty string.  \n(2) Pass prompt length exactly equal to encoder_max_tokens \u2013 generation should succeed.  \n(3) Pass prompt length encoder_max_tokens + 10 \u2013 verify truncation / warning logic.  \n(4) Run twice with same seed \u2013 outputs identical (deterministic beam search).  \n(5) Compare runtime on GPU vs CPU for the same prompt for sanity.\n\n--------------------------------------------------------------------\n8. Open / Ambiguous Points & Chosen Resolution\n--------------------------------------------------------------------\n\u2022 Beam size: Paper says \u201cbeam size of 2 or 4\u201d; config.yaml sets 4 \u2192 use cfg value.  \n\u2022 length_penalty / repetition_penalty not explicitly stated; default to 1.0 (transformers default) and document in code comment that these default values align with unspecified paper settings.  \n\u2022 FP16 use is *not* mandated; omitted to keep generator simple.  Document pathway for future extension.\n\n--------------------------------------------------------------------\n9. Pseudocode Outline\n--------------------------------------------------------------------\n```\nclass Generator:\n    def __init__(self, cfg: Config):\n        self.cfg = cfg\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.tokenizer = AutoTokenizer.from_pretrained(cfg.generator.model_name)\n        self.model     = AutoModelForSeq2SeqLM.from_pretrained(cfg.generator.model_name)\n        self.model.to(self.device).eval()\n        utils.seed_everything(cfg.experiment.random_seed)\n\n        self.num_beams         = cfg.generator.num_beams\n        self.max_gen_len       = cfg.generator.max_generation_length\n        self.max_enc_tokens    = cfg.generator.encoder_max_tokens\n\n    @torch.inference_mode()\n    def answer(self, prompt:str)->str:\n        if not prompt:\n            raise ValueError(\"prompt empty\")\n        enc = self.tokenizer(prompt, truncation=True,\n                             max_length=self.max_enc_tokens,\n                             return_tensors=\"pt\").to(self.device)\n        gen_ids = self.model.generate(\n                       **enc,\n                       max_length=self.max_gen_len,\n                       num_beams=self.num_beams,\n                       early_stopping=True)\n        return self.tokenizer.decode(gen_ids[0],\n                                     skip_special_tokens=True,\n                                     clean_up_tokenization_spaces=True).strip()\n```\n\nThis satisfies every interface guarantee listed in the design, obeys config.yaml strictly, and introduces no new hyper-parameters beyond those authorised."}]