[{"id": "chatcmpl-BUnbI9gtAkqCtF5ZQli28YRaEPGtM", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "REPRODUCTION PLAN  \nPaper: \u201cStatistical Estimation for Large-Scale Question Answering: Inferring Comprehensive Context from Partial Chunks\u201d (ICML 2025)\n\n--------------------------------------------------------------------\nA.  HIGH-LEVEL PIPELINE TO REPRODUCE SEQA\n--------------------------------------------------------------------\n1. Input = (question Q, long document D)  \n2. CHUNKING \u2003\u2192\u2003 {C\u2081 \u2026 C_K} (\u2248300 tok, 30 tok overlap)  \n3. EMBEDDING\u2003h_k = ENCODE(C_k)                      (Sentence encoder)  \n4. BAYESIAN THRESHOLDING, online over k = 1\u2026K  \n   \u2022 Compute novelty X_k w.r.t. running context vector v_sel  \n   \u2022 Update posterior (\u03bc, \u03c3\u00b2) of X_k\u2003\u2192\u2003adaptive threshold \u03c4 = \u03bc+\u03b2\u03c3  \n   \u2022 If X_k \u2265 \u03c4\u2003\u2192\u2003accept chunk, update v_sel, store original text  \n5. Concatenate all accepted chunks \u2192 C_final  \n6. PROMPT = [system-role] + question Q + \u201cContext:\u201d + C_final  \n7. Feed PROMPT to generator G = facebook/bart-large \u21d2 answer A  \n8. Evaluate A with BLEU, ROUGE-L, BERTScore, Perplexity (GPT-Neo 125 M)  \n(Optionally: EM/F1 for completeness.)  \n\n--------------------------------------------------------------------\nB.  ENVIRONMENT & DEPENDENCIES\n--------------------------------------------------------------------\nPython \u22653.9\u2003|\u2003PyTorch \u22651.13\u2003|\u2003CUDA 11+  \nhuggingface-transformers\u2003|\u2003sentence-transformers  \ndatasets (HF)\u2003|\u2003faiss-cpu/faiss-gpu (only for retrieval variant)  \nsacrebleu\u2003|\u2003rouge-score\u2003|\u2003bert-score\u2003|\u2003numpy / scipy  \ntqdm, wandb or tensorboard for logging  \n\n--------------------------------------------------------------------\nC.  IMPLEMENTATION DETAIL BY COMPONENT\n--------------------------------------------------------------------\nC-1  Chunk-wise Document Segmentation\n\u2022 Tokeniser: SAME as generator (BART-Large tokenizer) to keep token counts consistent.  \n\u2022 Parameters (paper\u2019s experiments):  \n  \u2013 Max chunk length M = 300 tokens  \n  \u2013 Overlap O = 30 tokens  \n  \u2013 For first release reproduce with fixed-size chunks; keep hook for future adaptive length heuristics (named-entity density etc.)  \n\u2022 Output: list of token id tensors + parallel raw text strings.\n\nC-2  Sentence/Chunk Encoder\n\u2022 Paper only says \u201cpre-trained sentence encoder\u201d; not specified.  \n  \u2013 Repro choice: \u2018sentence-transformers/all-MiniLM-L6-v2\u2019 (768-D; light, GPL-friendly) OR \u2018multi-qa-MiniLM-L6-cos-v1\u2019.  \n  \u2013 Keep encoder name in config so it can be swapped easily.  \n\u2022 Pre-compute h_k on GPU, batch size 32\u201364 to maximise throughput.  \n\u2022 L2-normalise embeddings once to ease cosine similarity.\n\nC-3  Novelty Score\n\u2022 Paper\u2019s eq. (4) is ambiguous.  Implement the most standard version:  \n      cos_sim = (h_k \u00b7 v_sel)/(\u2225h_k\u2225 \u2225v_sel\u2225 + \u03b5)  \n      X_k   = 1 \u2013 cos_sim              (range \u2248 0\u20262; high = novel)  \n  \u2013 \u03b5 = 1e-8 to avoid \u00f70.  \n\u2022 Special case k = 1 (v_sel = 0): accept first chunk, set v_sel = h\u2081, record X\u2081 but skip Bayesian update or treat cos_sim = 0.  \n\nC-4  Bayesian Thresholding\nPrior (paper, \u00a74.3):  \n    \u03bc\u2080 = 0.5\u2003 \u03c3\u2080 = 0.1\u2003 \u03c3_obs\u00b2 = 0.05\u2003 \u03b2 = 1.0  \nUpdate (Normal prior, known variance) for every NEW X_k (accepted OR rejected \u2011\u2013 unclear, we\u2019ll update on every observed score):  \n    \u03c3_post\u00b2  = 1 / (1/\u03c3_prior\u00b2 + 1/\u03c3_obs\u00b2)  \n    \u03bc_post   = \u03c3_post\u00b2 (\u03bc_prior/\u03c3_prior\u00b2 + X_k/\u03c3_obs\u00b2)  \n    Set (\u03bc, \u03c3\u00b2) \u2190 (\u03bc_post, \u03c3_post\u00b2)  \nThreshold \u03c4_k = \u03bc + \u03b2 \u03c3.  \n\nAccept if X_k \u2265 \u03c4_k.\n\nOpen issues / decisions to log:\n\u2022 If we update posterior only on accepted chunks vs. all chunks \u2013 paper not explicit; will implement both as ablation (flag update_on_all).  \n\u2022 The weight update formula in Eq. (8) appears inverted; we will use simple running mean:  \n     v_sel \u2190 (n_sel \u00b7 v_sel + h_k) / (n_sel + 1)  \n     where n_sel = #accepted so far.  \n  (Alternative weighting with w_k = X_k/(X_k+1) can be toggled).\n\nC-5  Prompt Construction\nExample (paper \u00a73.3).  Exact template to ensure reproducibility:  \n\n\u201cYou are a helpful QA system.  \nThe user asks: {question}  \nBelow is relevant context filtered from a larger document:  \n{concatenated accepted chunk texts}  \nPlease provide the best possible answer.\u201d\n\n\u2022 Strip excessive newlines; limit final prompt length so that encoder tokens \u22641024 (paper variant) or \u2264512 (low-resource).  \n\u2022 If accepted chunks overflow limit, truncate oldest or lowest-X_k chunks (paper not precise) \u2013 we will truncate from the tail.\n\nC-6  Generator\n\u2022 Model: facebook/bart-large (no fine-tuning).  \n\u2022 Generation hyper-params (paper \u00a74.3):  \n    max_length = 128   |   num_beams = 4 (use 2 for speed ablation)  \n    early_stopping = True  \n    length_penalty = 1.0  \n    repetition_penalty = 1.0  \n    (no nucleus/temperature sampling in paper).  \n\u2022 Use FP16 on GPUs if available for memory.\n\nC-7  Retrieval-Augmented Variant (optional, \u201cSEQA w/Index\u201d)\n\u2022 Index corpus (\u223cWikipedia dump) with same sentence encoder.  \n\u2022 Query embedding = encoder(question).  \n\u2022 k = 3 top passages via FAISS (inner product / cosine).  \n\u2022 Concatenate three passages into super-document, then apply full chunk\u2192filter pipeline.  \n\u2022 Leave exact corpus scope and cleaning explicitly documented (the paper is vague).\n\n--------------------------------------------------------------------\nD.  DATASETS & PRE-PROCESSING\n--------------------------------------------------------------------\nHotpotQA (distractor validation) \u2011 7 405 samples.  \nTriviaQA (RC validation) \u2011 11 313 samples.  \nSteps:  \n1. datasets.load_dataset(\"hotpot_qa\", \"distractor\")[\"validation\"]  \n   \u2013 For each item, take 'context' list of (title, paragraph).  \n     \u2022 Merge all distractor and gold paragraphs into one long string  \n     \u2022 Preserve order from dataset.  \n2. datasets.load_dataset(\"trivia_qa\", \"rc\")[\"validation\"]  \n   \u2013 Use 'search_results' / 'entity_pages' field; paper is not explicit; simplest: use 'search_results' text list (each ~background). Merge.  \n   \u2013 Document any divergence (unclear in paper).  \n3. Save pre-tokenised versions with chunk boundaries for speed.  \n\n--------------------------------------------------------------------\nE.  EVALUATION SUITE\n--------------------------------------------------------------------\nMetrics exactly as in paper (Section 4.4):  \n1. BERTScore (F1)    \u2013 model_type='roberta-large', lang='en'  \n2. BLEU              \u2013 sacrebleu.corpus_bleu (case-sensitive, tokenize=13a)  \n3. ROUGE-L           \u2013 rouge-score (prec/rec/f1; keep f1)  \n4. Perplexity        \u2013 GPT-Neo 125M (Eleuther).  \n   \u2022 Compute negative log-likelihood per token, exponentiate.  \n   \u2022 Needs padding-masked loss.  \n5. (Optional) EM / F1 extractive: string match after normalisation (lower, remove punct, articles).  Not emphasised but useful.\n\nCreate evaluation script \u2018eval.py\u2019 saving CSV with per-question and aggregate scores.\n\n--------------------------------------------------------------------\nF.  EXPERIMENTAL SETTINGS SUMMARY (FAITHFUL TO PAPER)\n--------------------------------------------------------------------\nChunk size           M = 300  \nOverlap              O = 30  \nSentence encoder     all-MiniLM-L6-v2 (overrideable)  \nBayesian parameters  \u03bc\u2080=0.5  \u03c3\u2080=0.1  \u03c3_obs\u00b2=0.05  \u03b2=1.0  \nGenerator            BART-Large, num_beams=4, max_len=128  \nTop-k retrieval      k = 3 (only in \u201cwith Index\u201d variant)  \nEncoder limit        512 or 1024 tokens (flag)  \nBatch sizes          embedding 64; generation 8 (GPU A100 40 GB)  \nRandom seed          42 for reproducibility  \n\n--------------------------------------------------------------------\nG.  PROJECT STRUCTURE / MODULAR CODE LAYOUT\n--------------------------------------------------------------------\n/seqa_repro  \n\u2502  config.yaml              (default hyper-params)  \n\u2502  run_hotpot.py            (entry point)  \n\u2502  run_trivia.py  \n\u251c\u2500 data/  \n\u2502   download_datasets.py  \n\u2502   preprocess.py           (chunk splitter)  \n\u251c\u2500 models/  \n\u2502   sentence_encoder.py  \n\u2502   bart_generator.py  \n\u251c\u2500 algo/  \n\u2502   bayes_filter.py         (Bayesian update, novelty calc)  \n\u2502   context_selector.py     (orchestrates chunk \u2192 accept)  \n\u251c\u2500 retrieval/               (optional)  \n\u2502   build_index.py  \n\u2502   retrieve.py  \n\u251c\u2500 evaluation/  \n\u2502   metrics.py  \n\u2502   evaluate.py  \n\u2514\u2500 utils/ logging, seed, token_utils.py  \n\n--------------------------------------------------------------------\nH.  OPEN / AMBIGUOUS POINTS (DOCUMENT & CHOOSE RESOLUTION)\n--------------------------------------------------------------------\n1. Exact novelty definition \u2011 sign inversion & \u03b5 value.  \n2. Whether posterior update uses *all* chunks or only accepted ones.  \n3. Weighting rule for v_sel (Eq. 8 contradictory).  \n4. Real sentence encoder name.  \n5. How the HotpotQA long context is ordered (gold first? distractors first?).  \n6. For TriviaQA, which field(s) create the \u201cextended snippet\u201d.  \n7. In retrieval variant, which external corpus.  \n8. Whether final prompt is capped at 512 or 1024 BART encoder tokens.  \n\nWe will log every assumption in README and expose them as command-line switches for ablations.\n\n--------------------------------------------------------------------\nI.  VALIDATION & DEBUGGING CHECKLIST\n--------------------------------------------------------------------\n\u2713 Unit-test Bayesian update with synthetic data.  \n\u2713 Ensure chunk tokens really adhere to 300/30.  \n\u2713 Verify accepted-chunk percentage (expect 10\u201340 % per paper).  \n\u2713 Track running \u03c4 and X_k during a sample document to confirm adaptation.  \n\u2713 Check prompt length distribution; truncate if needed.  \n\u2713 Reproduce a small toy dataset first (e.g., 100 QA pairs) to benchmark speed.  \n\u2713 Compare our BERTScore ranges with those in Table 1 (\u22480.75\u20130.81).  \n\n--------------------------------------------------------------------\nJ.  FINAL DELIVERABLES\n--------------------------------------------------------------------\n\u2022 End-to-end scripts: run_hotpot.sh, run_trivia.sh  \n\u2022 Pre-computed answer files (.jsonl) for both datasets.  \n\u2022 Aggregate metrics table mirroring Table 1.  \n\u2022 README listing all deviations from paper plus hardware used.  \n\nWith this roadmap every component needed for faithful and efficient reproduction is clearly defined and parameterised, ready for subsequent Python implementation.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680204, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 4450, "prompt_tokens": 10765, "total_tokens": 15215, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 1792, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 10752}}}, {"id": "chatcmpl-BUncq0bILIJTLo5AaTBOdiyTZtG2k", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "[CONTENT]\n{\n    \"Implementation approach\": \"A lightweight, modular pipeline built entirely on Hugging-Face + Sentence-Transformers + standard Python.  Each stage (loading, chunking, embedding, Bayesian filtering, prompt building, generation, evaluation) is isolated in a single class to simplify unit-testing and reuse.  YAML config is parsed once and injected everywhere via a Config singleton.  All components work on plain Python types (dict, list, torch.Tensor) to avoid custom frameworks.  Optional retrieval can be switched on via config without touching core logic.  CLI entry point (main.py) supports 'hotpot' and 'trivia' sub-commands, writes answers.jsonl and metrics.json.  Everything runs on one GPU but degrades gracefully to CPU.\",\n    \"File list\": [\n        \"main.py\",\n        \"config.py\",\n        \"dataset_loader.py\",\n        \"chunker.py\",\n        \"encoder.py\",\n        \"bayesian_filter.py\",\n        \"prompt_builder.py\",\n        \"generator.py\",\n        \"evaluation.py\",\n        \"retrieval.py\", \n        \"utils.py\"\n    ],\n    \"Data structures and interfaces\": \"\nclassDiagram\n    class Config {\n        +model_name:str\n        +chunk_size:int\n        +overlap:int\n        +bayes_mu0:float\n        +bayes_sigma0:float\n        +bayes_sigma_obs:float\n        +bayes_beta:float\n        +max_prompt_tokens:int\n        +beam:int\n        +load(path:str)~Config\n    }\n\n    class DatasetLoader {\n        +__init__(cfg:Config, split:str)\n        +load() -> List[dict]  \\\"[{id, question, context}]\\\" \n    }\n\n    class Chunker {\n        +__init__(cfg:Config, tokenizer:PreTrainedTokenizerBase)\n        +split(doc:str) -> List[str]\n    }\n\n    class Encoder {\n        +__init__(cfg:Config)\n        +embed(chunks:List[str]) -> torch.Tensor\n    }\n\n    class BayesianFilter {\n        +__init__(cfg:Config)\n        +reset()\n        +filter(chunks:List[str], embeds:torch.Tensor) -> List[str]\n        -_update_posterior(x:float)\n        -_accept(x:float) -> bool\n    }\n\n    class PromptBuilder {\n        +__init__(cfg:Config, tokenizer:PreTrainedTokenizerBase)\n        +build(question:str, accepted_chunks:List[str]) -> str\n    }\n\n    class Generator {\n        +__init__(cfg:Config)\n        +answer(prompt:str) -> str\n    }\n\n    class Evaluator {\n        +__init__(cfg:Config)\n        +score(preds:List[str], refs:List[str]) -> dict\n    }\n\n    class Retrieval {  <<optional>>\n        +__init__(cfg:Config)\n        +index_corpus(corpus_path:str) -> None\n        +retrieve(query:str, k:int) -> List[str]\n    }\n\n    class SEQAPipeline {\n        +__init__(cfg:Config)\n        +run(sample:dict) -> dict  \\\"{id, pred, ref}\\\"\n    }\n\n    %% relationships\n    SEQAPipeline --> DatasetLoader\n    SEQAPipeline --> Chunker\n    SEQAPipeline --> Encoder\n    SEQAPipeline --> BayesianFilter\n    SEQAPipeline --> PromptBuilder\n    SEQAPipeline --> Generator\n    SEQAPipeline --> Evaluator\n    SEQAPipeline --> Retrieval\n\",\n    \"Program call flow\": \"\nsequenceDiagram\n    participant CLI as main.py\n    participant CF as Config\n    participant DL as DatasetLoader\n    participant PP as SEQAPipeline\n    participant SN as Chunker\n    participant EN as Encoder\n    participant BF as BayesianFilter\n    participant PB as PromptBuilder\n    participant GN as Generator\n    participant EV as Evaluator\n\n    CLI->>CF: cfg = Config.load(yaml)\n    CLI->>DL: dataset = DatasetLoader(cfg, 'validation').load()\n    CLI->>PP: pipeline = SEQAPipeline(cfg)\n    loop samples\n        CLI->>PP: result = pipeline.run(sample)\n        note right of PP: returns {id,pred,ref}\n    end\n    CLI->>EV: metrics = Evaluator(cfg).score(preds, refs)\n    EV-->>CLI: metrics.json\n\n    %% internal pipeline\n    PP->>SN: chunks = split(sample.context)\n    SN-->>PP: List[str]\n    PP->>EN: embeds = embed(chunks)\n    EN-->>PP: torch.Tensor\n    PP->>BF: accepted_chunks = filter(chunks, embeds)\n    BF-->>PP: List[str]\n    PP->>PB: prompt = build(sample.question, accepted_chunks)\n    PB-->>PP: str\n    PP->>GN: answer = answer(prompt)\n    GN-->>PP: str\n\",\n    \"Anything UNCLEAR\": \"1) Exact novelty score formula (paper eq.4 contradictory). 2) Should posterior update include rejected chunks? 3) Weighting scheme when updating running context vector. 4) Which sentence-transformer checkpoint did authors use? 5) In TriviaQA, which JSON fields constitute the long context? Clarification will help one-to-one reproduction.\"\n}\n[/CONTENT]", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680300, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 1389, "prompt_tokens": 13928, "total_tokens": 15317, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 256, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 10752}}}, {"id": "chatcmpl-BUndBzuqDKviEkjlno2YF60Mzel3T", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "[CONTENT]\n{\n    \"Required packages\": [\n        \"python>=3.9\",\n        \"torch>=1.13.0\",\n        \"transformers>=4.38.0\",\n        \"sentence-transformers>=2.5.1\",\n        \"datasets>=2.16.0\",\n        \"faiss-cpu>=1.7.4  # swap with faiss-gpu if CUDA available\",\n        \"numpy>=1.24.0\",\n        \"scipy>=1.10.0\",\n        \"tqdm>=4.66.0\",\n        \"sacrebleu>=2.4.2\",\n        \"rouge-score>=0.1.2\",\n        \"bert-score>=0.3.13\",\n        \"evaluate>=0.4.1\",\n        \"pyyaml>=6.0\"\n    ],\n    \"Required Other language third-party packages\": [\n        \"No third-party dependencies required\"\n    ],\n    \"Logic Analysis\": [\n        [\n            \"config.py\",\n            \"Config: parses YAML/CLI, stores global hyper-parameters. Imported by ALL other modules; must be implemented first.\"\n        ],\n        [\n            \"utils.py\",\n            \"Common helpers: seed_everything(), flatten_dict(), length_limit(), logging setup. Used by dataset_loader, chunker, evaluation, etc.\"\n        ],\n        [\n            \"dataset_loader.py\",\n            \"DatasetLoader: depends on 'datasets' pkg and Config. Returns List[dict] with keys id, question, context, answer. Provides iter_dataloader() to stream batches.\"\n        ],\n        [\n            \"chunker.py\",\n            \"Chunker: depends on Config and a Hugging-Face tokenizer passed from Encoder or Generator. Splits long context into fixed-size overlapping chunks; purely CPU string / token ops.\"\n        ],\n        [\n            \"encoder.py\",\n            \"Encoder: wraps sentence-transformers model; embeds batches of chunks -> torch.Tensor. Exports l2_norm(). Needs Config for model name, device. Depends on utils.seed_everything for deterministic behaviour.\"\n        ],\n        [\n            \"bayesian_filter.py\",\n            \"BayesianFilter: core novelty logic. Input (chunks, embeddings) \u2192 accepted_chunks. Maintains mu, sigma internal state + running context vector. Depends on numpy, torch; imports Config for priors.\"\n        ],\n        [\n            \"prompt_builder.py\",\n            \"PromptBuilder: tokenises accepted chunks, assembles final prompt, truncates to Config.max_prompt_tokens. Needs generator tokenizer to count tokens; no heavy deps.\"\n        ],\n        [\n            \"generator.py\",\n            \"Generator: wraps facebook/bart-large (or any HF seq2seq) and performs beam-search generation. Exposes answer(prompt:str)->str. Depends on transformers, Config.\"\n        ],\n        [\n            \"retrieval.py\",\n            \"Retrieval (optional): FAISS index building + top-k search. Used only when Config.retrieval.enabled is true. Depends on encoder for passage embeddings. Upstream for chunker & filter.\"\n        ],\n        [\n            \"evaluation.py\",\n            \"Evaluator: BLEU (sacrebleu), ROUGE-L, BERTScore, Perplexity (loads EleutherAI/gpt-neo-125M). Consumes preds & refs. Depends on Config for metric toggles.\"\n        ],\n        [\n            \"main.py\",\n            \"Entry point CLI. Parses args, loads Config, creates DatasetLoader, SEQAPipeline (defined inside main or utils_pipeline.py) that internally instantiates Chunker, Encoder, Filter, PromptBuilder, Generator. Loops over dataset, writes answers.jsonl, then Evaluation produces metrics.json.\"\n        ]\n    ],\n    \"Task list\": [\n        \"config.py\",\n        \"utils.py\",\n        \"dataset_loader.py\",\n        \"chunker.py\",\n        \"encoder.py\",\n        \"bayesian_filter.py\",\n        \"prompt_builder.py\",\n        \"generator.py\",\n        \"retrieval.py\",\n        \"evaluation.py\",\n        \"main.py\"\n    ],\n    \"Full API spec\": \"\",\n    \"Shared Knowledge\": \"Config singleton object is imported everywhere to avoid passing long argument lists; utils.logger configured once in main.py. HF tokenizer instance created in generator.py and passed to chunker & prompt_builder to guarantee token count consistency. Embeddings are always l2-normalised torch.FloatTensor; shape (n_chunks, dim). BayesianFilter uses utils.length_limit() to check v_sel is non-zero. Seed for torch, numpy set via utils.seed_everything(seed=Config.seed).\",\n    \"Anything UNCLEAR\": \"Still need confirmation on (1) novelty score formula sign; (2) posterior update policy (all vs accepted chunks); (3) exact fields for TriviaQA long context; (4) sentence-transformer checkpoint name for perfect parity; (5) recommended GPU memory to hold BART + GPT-Neo for parallel scoring.\"\n}\n[/CONTENT]", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680321, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 1219, "prompt_tokens": 15722, "total_tokens": 16941, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 192, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}}}, {"id": "chatcmpl-BUndZJl5YUNKrwxedy0WXbtt01xSK", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "## Code: config.yaml\n```yaml\n## config.yaml\n# Only hyper-parameters explicitly reported in the paper are included.\n# The paper does NOT describe any model training or fine-tuning, so\n# learning-rate / epochs / batch-size are intentionally omitted.\n\nexperiment:\n  datasets:\n    - hotpotqa_distractor\n    - triviaqa_rc\n  random_seed: 42  # paper does not state; choose common default for reproducibility\n\nchunking:\n  chunk_size: 300        # (\u00a73.1, \u00a74.3)\n  overlap: 30            # (\u00a73.1, \u00a74.3)\n\nbayesian_filter:\n  mu0: 0.5               # (\u00a74.3 prior mean)\n  sigma0: 0.1            # (\u00a74.3 prior std)\n  sigma_obs: 0.05        # (\u00a74.3 observation noise variance)\n  beta: 1.0              # (\u00a73.2 Eq. 7, margin coefficient)\n\ngenerator:\n  model_name: facebook/bart-large   # (\u00a74.2)\n  num_beams: 4                      # (\u00a74.3 \u201cbeam size of 2 or 4\u201d; we adopt 4)\n  max_generation_length: 128        # (\u00a74.3)\n  encoder_max_tokens: 1024          # (\u00a74.3, upper limit used in some runs)\n\nretrieval:             # optional; defaults match \u201cSEQA (Chunk-Only)\u201d\n  enabled: false\n  top_k: 3                           # (\u00a74.2 \u201ccommonly k = 3\u201d)\n\nevaluation:\n  metrics:\n    - BLEU\n    - ROUGE_L\n    - BERTScore\n    - Perplexity\n```", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680345, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 961, "prompt_tokens": 16899, "total_tokens": 17860, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 576, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 13824}}}]