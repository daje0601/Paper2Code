[{"id": "chatcmpl-BUnmr3hdPX1yaPPIyIT9FA1OwEqAU", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "evaluation.py  \u2011  Logic Analysis\n================================\n\nObjective  \n---------  \nImplement `Evaluator` that reproducibly computes the four *generative* QA metrics mandated by the paper (\u00a74.4) and specified in `config.yaml`:\n\n1. BLEU  (SacreBLEU v2.4.2)  \n2. ROUGE-L  (rouge-score 0.1.2)  \n3. BERTScore F1  (bert-score 0.3.13; model **roberta-large**)  \n4. Perplexity  (negative-log-likelihood based on **EleutherAI/gpt-neo-125M**)\n\nThe class **MUST** follow the interface declared in the design:\n\n```\nclass Evaluator:\n    +__init__(cfg:Config)\n    +score(preds:List[str], refs:List[str]) -> dict\n```\n\nNo other public methods or attributes may be exposed.\n\n\nHigh-Level Algorithm  \n--------------------\n\n```\n__init__\n\u2502 1. persist Config reference\n\u2502 2. parse cfg.evaluation.metrics list into a boolean flags dict\n\u2502 3. lazily load heavy artefacts (GPT-Neo, BERTScore IDF stats) ONLY for\n\u2502    metrics that are actually requested; keep handles on self.*\n\u2514\u25004. set device = 'cuda' if torch.cuda.is_available() else 'cpu'\n```\n\n```\nscore(preds, refs)\n\u2502 1. sanity check: len(preds) == len(refs); raise ValueError otherwise\n\u2502 2. for each metric flag invoke the corresponding _compute_* helper\n\u2502 3. collect scalar results in an OrderedDict that respects the\n\u2502    order declared in cfg.evaluation.metrics\n\u2514\u25004. return the dict, e.g.\n      {\"BLEU\": 5.17, \"ROUGE_L\": 0.0421, \"BERTScore\": 0.7894, \"Perplexity\": 278.6}\n```\n\nMetric-Specific Logic  \n---------------------\n\n### 1. BLEU ( _sacrebleu_ )\n\n\u2022 Use `sacrebleu.corpus_bleu(preds, [refs], tokenize='13a', lowercase=False)`.  \n\u2022 Output the *score* attribute (already percentage).  \n\u2022 SacreBLEU handles tokenisation internally; **no ad-hoc preprocessing** permitted\u2014this exactly mirrors paper\u2019s evaluation consistency.  \n\u2022 The function is deterministic; no seed required.\n\nHelper:\n\n```python\ndef _compute_bleu(self, preds, refs):\n    import sacrebleu\n    bleu = sacrebleu.corpus_bleu(preds, [refs], tokenize=\"13a\", lowercase=False)\n    return bleu.score          # float\n```\n\n### 2. ROUGE-L ( _rouge_score_ )\n\n\u2022 `rouge-score` returns precision/recall/f1; keep F1.  \n\u2022 Need to loop over pairs \u2192 accumulate F1, then mean.  \n\u2022 Tokeniser: same default Porter stemmer used in library; do **not** lower-case unless library does.  \n\u2022 Implementation:\n\n```python\ndef _compute_rouge(self, preds, refs):\n    from rouge_score import rouge_scorer\n    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    scores = [scorer.score(r, p)['rougeL'].fmeasure  # NB order\n              for p, r in zip(preds, refs)]\n    return float(np.mean(scores))\n```\n\n### 3. BERTScore ( _bert_score_ )\n\n\u2022 Call `bert_score.score` with:  \n  \u2013 `model_type='roberta-large'` (paper default)  \n  \u2013 `lang='en'`  \n  \u2013 `rescale_with_baseline=True` for stability (common practice).  \n\u2022 Returns P/R/F1 tensors; index [2] for F1; take `.mean().item()`.  \n\u2022 GPU use if available.  \n\u2022 Load occurs once during first call; heavy.\n\n```python\ndef _compute_bertscore(self, preds, refs):\n    from bert_score import score as bs_score\n    P, R, F1 = bs_score(preds, refs,\n                        model_type=\"roberta-large\",\n                        lang=\"en\",\n                        rescale_with_baseline=True,\n                        verbose=False,\n                        device=self.device)\n    return float(F1.mean().item())\n```\n\n### 4. Perplexity (GPT-Neo 125M)\n\nDefinition (paper \u00a74.4): feed generated answer into GPT-Neo-125M and compute exponentiated average negative log-likelihood per token.\n\nImplementation steps:\n\n```\n1. Load tokenizer & model ('EleutherAI/gpt-neo-125M'), padding_side='left'\n2. Model.eval(); no_grad(); half precision if cuda & cfg.fp16_perplexity flag (optionally)\n3. Batch answers to avoid OOM (batch_size configurable; default 8)\n4. For each batch:\n     a. token_ids = tokenizer(batch, return_tensors=\"pt\", padding=True)\n     b. shift tokens: labels = token_ids.clone(); labels[padded_tokens] = -100\n     c. loss = model(input_ids, attention_mask, labels=labels).loss       # averaged per token\n     d. ppl = torch.exp(loss)\n5. Final PPL = mean(list_of_ppl).cpu().item()\n```\n\nNote: We compute perplexity **only on predictions** (not reference) as done in paper (they speak of *\u201cgauge fluency of outputs\u201d*).  \n\nEdge Cases:  \n\u2022 Empty predictions \u21d2 treat as \u201c<eos>\u201d token to avoid NaN.  \n\u2022 Long answers that exceed model context (2048 tokens) are truncated automatically by tokenizer.\n\nHelper:\n\n```python\ndef _compute_perplexity(self, preds):\n    import torch\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    if not hasattr(self, \"_ppl_model\"):\n        tok = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n        mdl = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n        mdl.to(self.device); mdl.eval()\n        self._ppl_tokenizer, self._ppl_model = tok, mdl\n\n    batch_size = 8\n    ppl_scores = []\n    for i in range(0, len(preds), batch_size):\n        batch = preds[i:i+batch_size]\n        enc = self._ppl_tokenizer(batch, return_tensors=\"pt\",\n                                  padding=True, truncation=True).to(self.device)\n        labels = enc[\"input_ids\"].clone()\n        labels[enc[\"attention_mask\"] == 0] = -100\n        with torch.no_grad():\n            loss = self._ppl_model(**enc, labels=labels).loss\n        ppl_scores.append(torch.exp(loss).cpu())\n    return float(torch.mean(torch.stack(ppl_scores)).item())\n```\n\nComputational Notes  \n-------------------\n\n\u2022 BLEU & ROUGE negligible.  \n\u2022 BERTScore is GPU-heavy (roberta-large); approx 2\u20133 GB VRAM for 1k pairs.  \n\u2022 Perplexity: GPT-Neo-125M consumes ~2.5 GB VRAM in fp32; we can adopt fp16 casting if CUDA + A100/RTX but keep as fp32 by default to avoid overflow.  \n\nResource fallback:\nIf `self.device == 'cpu'`, both BERTScore and Perplexity still work but are slow; emit a `warnings.warn` message.\n\nRandomness / Reproducibility  \n-----------------------------\n\n\u2022 Provide `utils.seed_everything(cfg.random_seed)` call inside __init__ to ensure deterministic behaviour of torch/numpy.  \n\u2022 SacreBLEU & ROUGE deterministic by design.  \n\u2022 BERTScore has internal torch operations; seed guarantees repeatability.\n\nEdge-Condition Handling  \n-----------------------\n\n\u2022 Non-ASCII characters handled transparently by underlying libs (all Unicode aware).  \n\u2022 Mismatched prediction/ref lengths already caught by sanity check.  \n\u2022 Empty dataset \u21d2 raise ValueError (caller side).\n\nTime Complexity  \n---------------\n\nLet N = #examples.\n\n| Metric      | Complexity | Bottleneck                           |\n|-------------|------------|--------------------------------------|\n| BLEU        | O(N)       | negligible                           |\n| ROUGE-L     | O(N\u00b7L)     | where L=avg tokens per answer        |\n| BERTScore   | O(N\u00b7L)     | heavy \u2011 passes through Roberta-large |\n| Perplexity  | O(N\u00b7L)     | heavy \u2011 passes through GPT-Neo       |\n\nWe therefore batch BERTScore & PPL to fully utilise GPU.\n\nInternal Attributes Summary  \n---------------------------\n\n```\nself.cfg                # Config singleton\nself.device             # 'cuda' or 'cpu'\nself._bert_score_model  # cached tuple P,R,F1 (actually handled inside bert-score)\nself._ppl_model         # GPT-Neo model (lazy)\nself._ppl_tokenizer     # GPT-Neo tokenizer (lazy)\nself.metrics_flags      # {'BLEU':True, 'ROUGE_L':True, ...}\n```\n\nPseudocode Skeleton  \n-------------------\n\n```python\nclass Evaluator:\n    def __init__(self, cfg: Config):\n        self.cfg = cfg\n        utils.seed_everything(cfg.experiment.random_seed)\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        # flags\n        wanted = {m.upper(): True for m in cfg.evaluation.metrics}\n        self.metrics_flags = {k: wanted.get(k, False) for k in\n                              (\"BLEU\", \"ROUGE_L\", \"BERTSCORE\", \"PERPLEXITY\")}\n\n        # Lazy placeholders\n        self._ppl_model = self._ppl_tokenizer = None\n        # no preload for bert-score; library handles caching\n\n    def score(self, preds: List[str], refs: List[str]) -> dict:\n        if len(preds) != len(refs):\n            raise ValueError(\"pred/ref length mismatch\")\n\n        results = collections.OrderedDict()\n        if self.metrics_flags[\"BLEU\"]:\n            results[\"BLEU\"] = self._compute_bleu(preds, refs)\n        if self.metrics_flags[\"ROUGE_L\"]:\n            results[\"ROUGE_L\"] = self._compute_rouge(preds, refs)\n        if self.metrics_flags[\"BERTSCORE\"]:\n            results[\"BERTScore\"] = self._compute_bertscore(preds, refs)\n        if self.metrics_flags[\"PERPLEXITY\"]:\n            results[\"Perplexity\"] = self._compute_perplexity(preds)\n\n        return results\n```\n\nUnit-Testing Strategy  \n---------------------\n\n1. Hard-code two trivial examples:  \n   \u2013 pred = \u201chello world\u201d, ref = \u201chello world\u201d   \u2794 BLEU \u2248 100, ROUGE-L \u2248 1, BERTScore > 0.99, PPL reasonable.  \n2. Input arrays length mismatch should raise exception.  \n3. Toggle individual metrics in `config.yaml` and assert skipped ones are absent in output dict.\n\nAll these tests live in `tests/test_evaluator.py` (not part of current task list but assists future maintenance).\n\nCompliance Check  \n----------------\n\n\u2713  100 % aligned with **Data structures and interfaces**: we only provide `__init__` and `score`.  \n\u2713  All hyper-parameters (model names, batch sizes, seed) obtained from `config.yaml` or sensible defaults *without* introducing hidden knobs.  \n\u2713  Uses only packages enumerated in \u201cRequired packages\u201d.  \n\u2713  No deviation from paper\u2019s metric definitions.  \n\u2713  Reproducibility ensured via fixed seed & deterministic tokenisation pipelines.\n\nThis completes the logic analysis needed to code `evaluation.py`.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680921, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 2906, "prompt_tokens": 15806, "total_tokens": 18712, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 384, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}}}]