[{"id": "chatcmpl-BUndtH30LQUinsdcqCI2tbEkW1RjL", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "config.py  \u2011 Logic Analysis\n===========================\n\nRole in the system  \n------------------  \nConfig is the *first* module that must be imported by every other file so that all runtime parameters (chunk length, Bayesian priors, model names, etc.) are defined **exactly** as in `config.yaml`, i.e. as reported in the paper.  No module is allowed to hard-code a constant: every hyper-parameter must be obtained from this singleton.\n\nPaper traceability  \n------------------  \n\u2022 chunk_size, overlap \u2192 \u00a73.1 / \u00a74.3  \n\u2022 \u03bc\u2080, \u03c3\u2080, \u03c3_obs, \u03b2     \u2192 \u00a73.2 Eq.(7) & \u00a74.3  \n\u2022 model_name, num_beams, max_generation_length, encoder_max_tokens \u2192 \u00a74.2 / \u00a74.3  \n\u2022 retrieval.top_k       \u2192 \u00a74.2  \nAll of these are **explicitly** provided in `config.yaml`; config.py must forbid inventing anything else.\n\nKey design requirements (from \u201cData structures and interfaces\u201d)  \n---------------------------------------------------------------  \nclass Config  \n    + model_name:str  \n    + chunk_size:int  \n    + overlap:int  \n    + bayes_mu0:float  \n    + bayes_sigma0:float  \n    + bayes_sigma_obs:float  \n    + bayes_beta:float  \n    + max_prompt_tokens:int   \u2190 derived from generator.encoder_max_tokens  \n    + beam:int                \u2190 num_beams  \n    + load(path:str) \u2192 Config\n\nImplementation strategy  \n-----------------------  \n\n1. **Singleton pattern**  \n   \u2022 class variable `_instance` caches the object returned by `load()` so subsequent `Config()` calls just return the same reference.  \n   \u2022 prevents silent divergence if modules import Config at different moments.\n\n2. **YAML parsing** (`pyyaml`)  \n   \u2022 `load(path)` reads the YAML file once, stores raw dict.  \n   \u2022 Throws `FileNotFoundError` or `KeyError` with *clear* messages if any section/field is missing.  \n\n3. **Attribute mapping & validation**  \n   \u2022 Flat public attributes exactly as the interface diagram; values copied from nested dict:  \n       self.chunk_size           = y['chunking']['chunk_size']  \n       self.bayes_mu0            = y['bayesian_filter']['mu0']  \n       \u2026 etc.  \n   \u2022 `assert` statements (or custom `validate()`) to guarantee:  \n       0 < overlap < chunk_size, \u03c30>0, \u03c3_obs>0, beta>0, encoder_max_tokens >= chunk_size*2  (paper used 300, 1024 so OK).  \n\n4. **CLI override** (optional but useful)  \n   \u2022 `argparse` with `--config` (default `config.yaml`) plus `--set key=value` style overrides.  \n   \u2022 Only keys already in YAML may be overridden; extra keys raise an error -> preserves paper fidelity while still allowing ablations.\n\n5. **Exposure of nested sub-configs**  \n   \u2022 although public flats already exist, keep the original nested dict in `self.raw` for advanced modules (e.g. `evaluation.py` needs list of metrics).  \n   \u2022 provide `get(path:str, default=None)` helper where path is dot-separated (`evaluation.metrics`).\n\n6. **Device, seed and env**  \n   \u2022 Not in YAML (paper is silent), but pipeline needs them. Because rule #5 says \u201cDo not invent new public members\u201d, device/seed utilities live in `utils.py`, not in Config, but Config exposes the seed defined in yaml (`experiment.random_seed`) so utils can consume that.\n\n7. **Immutability**  \n   \u2022 After `validate()`, freeze attributes via `__setattr__` override to prevent accidental mutation during runtime.\n\n8. **max_prompt_tokens logic**  \n   \u2022 The prompt length limit for `PromptBuilder` should be identical to `generator.encoder_max_tokens` by default.  Therefore during `load()` we set  \n        self.max_prompt_tokens = y['generator']['encoder_max_tokens']  \n     so other modules don\u2019t duplicate logic.\n\nInterface compliance check  \n--------------------------  \nAll attributes listed in the design diagram are present; extra helpers (`raw`, `get()`) are private or explicitly documented but **not** referenced by other modules in diagrams, so we stay compliant.\n\nFailure modes & logging  \n-----------------------  \n\u2022 Any missing field \u2192 raise error at startup (fail-fast).  \n\u2022 Any attempt to read Config before `Config.load()` is called in `main.py` \u2192 raises RuntimeError (\u201cConfig not initialised\u201d).  \n\u2022 Logging of parsed values (INFO) once, so experiment hyper-parameters are recorded in stdout/stderr logs.\n\nEdge cases addressed  \n--------------------  \n\u2022 YAML numbers may load as int or float; casting done explicitly.  \n\u2022 If retrieval.enabled == False, retrieval.top_k may still exist; Config keeps it but downstream modules ignore.  \n\nOutcome  \n-------  \n`config.py` delivers a rigorously validated, immutable singleton that exactly mirrors the paper\u2019s hyper-parameters and provides a single point of truth for every other component in the reproduction codebase.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680365, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 1344, "prompt_tokens": 15782, "total_tokens": 17126, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 256, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 10624}}}]