[{"id": "chatcmpl-BUnlYmSnkbpEwqq0YlZ9dqOTXLr2a", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "# retrieval.py  \u2013  Logic Analysis\n#\n# Role within overall design\n# --------------------------\n# This module is *optional* and only activated when `Config.retrieval.enabled`\n# is True.  In the \u201cSEQA (With Index)\u201d variant described in the paper (\u00a74.2),\n# a light\u2010weight vector search retrieves **top-k passages** *before* the\n# chunk-and-Bayesian-filter pipeline runs.  The Retrieval class therefore has\n# three responsibilities:\n#\n#   1.  Build (or load) a FAISS index over a corpus of passages that are\n#       potential sources of information for QA.\n#   2.  Keep a mapping from FAISS row-ids  \u2192  raw passage text  (and, if\n#       desired, meta-data such as title or document id).\n#   3.  Provide a `retrieve(query, k) -> List[str]` method that returns the\n#       **raw texts** of the k most similar passages, using the *same* sentence\n#       encoder that the rest of the pipeline employs for chunk novelty\n#       scoring.  The caller can then concatenate those passages (or process\n#       them individually) and feed them into `Chunker` + `BayesianFilter`.\n#\n# Alignment with \u201cData structures and interfaces\u201d\n# -----------------------------------------------\n# According to the design, Retrieval must expose:\n#\n#     class Retrieval:\n#         +__init__(cfg:Config)\n#         +index_corpus(corpus_path:str) -> None\n#         +retrieve(query:str, k:int) -> List[str]\n#\n# \u2022  `__init__` receives the global `Config` instance and optionally an already\n#    initialised `Encoder` so that we do **not** load the sentence-transformer\n#    twice.  If an Encoder object is *not* handed in, Retrieval will create its\n#    own instance configured identically to `encoder.py`; this keeps the public\n#    signature simple while avoiding duplicated GPU memory usage when the\n#    pipeline supplies one.\n#\n# \u2022  `index_corpus` reads a text file (or directory, or Hugging-Face dataset)\n#    specified by `corpus_path`.  For every atomic *passage* (granularity is a\n#    design choice, see below) it:\n#\n#       \u2013  Calls `Encoder.embed()` to obtain an L2-normalised embedding.\n#       \u2013  Appends that vector to a growing NumPy array.\n#       \u2013  Stores the original text (and optional meta-data) in\n#          `self._id_to_text[row_id]`.\n#\n#    After all passages are processed the method initialises a FAISS\n#    `IndexFlatIP` (inner product \u21d2 cosine similarity because vectors are\n#    normalised) and adds the array of passage vectors.  If\n#    `Config.retrieval.save_path` is present, the FAISS index and the\n#    id-to-text mapping are serialised to disk so that subsequent runs can\n#    simply `faiss.read_index()` instead of rebuilding.\n#\n# \u2022  `retrieve` guarantees that the index is loaded/built (`self._ready == True`)\n#    and then encodes the **query string** with the same Encoder.  It calls\n#\n#          D, I = self._index.search(q_vec[np.newaxis, \u2026], k)\n#\n#    and returns `[self._id_to_text[i] for i in I[0] if i != -1]`.\n#\n# Design Decisions & Paper Fidelity\n# ---------------------------------\n# 1. **Encoder re-use**  \n#      The sentence encoder must be *identical* to the one used for chunk\n#      embeddings so that cosine distances are comparable.  We therefore accept\n#      an `Encoder` instance in `__init__`.  The Retrieval class never performs\n#      an additional `.fit()` step\u2014this aligns with the paper\u2019s statement that\n#      no fine-tuning is performed.\n#\n# 2. **Passage granularity**  \n#      The paper\u2019s description (\u201cretrieve a small set of top-ranked passages\u201d)\n#      is vague.  For HotpotQA & TriviaQA reproduction we will:\n#\n#        \u2022 Split Wikipedia dump pages (or the dataset-provided paragraphs) by\n#          *paragraph boundaries*; each paragraph becomes one passage.\n#        \u2022 Keep passage length \u2264 Config.chunking.chunk_size (300 tokens) so\n#          they already fit BART\u2019s encoder in worst case.\n#\n#      This ensures the vector index is not polluted by excessively long text\n#      and keeps retrieval latency low.\n#\n# 3. **FAISS index type**  \n#      \u2022 `IndexFlatIP` is chosen because it is exact and supports GPU\n#        acceleration (`faiss.index_cpu_to_gpu`) without additional code.  \n#      \u2022 For large corpora one could switch to `IndexIVFFlat`, but exact search\n#        is safer for reproducibility.  This setting is recorded in\n#        `config.yaml` only if changed manually\u2014otherwise defaults apply.\n#\n# 4. **Cosine vs. Inner-product Normalisation**  \n#      All vectors are L2 normalised at creation time (`Encoder.embed()` already\n#      does this), so inner product equals cosine similarity.  This conforms to\n#      the novelty-score definition (\u00a73.2) and ensures metric consistency.\n#\n# 5. **Top-k default**  \n#      Retrieval falls back to `cfg.retrieval.top_k` (default = 3) when the\n#      caller passes `k=None`.  This mirrors the paper\u2019s \u201ccommonly k = 3\u201d.\n#\n# 6. **Batching & Memory**  \n#      The `index_corpus()` implementation embeds passages in mini-batches\n#      (`cfg.embedding.batch_size` if such a field exists, else 64) and streams\n#      them through the encoder to avoid OOM errors when building large\n#      indices.  Embeddings are initially collected in a Python list, then\n#      stacked into a single `np.float32` array for FAISS consumption.\n#\n# 7. **Persistence and Cache Validity**  \n#      Because building an index over full Wikipedia can exceed 30 minutes, the\n#      module checks for a `.faiss` file and a `.jsonl` (id\u2192text) sidecar; if\n#      both exist and `cfg.retrieval.force_rebuild` is False, it loads them.\n#      The logic hashes `cfg.encoder.model_name` and the corpus file path to\n#      detect mismatches and avoid silent reuse of an incompatible index.\n#\n# 8. **Thread-safety / Multi-processing**  \n#      The Retrieval class is intentionally *not* fork-safe because FAISS\n#      indexes are tricky to serialise across processes.  The pipeline should\n#      therefore instantiate Retrieval in the main process and share it across\n#      worker threads if batched retrieval is ever required.  This limitation\n#      is acceptable because top-k search for a single query is ~O(1 ms) for\n#      moderate corpus sizes with `IndexFlatIP`.\n#\n# 9. **Integration Points**  \n#      In `main.py` or `SEQAPipeline`:\n#\n#          if cfg.retrieval.enabled:\n#              retrieval = Retrieval(cfg, encoder)\n#              retrieval.index_corpus(cfg.retrieval.corpus_path)\n#          \u2026\n#          if cfg.retrieval.enabled:\n#              passages = retrieval.retrieve(sample[\"question\"],\n#                                            cfg.retrieval.top_k)\n#              long_document = \" \".join(passages)\n#          else:\n#              long_document = sample[\"context\"]\n#\n#      This string `long_document` is then passed to `Chunker.split()`.\n#\n# Edge Cases & Error Handling\n# ---------------------------\n# \u2022 Calling `retrieve()` before `index_corpus()` raises `RuntimeError`.\n# \u2022 If FAISS returns `-1` (no passage), we simply return an empty list; the\n#   caller must decide how to proceed (e.g., fall back to original context).\n# \u2022 If corpus size < k, we return as many passages as available.\n#\n# Hooks for Future Research\n# -------------------------\n# \u2022 **Re-ranker**: After FAISS search we could insert a cross-encoder\n#   re-ranker.  Not included because the paper explicitly sticks to *vector\n#   similarity*.\n# \u2022 **Adaptive top-k**: \u03b2-dependent threshold on similarity to dynamically\n#   decide k (mirroring Bayesian thresholding).  Out of scope but easy to add.\n#\n# References to `config.yaml`\n# ---------------------------\n#     retrieval:\n#       enabled: false          # toggles use\n#       top_k: 3                # default k\n#       corpus_path: data/wiki_paras.jsonl   # user provided\n#       save_path: data/wiki.faiss           # optional output\n#       force_rebuild: false    # ignore cache\n#\n# Anything outside these keys must *not* be auto-invented to stay faithful to\n# the instruction \u201cDo not invent or assume any values\u201d.\n#\n# End of Logic Analysis for retrieval.py", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680840, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 2816, "prompt_tokens": 15799, "total_tokens": 18615, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 832, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 15616}}}]