```python
## bayesian_filter.py
"""
bayesian_filter.py

Implementation of the BayesianFilter class responsible for adaptive
novelty-based chunk selection as described in Section 3.2 of the paper
“Statistical Estimation for Large-Scale Question Answering”.

The filter operates **online** over a sequence of pre-embedded chunks,
maintaining a Bayesian posterior over novelty scores and a running
context vector.  Chunks whose novelty exceeds an adaptive threshold
are accepted and later passed to the prompt builder.

Public API
----------
class BayesianFilter:
    __init__(cfg: Config)
    reset() -> None
    filter(chunks: List[str], embeds: torch.Tensor) -> List[str]

(All other methods are private helpers and must not be called
directly from outside the class.)
"""

from __future__ import annotations

import math
from typing import List

import torch
import torch.nn.functional as F

from config import Config
from utils import setup_logging

__all__ = ["BayesianFilter"]


class BayesianFilter:  # noqa: WPS110  (name chosen intentionally – domain term)
    """Adaptive Bayesian novelty filter for document chunks."""

    # ------------------------------------------------------------------ #
    # Construction
    # ------------------------------------------------------------------ #
    def __init__(self, cfg: Config) -> None:
        """
        Parameters
        ----------
        cfg:
            Immutable experiment-wide configuration object loaded from
            *config.yaml*.  Hyper-parameters relevant to this class:

            • cfg.bayes_mu0          (float)  – prior mean  μ₀
            • cfg.bayes_sigma0       (float)  – prior std-dev σ₀
            • cfg.bayes_sigma_obs    (float)  – observation noise variance σ²_obs
            • cfg.bayes_beta         (float)  – margin coefficient β
        """
        self._logger = setup_logging(cfg, name=self.__class__.__name__)

        # --- Hyper-parameters (validated) --------------------------------- #
        self._mu0: float = float(getattr(cfg, "bayes_mu0", 0.5))
        self._sigma0: float = float(getattr(cfg, "bayes_sigma0", 0.1))
        self._sigma_obs: float = float(getattr(cfg, "bayes_sigma_obs", 0.05))
        self._beta: float = float(getattr(cfg, "bayes_beta", 1.0))

        if self._sigma0 <= 0:
            raise ValueError("bayes_sigma0 must be > 0.")
        if self._sigma_obs <= 0:
            raise ValueError("bayes_sigma_obs must be > 0.")
        if self._beta <= 0:
            raise ValueError("bayes_beta must be > 0.")

        # ---- Internal mutable state -------------------------------------- #
        self._mu: float = self._mu0  # posterior mean      μ
        self._sigma: float = self._sigma0  # posterior std-dev σ
        self._sigma2_obs: float = self._sigma_obs  # variance (keep as var)
        self._v_sel: torch.Tensor | None = None  # running context vector
        self._n_sel: int = 0  # how many chunks accepted so far

        # Private flag for ablation studies (update posterior on all vs
        # accepted chunks only).  *Not* part of the public interface.
        self._update_on_all: bool = True

        self._eps: float = 1.0e-8  # numeric stability in cosine

        self._logger.debug(
            "BayesianFilter initialised with μ0=%.4f, σ0=%.4f, σ²_obs=%.4f, β=%.3f",
            self._mu0,
            self._sigma0,
            self._sigma2_obs,
            self._beta,
        )

    # ------------------------------------------------------------------ #
    # Public helpers
    # ------------------------------------------------------------------ #
    def reset(self) -> None:
        """Restore the filter to its **prior** state (fresh document)."""
        self._mu = self._mu0
        self._sigma = self._sigma0
        self._v_sel = None
        self._n_sel = 0
        self._logger.debug("Filter state reset to prior.")

    # ------------------------------------------------------------------ #
    # Main API method
    # ------------------------------------------------------------------ #
    def filter(self, chunks: List[str], embeds: torch.Tensor) -> List[str]:
        """Select a subset of *chunks* deemed sufficiently novel.

        Parameters
        ----------
        chunks:
            List of raw chunk strings (length K).
        embeds:
            Tensor of shape (K, d) containing **L2-normalised**
            embeddings on CPU *or* GPU.

        Returns
        -------
        List[str]
            Ordered list of accepted chunk strings.
        """
        if not isinstance(chunks, list):
            raise TypeError("`chunks` must be List[str].")
        if embeds.ndim != 2:
            raise ValueError("`embeds` must be a 2-D tensor (K × d).")
        if len(chunks) != embeds.shape[0]:
            raise ValueError("Length mismatch between chunks and embeds.")

        # (Re-)initialise state for this document
        self.reset()

        accepted: List[str] = []
        device = embeds.device  # ensure all tensors live on same device

        for idx, (chunk_txt, h_k) in enumerate(zip(chunks, embeds)):  # noqa: WPS359
            # FIRST CHUNK: always accept ----------------------------------- #
            if self._v_sel is None:
                accepted.append(chunk_txt)
                self._v_sel = h_k.clone()
                self._n_sel = 1

                # Update posterior with dummy novelty (set to 1.0 > μ0 usually)
                x_k = float(1.0)
                self._update_posterior(x_k)
                self._logger.debug(
                    "Accepted first chunk idx=%d (bootstrap). μ=%.4f σ=%.4f",
                    idx,
                    self._mu,
                    self._sigma,
                )
                continue

            # Ensure running vector is unit-norm (might accumulate drift)
            self._v_sel = F.normalize(self._v_sel, p=2, dim=0)

            # NOVELTY SCORE (cosine distance) ------------------------------ #
            cos_sim = torch.dot(h_k, self._v_sel).clamp(-1.0, 1.0).item()
            x_k = 1.0 - cos_sim  # higher = more novel

            # Update posterior (all chunks by default) --------------------- #
            if self._update_on_all:
                self._update_posterior(x_k)

            # Adaptive threshold (using *updated* posterior) --------------- #
            if self._accept(x_k):
                # Accept chunk
                accepted.append(chunk_txt)
                # Running mean update of context vector
                self._v_sel = (
                    (self._n_sel * self._v_sel.to(device)) + h_k
                ) / (self._n_sel + 1)
                self._n_sel += 1
                # Normalise after update
                self._v_sel = F.normalize(self._v_sel, p=2, dim=0)

                # If posterior should only track accepted chunks,
                # perform update here instead.
                if not self._update_on_all:
                    self._update_posterior(x_k)

                self._logger.debug(
                    "Accepted chunk idx=%d  x=%.4f  τ=%.4f  (μ=%.4f, σ=%.4f)",
                    idx,
                    x_k,
                    self._mu + self._beta * self._sigma,
                    self._mu,
                    self._sigma,
                )
            else:
                self._logger.debug(
                    "Rejected chunk idx=%d  x=%.4f  τ=%.4f  (μ=%.4f, σ=%.4f)",
                    idx,
                    x_k,
                    self._mu + self._beta * self._sigma,
                    self._mu,
                    self._sigma,
                )

        # Safeguard: ensure at least *one* chunk survives
        if not accepted and chunks:
            self._logger.warning(
                "No chunk passed the novelty threshold; "
                "falling back to keeping the first chunk."
            )
            accepted.append(chunks[0])

        return accepted

    # ------------------------------------------------------------------ #
    # Private helpers
    # ------------------------------------------------------------------ #
    def _update_posterior(self, x: float) -> None:
        """Online conjugate update of Normal prior with known variance."""
        sigma2_prior = self._sigma ** 2
        sigma2_post = 1.0 / (1.0 / sigma2_prior + 1.0 / self._sigma2_obs)
        mu_post = sigma2_post * (
            self._mu / sigma2_prior + x / self._sigma2_obs
        )

        # Convert variance back to std-dev
        self._sigma = math.sqrt(max(sigma2_post, 1e-12))
        self._mu = mu_post

    def _accept(self, x: float) -> bool:
        """Decision rule: accept if x ≥ μ + βσ."""
        threshold = self._mu + self._beta * self._sigma
        return x >= threshold
```