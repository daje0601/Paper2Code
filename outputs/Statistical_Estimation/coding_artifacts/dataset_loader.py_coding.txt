```python
## dataset_loader.py
"""
dataset_loader.py

Implementation of the `DatasetLoader` class declared in the project
design.  It provides a *thin* abstraction layer over Hugging-Face
`datasets` so that the remainder of the code-base can treat every QA
benchmark through a uniform dictionary schema:

    {
        "id":       str,   # unique sample identifier
        "question": str,   # natural-language query
        "context":  str,   # (very) long document(s) merged
        "answer":   str    # reference answer (single string)
    }

The loader supports eager `.load()` as well as streaming
`.iter_dataloader()` access, covers all datasets cited in the target
paper (HotpotQA *Distractor* and TriviaQA *RC*) and is trivially
extensible via the internal `_DATASET_REGISTRY` table.

The heavy lifting of **chunking**, **embedding** and **Bayesian
filtering** is *not* performed here; the loader is strictly responsible
for fetching and normalising raw data.

Usage Example
-------------
>>> from config import Config
>>> from dataset_loader import DatasetLoader
>>> cfg = Config.load("config.yaml")
>>> dl  = DatasetLoader(cfg, dataset_key="hotpotqa_distractor")
>>> samples = dl.load()
>>> print(samples[0].keys())
dict_keys(['id', 'question', 'context', 'answer'])
"""

from __future__ import annotations

import itertools
import random
import unicodedata
from typing import Dict, Iterator, List, Sequence

from datasets import load_dataset  # type: ignore

from config import Config
from utils import setup_logging, seed_everything

# --------------------------------------------------------------------------- #
# Dataset-specific parsing helpers                                            #
# --------------------------------------------------------------------------- #


def _normalise(text: str) -> str:
    """Apply minimal Unicode normalisation and strip whitespace."""
    return unicodedata.normalize("NFC", text).strip()


# ---------- HotpotQA (distractor) ------------------------------------------ #


def _parse_hotpot(example: Dict[str, object]) -> Dict[str, str]:
    """Convert a single *raw* HotpotQA row to the canonical schema."""
    # Raw schema reference:
    # https://huggingface.co/datasets/hotpot_qa
    ctx_segments: List[str] = []
    for title, paragraph in example.get("context", []):  # type: ignore[arg-type]
        if not paragraph:
            # Skip empty paragraphs gracefully.
            continue
        # Title prefix provides weak structure (use Markdown header style).
        ctx_segments.append(f"## {title}\n{paragraph}")

    long_document = "\n\n".join(ctx_segments)
    if not long_document:
        long_document = " "  # avoid an empty context string downstream

    ans_field = example.get("answer")
    # Some HF versions wrap answers in dicts; others are plain strings.
    if isinstance(ans_field, dict):
        answer_str: str = ans_field.get("spans", ans_field.get("text", ""))  # type: ignore[arg-type]
        if isinstance(answer_str, list):
            # Choose first answer if multiple.
            answer_str = answer_str[0] if answer_str else ""
    else:
        answer_str = str(ans_field)

    return {
        "id": str(example.get("id")),
        "question": _normalise(str(example.get("question", ""))),
        "context": _normalise(long_document),
        "answer": _normalise(answer_str),
    }


# ---------- TriviaQA (RC) --------------------------------------------------- #


def _parse_trivia(example: Dict[str, object]) -> Dict[str, str]:
    """Convert a single *raw* TriviaQA row to the canonical schema."""
    # Raw schema reference:
    # https://huggingface.co/datasets/trivia_qa
    # Extract extended context hierarchy: context > search_results > entity_pages
    if example.get("context"):
        # 'context' is usually a list[str] of snippets
        paragraphs = example["context"]  # type: ignore[index]
        long_document = "\n\n".join(p for p in paragraphs if p)
    elif example.get("search_results"):
        snippets = [
            r.get("snippet", "")  # type: ignore[arg-type]
            for r in example["search_results"]  # type: ignore[index]
            if r.get("snippet")
        ]
        long_document = "\n\n".join(snippets)
    else:
        # Fallback to entity_pages paragraphs
        paragraph_list: List[str] = []
        for page in example.get("entity_pages", []):  # type: ignore[arg-type]
            paragraph_list.extend(page.get("paragraphs", []))
        long_document = "\n\n".join(paragraph_list)

    if not long_document:
        long_document = " "  # safeguard against empty contexts

    # Answers in HF TriviaQA are dicts with `value` and possibly aliases.
    ans_field = example.get("answer")
    if isinstance(ans_field, dict):
        answer_str = ans_field.get("value", "")
    elif isinstance(ans_field, list):
        answer_str = ans_field[0] if ans_field else ""
    else:
        answer_str = str(ans_field)

    qid = example.get("question_id") or example.get("id") or hash(
        example.get("question", "")
    )

    return {
        "id": str(qid),
        "question": _normalise(str(example.get("question", ""))),
        "context": _normalise(long_document),
        "answer": _normalise(str(answer_str)),
    }


# --------------------------------------------------------------------------- #
# Dataset registry â€“ add new datasets here                                    #
# --------------------------------------------------------------------------- #
_DATASET_REGISTRY: Dict[str, Dict[str, object]] = {
    "hotpotqa_distractor": {
        "hf_name": "hotpot_qa",
        "hf_config": "distractor",
        "split": "validation",
        "parser": _parse_hotpot,
        "expected_len": 7405,  # reference value for sanity check
    },
    "triviaqa_rc": {
        "hf_name": "trivia_qa",
        "hf_config": "rc",
        "split": "validation",
        "parser": _parse_trivia,
        "expected_len": 11313,
    },
}


# --------------------------------------------------------------------------- #
# Main public class                                                           #
# --------------------------------------------------------------------------- #
class DatasetLoader:  # noqa: WPS110 (name chosen deliberately)
    """Dataset abstraction that hides HF implementation quirks."""

    # --------------------------------------------------------------------- #
    # Construction
    # --------------------------------------------------------------------- #
    def __init__(self, cfg: Config, dataset_key: str, *, split: str | None = None) -> None:
        """
        Parameters
        ----------
        cfg:
            Global configuration object (singleton).
        dataset_key:
            String identifier from ``_DATASET_REGISTRY``.  Typically one
            of the keys listed in *config.yaml* under ``experiment.datasets``.
        split:
            Dataset split (default: registry entry or "validation").
        """
        self._cfg = cfg
        self._meta = _DATASET_REGISTRY.get(dataset_key)
        if self._meta is None:
            raise KeyError(
                f"Unknown dataset_key='{dataset_key}'.  "
                f"Available keys: {list(_DATASET_REGISTRY.keys())}"
            )

        self._split = split or str(self._meta.get("split", "validation"))
        self._logger = setup_logging(cfg, name=f"DatasetLoader[{dataset_key}]")
        self._parser = self._meta["parser"]  # type: ignore[assignment]

        # Ensure deterministic behaviour (e.g. for shuffling later on)
        seed_everything(cfg.random_seed)

        self._logger.info(
            "Initialised DatasetLoader for %s (split=%s).", dataset_key, self._split
        )

    # --------------------------------------------------------------------- #
    # Public loading helpers
    # --------------------------------------------------------------------- #
    def load(self) -> List[Dict[str, str]]:
        """Eagerly load **all** samples into memory and return a list."""
        self._logger.info("Loading dataset eagerly into memory ...")
        # download + load
        dataset = load_dataset(
            path=self._meta["hf_name"],  # type: ignore[arg-type]
            name=self._meta["hf_config"],  # type: ignore[arg-type]
            split=self._split,
            streaming=False,
        )

        samples: List[Dict[str, str]] = [
            self._parser(raw)  # type: ignore[arg-type]
            for raw in dataset  # type: ignore[arg-type]
        ]

        self._post_load_stats(samples, mode="eager")
        self._sanity_checks(samples)
        return samples

    def iter_dataloader(
        self,
        *,
        batch_size: int = 1,
        shuffle: bool = False,
    ) -> Iterator[List[Dict[str, str]]]:
        """Yield batches of canonicalised samples (streaming or in-memory).

        Parameters
        ----------
        batch_size:
            Number of items yielded per iteration.  Set ``batch_size==1`` for
            true streaming mode (memory efficient).
        shuffle:
            Whether to iterate in a shuffled order; deterministic given the
            random seed from *Config*.
        """
        if batch_size <= 0:
            raise ValueError("`batch_size` must be a positive integer.")

        # Decide between streaming vs eager underlying dataset
        streaming = batch_size == 1 and not shuffle
        dataset = load_dataset(
            path=self._meta["hf_name"],  # type: ignore[arg-type]
            name=self._meta["hf_config"],  # type: ignore[arg-type]
            split=self._split,
            streaming=streaming,
        )

        if streaming:
            # Iteratively parse and yield batches of size 1 (no shuffle)
            it = map(self._parser, dataset)  # type: ignore[arg-type]
            yield from (([item] for item in it))
            return  # streaming path ends here

        # ----- Non-streaming branch: load into memory first --------------- #
        buffer: List[Dict[str, str]] = [
            self._parser(ex)  # type: ignore[arg-type]
            for ex in dataset  # type: ignore[arg-type]
        ]

        if shuffle:
            random.Random(self._cfg.random_seed).shuffle(buffer)

        # Chunk into batches
        for idx in range(0, len(buffer), batch_size):
            yield buffer[idx : idx + batch_size]  # noqa: WPS362 (slice OK)

    # --------------------------------------------------------------------- #
    # Private helpers
    # --------------------------------------------------------------------- #
    def _post_load_stats(self, samples: Sequence[Dict[str, str]], *, mode: str) -> None:
        """Log basic corpus statistics for sanity checking."""
        n = len(samples)
        avg_len = int(
            sum(len(s["context"]) for s in samples) / n if n else 0,
        )
        self._logger.info(
            "Loaded %d samples (%s mode).  Avg context chars: %d",
            n,
            mode,
            avg_len,
        )

    def _sanity_checks(self, samples: Sequence[Dict[str, str]]) -> None:
        """Lightweight validation that parsed examples look reasonable."""
        exp_len = self._meta.get("expected_len")
        if isinstance(exp_len, int) and len(samples) != exp_len:
            self._logger.warning(
                "Dataset length mismatch: expected %d, got %d.",
                exp_len,
                len(samples),
            )

        # Ensure essential keys present and non-empty
        for key in ("id", "question", "context", "answer"):
            if any(key not in s or not isinstance(s[key], str) for s in samples):
                raise RuntimeError(f"Parsed samples missing mandatory key '{key}'.")

        # Check there is at least some context
        if any(len(s["context"].strip()) == 0 for s in samples):
            self._logger.warning("One or more samples have empty context strings.")

```