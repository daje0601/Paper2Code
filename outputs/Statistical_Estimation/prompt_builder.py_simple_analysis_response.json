[{"id": "chatcmpl-BUnk2Zags4iXPS6qfKM5Gh6oi21LU", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "prompt_builder.py \u2013 Logic Analysis\n----------------------------------\n\nObjective  \nImplement class PromptBuilder conforming exactly to the interface defined in \u201cData structures and interfaces\u201d:\n\nclass PromptBuilder  \n    + __init__(cfg: Config, tokenizer: PreTrainedTokenizerBase)  \n    + build(question: str, accepted_chunks: List[str]) -> str\n\nNo additional public methods are allowed.\n\nKey functionalities to cover  \n1. Assemble the textual prompt that will be fed into facebook/bart-large.  \n2. Make sure the total number of tokens (as counted by the SAME tokenizer used by the generator) never exceeds the hard limit given in `config.yaml` under `generator.encoder_max_tokens`.  \n3. If the limit would be exceeded, truncate *only* the context portion (i.e. the list of accepted chunks).  The question and the system / instruction lines must stay intact because they are essential for correct generation.  \n4. Keep the order of accepted chunks (oldest \u2192 newest) unless the limit forces truncation.  In that case we drop chunks from the *tail* (i.e. least-recently accepted) so that the model still gets the earliest evidence, which typically contains high-level background information.  This is a deterministic rule and therefore reproducible.  \n5. Edge-case handling: if `accepted_chunks` is empty, insert a default placeholder line (\u201cNo additional context was deemed novel.\u201d) so that the prompt structure is always well-formed.  \n6. Implementation must be side-effect free: the method `build()` returns a *python str* object; it does not mutate in-place any argument nor touch global state.  \n7. All numeric constants must come exclusively from `Config` or be fixed textual literals specified by the paper; **never invent hidden magic numbers**.\n\nDetailed step-by-step algorithm\n-------------------------------\n\nGiven:\n\u2022 cfg: Config instance (singleton)  \n\u2022 tokenizer: PreTrainedTokenizerBase already loaded by Generator and passed in (ensures identical token counting).  \n\u2022 question: str  \n\u2022 accepted_chunks: List[str] (plain text, already filtered by BayesianFilter in original order of acceptance)\n\nLet\n  ROLE_LINE       = \"You are a helpful QA system.\"\n  QUESTION_LINE   = f\"The user asks: {question}\"\n  CONTEXT_HEADER  = \"Below is relevant context filtered from a larger document:\"\n  ENDING_LINE     = \"Please provide the best possible answer.\"\n\nTokenisation helper  \n    def _count_tokens(text: str) -> int:\n        return len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n\nConstruction procedure  \n1. Build *fixed* skeleton (header) \u2192 HEADER_TEXT =\n        ROLE_LINE + \"\\n\" +\n        QUESTION_LINE + \"\\n\" +\n        CONTEXT_HEADER + \"\\n\"\n\n2. Build *tail* instruction \u2192 TAIL_TEXT =\n        \"\\n\" + ENDING_LINE\n\n3. Determine **budget** for context:\n        max_prompt_tokens = cfg.generator.encoder_max_tokens     # 1024 by default\n        header_tokens = _count_tokens(HEADER_TEXT)\n        tail_tokens   = _count_tokens(TAIL_TEXT)\n        budget = max_prompt_tokens - header_tokens - tail_tokens\n   (budget \u2265 0 is guaranteed because encoder_max_tokens in config should be \u2265 both header+tail, but assert to be safe)\n\n4. Fill context greedily from `accepted_chunks`\n   INITIALISE: selected_chunks = []; running_len = 0\n   FOR chunk in accepted_chunks:\n        tok_len = _count_tokens(chunk) + 1   # +1 for the \u201c\\n\u201d that will join chunks\n        IF running_len + tok_len > budget:\n            BREAK      # stop adding; we have hit the limit\n        selected_chunks.append(chunk)\n        running_len += tok_len\n\n   After the loop:\n       IF selected_chunks is empty:\n            selected_chunks.append(\"No additional context was deemed novel.\")\n\n5. Join everything:\n        context_text = \"\\n\".join(selected_chunks)\n        prompt = HEADER_TEXT + context_text + TAIL_TEXT\n\n6. **Optional sanity check** (useful for unit tests but not mandatory in production):\n        total_tokens = _count_tokens(prompt)\n        assert total_tokens <= max_prompt_tokens, \\\n            f\"Prompt length {total_tokens} exceeds limit {max_prompt_tokens}\"\n\n7. Return `prompt`.\n\nCorner cases & decisions justified against the paper\n----------------------------------------------------\n\u2022 Truncation strategy is not specified in the paper; keeping earliest chunks aligns with paper\u2019s emphasis on preserving \u201ccontinuity across chunk boundaries\u201d (\u00a73.1) and \u201csuccinct yet representative context\u201d (\u00a7Abstract).  \n\u2022 We do **not** attempt to individually truncate long chunks, because chunk size is already tightly bounded at 300 tokens (cfg.chunking.chunk_size). Dropping whole chunks keeps semantic integrity.  \n\u2022 The header / tail text exactly mirrors the example wording in \u00a73.3. This maintains faithfulness without introducing new instructions that might bias generation.  \n\u2022 We rely on the Config field `generator.encoder_max_tokens`; if a future experiment changes this value, PromptBuilder will automatically respect it without code modification.  \n\u2022 No external packages beyond `transformers` are necessary; therefore prompt_builder.py will only `import typing`, `from .config import Config`, and `from transformers import PreTrainedTokenizerBase`.\n\nUnit-test outline (to be placed elsewhere)\n------------------------------------------\n1. With encoder_max_tokens=50, supply 10 dummy chunks each 10 tokens long.  Verify that build() returns a prompt \u226450 tokens and that only first 3-4 chunks are kept.  \n2. With empty accepted_chunks list.  Confirm placeholder text is present and total length \u2264 limit.  \n3. High-limit scenario: supply many chunks such that total tokens < limit \u2192 assert all chunks appear.  \n\nIntegration points\n------------------\n\u2022 `Generator` loads the tokenizer and hands it to PromptBuilder (`pipeline` does the wiring).  \n\u2022 `BayesianFilter` already outputs accepted_chunks in `List[str]` form.  \n\u2022 PromptBuilder is purely string manipulation; no heavy dependencies \u2192 minimal overhead in the runtime pipeline.\n\nWith this logic analysis, the implementation of prompt_builder.py can proceed directly, guaranteed to satisfy the design contract while adhering strictly to the experimental setup described in the paper and the config.yaml settings.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680746, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 1831, "prompt_tokens": 15792, "total_tokens": 17623, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 512, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 15616}}}]