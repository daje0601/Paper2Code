[{"id": "chatcmpl-BUnffYrdzqPJTC8ZAiieyIMmEMRFj", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "dataset_loader.py  \u2011 Logic Analysis\n================================================\n\nObjective  \n---------  \nImplement a DatasetLoader class that serves a *single* responsibility: load and standardise QA samples coming from Hugging-Face datasets so the rest of the pipeline can operate without caring about dataset-specific quirks.  It must:\n\n1. Use configuration values from `Config` (parsed from `config.yaml`) \u2013 **never hard-code** dataset names or paths.  \n2. Return samples in the canonical Python dict format:  \n\n   ```python\n   {\n       \"id\": str,           # unique question ID\n       \"question\": str,     # natural-language query\n       \"context\": str,      # very long document(s) merged into one\n       \"answer\": str        # reference answer string (first answer if list)\n   }\n   ```  \n\n3. Provide:\n   \u2022 `load()`    \u2192 returns `List[dict]` (eager) *or*  \n   \u2022 `iter_dataloader(batch_size=1, shuffle=False)` \u2192 generator / iterator (streaming)  \n\n   The streaming iterator is crucial for low-RAM machines because contexts can be >50K tokens.\n\n4. Support at least the two datasets used in the paper:\n\n   a. `hotpotqa_distractor` (validation split)  \n   b. `triviaqa_rc`        (validation split)\n\n   Future datasets can be added with minimal code changes.\n\n--------------------------------------------------------------------\nKey Design Decisions\n--------------------------------------------------------------------\n\nA.  How to identify dataset & split  \n   *Config* will store *string keys* exactly matching HF hub identifiers:  \n\n   ```\n   experiment.datasets:\n     - hotpotqa_distractor\n     - triviaqa_rc\n   ```\n\n   We parse each key into  \n   \u2022 `base_name`   \u2013 e.g. \"hotpot_qa\" or \"trivia_qa\"  \n   \u2022 `config_name` \u2013 e.g. \"distractor\" or \"rc\"  \n   Mapping logic will live in a small registry dict inside DatasetLoader:\n\n   ```python\n   _DATASET_REGISTRY = {\n       \"hotpotqa_distractor\": {\n           \"hf_name\": \"hotpot_qa\",\n           \"hf_config\": \"distractor\",\n           \"split\": \"validation\",\n           \"parser\": _parse_hotpot            # dataset-specific function\n       },\n       \"triviaqa_rc\": {\n           \"hf_name\": \"trivia_qa\",\n           \"hf_config\": \"rc\",\n           \"split\": \"validation\",\n           \"parser\": _parse_trivia\n       }\n   }\n   ```\n\nB.  Parsing rules per dataset  \n   1. HotpotQA / distractor  \n\n      HF fields:  \n      ```\n      {\n          'id': str,\n          'question': str,\n          'context': List[[title:str, paragraph:str]],\n          'answer': str,\n          ...\n      }\n      ```  \n\n      \u2022 Merge **all** paragraphs (gold + distractor) in the order provided by dataset.  \n      \u2022 Prepend the Wikipedia title before each paragraph to preserve weak structure, e.g.  \n        `\"## {title}\\n{paragraph}\\n\\n\"`  \n      \u2022 Join all into one giant string.  \n      \u2022 Return first element of 'answer' if it is a list; else the string itself.\n\n   2. TriviaQA / rc  \n\n      The HF RC split exposes:  \n      ```\n      {\n          'question': str,\n          'answer': {'value': str, ...},\n          'context': List[str]  # may not exist!\n          'search_results': List[{'snippet': str, ...}],\n          'entity_pages': List[{'paragraphs': List[str], ...}]\n      }\n      ```  \n\n      *Ambiguity in the paper:* which context field is used.  \n      Resolution (principle: include **extended snippets**):  \n\n      \u2022 If `'context'` key exists and is non-empty \u2192 use it.  \n      \u2022 Else fallback to `'search_results'` snippets (common in HF processing).  \n      \u2022 Else fallback to concatenating all paragraphs from `entity_pages`.  \n\n      Implementation detail:\n\n      ```python\n      def _parse_trivia(example):\n          q = example['question']\n          ans = example['answer']['value'] if isinstance(example['answer'], dict) else example['answer']\n          if example.get('context'):\n              long_doc = \"\\n\\n\".join(example['context'])\n          elif example.get('search_results'):\n              long_doc = \"\\n\\n\".join(sr['snippet'] for sr in example['search_results'] if sr['snippet'])\n          else:\n              pages = example.get('entity_pages', [])\n              para_list = []\n              for p in pages:\n                  para_list.extend(p.get('paragraphs', []))\n              long_doc = \"\\n\\n\".join(para_list)\n          return {'question': q, 'context': long_doc, 'answer': ans, 'id': example.get('id', hash(q))}\n      ```\n\n      We document this choice in README because the paper is silent.\n\nC.  Streaming vs eager loading  \n   \u2022 For `iter_dataloader`, we read the HF dataset in *streaming* mode (`datasets.load_dataset(..., streaming=True)`) if `batch_size==1`.  \n   \u2022 For eager `load()`, we fully materialise the validation split (\u22487k + 11k samples; safe for RAM).\n\nD.  Deterministic shuffling  \n   \u2022 We respect `Config.random_seed` when `shuffle=True` by seeding Python `random` & NumPy RNGs.\n\nE.  Field normalisation  \n   \u2022 Strip leading/trailing whitespace on `question`, `context`, `answer`.  \n   \u2022 Ensure Unicode normalisation (NFC) to avoid later tokenizer surprises.\n\n--------------------------------------------------------------------\nInterface Specification\n--------------------------------------------------------------------\n\n```python\nclass DatasetLoader:\n    def __init__(self, cfg: Config, dataset_key: str):\n        \"\"\"\n        dataset_key \u2013 one of the strings in cfg.experiment.datasets\n        \"\"\"\n        self.cfg = cfg\n        self.meta = _DATASET_REGISTRY[dataset_key]\n\n    def load(self) -> List[Dict[str, str]]:\n        \"\"\"\n        Return entire split in memory.\n        Raises KeyError if dataset_key unknown.\n        \"\"\"\n        ...\n\n    def iter_dataloader(self,\n                        batch_size: int = 1,\n                        shuffle: bool = False\n                        ) -> Iterator[List[Dict[str, str]]]:\n        \"\"\"\n        Streaming iterator that yields lists of length <= batch_size.\n        If shuffle is True it first loads indices into memory, shuffles\n        with cfg.random_seed, then iterates deterministically.\n        \"\"\"\n        ...\n```\n\n*NB:* The registry parsers (`_parse_hotpot`, `_parse_trivia`, etc.) are implemented as `@staticmethod` or nested functions and are not part of the public interface.\n\n--------------------------------------------------------------------\nAlgorithmic Steps (per dataset sample)\n--------------------------------------------------------------------\n\n1. Read raw example from HF dataset.  \n2. Apply dataset-specific parser \u2192 canonical dict.  \n3. Optionally validate that `context` length (in characters) > `answer` length to catch parsing errors.  \n4. Yield or append.\n\n--------------------------------------------------------------------\nEdge-case Handling\n--------------------------------------------------------------------\n\n\u2022 **Empty or None context** \u2013 skip sample and log warning via `utils.logger`.  \n\u2022 **Multiple reference answers** \u2013 choose first answer; store full list under hidden key `_all_answers` if needed for EM/F1 ablation.  \n\u2022 **Overly long strings** \u2013 no limit here; truncation handled later by Chunker.  \n\u2022 **Encoding errors** \u2013 dataset loader enforces `example.encode('utf-8', errors='replace')`.\n\n--------------------------------------------------------------------\nLogging & Monitoring\n--------------------------------------------------------------------\n\nAt construction time, log:\n\n```\nLoaded HotpotQA-distractor validation: 7405 samples\nAvg raw context chars: 14532\n```\n\nAll warnings/errors also funnel to the central logger.\n\n--------------------------------------------------------------------\nReproducibility Considerations\n--------------------------------------------------------------------\n\n\u2022 The loader does **not** shuffle by default (`shuffle=False`) to guarantee deterministic sample order (important for caching accepted chunk positions later).  \n\u2022 When `shuffle=True`, the seed from `cfg.experiment.random_seed` ensures repeatability.\n\n--------------------------------------------------------------------\nDependencies\n--------------------------------------------------------------------\n\n```python\nfrom datasets import load_dataset\nimport numpy as np, random, typing, itertools\nfrom .config import Config\nfrom .utils import logger, seed_everything\n```\n\nNo Torch or Transformers dependence here \u2013 keeps dataloader lightweight.\n\n--------------------------------------------------------------------\nValidation Tests (unit-tests)\n--------------------------------------------------------------------\n\n1. `assert len(dl.load()) == expected_count` (7405, 11313).  \n2. First sample keys are present and non-empty.  \n3. Context string contains *at least* one newline separator.  \n4. Iterator mode with `batch_size=8` returns exactly `ceil(N/8)` batches.  \n5. Shuffle vs non-shuffle produce different order but same set of ids.\n\n--------------------------------------------------------------------\nOpen Issues / Ambiguities Acknowledged\n--------------------------------------------------------------------\n\n1. TriviaQA long context source \u2013 we selected a deterministic hierarchy (`context` > `search_results` > `entity_pages`).  Will document for review.  \n2. Names of HF fields can change in future versions; we will pin dataset version (`datasets==2.16.0`) and add graceful fallback logging.  \n3. Some TriviaQA answers are *lists* not dicts; parser must handle both.\n\n--------------------------------------------------------------------\nInteraction with Downstream Modules\n--------------------------------------------------------------------\n\n\u2022 Output dict directly consumed by `SEQAPipeline.run(sample)` (main.py).  \n\u2022 No tokenisation is done here; that is Chunker\u2019s responsibility.  \n\u2022 Id field is propagated to evaluation so prediction files can be matched to refs.\n\n--------------------------------------------------------------------\nConclusion\n--------------------------------------------------------------------\n\nThis logic ensures DatasetLoader is:\n\n\u2022 faithful to the paper\u2019s datasets and splits,  \n\u2022 agnostic to downstream processing details,  \n\u2022 memory-efficient via streaming,  \n\u2022 reproducible through Config and seeded shuffling,  \n\u2022 extensible to new datasets by adding one registry entry.\n\nAll other pipeline components can rely on its strict interface, avoiding any dataset-specific branching elsewhere in the codebase.", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680475, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 2503, "prompt_tokens": 15796, "total_tokens": 18299, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 384, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 15616}}}]