[{"id": "chatcmpl-BUnnhfHp2UW6rKauHoW4Dy2RSw6Cn", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "```python\n\"\"\"\nmain.py \u2013 LOGIC ANALYSIS / BLUE-PRINT  *no executable code yet*\n\nThe purpose of this file is to map, step-by-step, how the CLI entry\npoint orchestrates every module in the architecture specified in the\ndesign doc.  The real implementation will later replace the \u201cpass\u201d or\npseudo-code blocks with working code, but **MUST NOT** deviate from the\ndata structures, method signatures or the config-driven behaviour that\nhave already been finalised.\n\nAll decisions, control-flow, object lifetimes, and I/O paths are\ndocumented here so that coding can proceed mechanically and reviewers /\ncollaborators can cross-check that we remain faithful to\n\n    \u2022 the research paper\u2019s methodology,\n    \u2022 the reproduction plan,\n    \u2022 the class/sequence diagrams in \u201cData structures and interfaces\u201d,\n    \u2022 the hyper-parameters in config.yaml.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nHIGH-LEVEL RESPONSIBILITIES OF main.py\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n1.  Parse CLI arguments (config path, target dataset, output folder,\n    optional overrides such as --device or --retrieval).\n\n2.  Load YAML into a Config singleton via Config.load().\n    This also exposes every hyper-parameter to the rest of the code.\n\n3.  Call utils.seed_everything(cfg.random_seed) immediately to lock down\n    numpy / torch / random reproducibility.\n\n4.  Instantiate the DatasetLoader for the requested split (\u201cvalidation\u201d)\n    and dataset name (either \u201chotpotqa_distractor\u201d or \u201ctriviaqa_rc\u201d).\n\n5.  Build the SEQAPipeline object which wires together:\n\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502Chunker \u2502  \u2190 receives tokenizer from Generator\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u25b2\n             \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502Encoder \u2502  (sentence-transformer)\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u25b2\n             \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502BayesianFilter\u2502  (keeps state per document)\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u25b2\n             \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502PromptBuilder \u2502  \u2190 same tokenizer instance\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u25b2\n             \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502Generator\u2502  (facebook/bart-large; owns tokenizer, device)\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n             \u25b2\n             \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502Retrieval (opt) \u2502  (only if cfg.retrieval.enabled)\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    NOTE:  The pipeline owns **one** tokenizer instance (from\n    Generator) in order to keep token counting consistent between\n    chunking, prompt-building, and generation, as required by the\n    plan.\n\n6.  Iterate through the dataset with tqdm progress bar:\n        for sample in dataset:\n            result = pipeline.run(sample)\n            \u2022 result is a dict {id, pred, ref}\n            \u2022 Append to in-memory list for later evaluation\n            \u2022 Stream write each result as JSONL to\n              f\"{out_dir}/answers_{dataset_name}.jsonl\"\n      The run() method encapsulates the per-document logic:\n         - chunker.split on sample[\"context\"]\n         - encoder.embed on those chunks\n         - bayesian_filter.filter to pick accepted chunks\n         - prompt_builder.build to get the text prompt\n         - generator.answer(prompt) to generate the answer\n         - (optional) retrieval.retrieve if enabled BEFORE chunking\n      All this follows exactly the sequence diagram in the design.\n\n7.  After finishing the loop call Evaluator.score(preds, refs) to obtain\n    the metrics specified in config[\"evaluation\"][\"metrics\"]:\n        { \"BLEU\": value, \"ROUGE_L\": value, \"BERTScore\": value,\n          \"Perplexity\": value }\n\n8.  Serialize the metrics dict to\n        f\"{out_dir}/metrics_{dataset_name}.json\"\n\n9.  Exit with return code 0 so that shell scripts can detect success.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nDETAILED FLOW  (corresponds one-to-one with Program call flow diagram)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nCLI (argparse)        \u2013->  cfg = Config.load(yaml_path)\n                         \u2514\u2500> utils.seed_everything(cfg.random_seed)\n                         \u2514\u2500> dataset = DatasetLoader(cfg, 'validation').load()\n\n# Instantiate pipeline ONCE \u2013 it keeps models on GPU/CPU memory\n                         \u2514\u2500> pipeline = SEQAPipeline(cfg)\n\nLOOP over dataset:\n                         \u2514\u2500> result = pipeline.run(sample)\n                                internally:\n                                   chunks    = Chunker.split(context)\n                                   embeds    = Encoder.embed(chunks)\n                                   accepted  = BayesianFilter.filter(chunks, embeds)\n                                   prompt    = PromptBuilder.build(question, accepted)\n                                   answer    = Generator.answer(prompt)\n                                   return {\"id\": sample[\"id\"],\n                                           \"pred\": answer.strip(),\n                                           \"ref\": sample[\"answer\"]}\n                         \u2514\u2500> write JSONL line, accumulate preds, refs\n\nAfter loop:            \u2514\u2500> metrics = Evaluator(cfg).score(preds, refs)\n                         \u2514\u2500> json.dump(metrics, metrics_path)\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nIMPORTANT EDGE-CASES & IMPLEMENTATION NOTES\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2022  Dataset iteration must RE-INITIALISE a fresh BayesianFilter for EACH\n   document (HotpotQA sample == one QA pair with possibly many\n   paragraphs).  State is NOT carried across samples.\n\n\u2022  Device handling:\n     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n     All heavy modules (Encoder, Generator, optional Retrieval) must be\n     moved to that device.  main.py decides the device once and passes\n     it down via Config or constructor arguments (not via global var).\n\n\u2022  Retrieval flag:\n     If cfg.retrieval.enabled is false (default for \u201cSEQA Chunk-Only\u201d)\n     skip *all* Retrieval calls to avoid extra overhead.\n     If true   \u2013 instantiate Retrieval once, reuse across samples.\n\n\u2022  Streaming output:\n     The JSONL writer should be opened with buffering to avoid losing\n     data on unexpected crashes.  Consider writing every N lines.\n\n\u2022  Memory:\n     Do **NOT** store tokenised chunks or embeddings for *all* samples\n     at once.  The loop processes and discards per sample.\n\n\u2022  Metrics compute:\n     Evaluator.score expects *lists* of hypothesis strings and\n     reference strings (same ordering).  No tensors are kept.\n\n\u2022  Conformity:\n     All method calls used in main.py **must** exist in the class\n     diagrams under \u201cData structures and interfaces\u201d.  We explicitly\n     avoid inventing new public methods.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nCLI SIGNATURE (proposed, still within allowed flexibility)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n$ python main.py \\\n      --config  config.yaml \\\n      --dataset hotpotqa_distractor \\\n      --out_dir results/ \\\n      [--device cuda:0] \\\n      [--retrieve]         # toggles cfg.retrieval.enabled True\n\nThe flag \u201c--retrieve\u201d is *only* a convenience to override the YAML;\ninternally we update the Config instance after loading YAML but before\nthe pipeline is built.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nMAPPING TO PAPER\u2019S EXPERIMENT SECTION\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2022  Chunk size / overlap, Bayesian priors, beam-size, gen length,\n   retrieval top-k, encoder max tokens \u2013 all read directly from\n   config.yaml \u2192 satisfies   \u201cAlways reference settings from the\n   config.yaml file. Do not invent or assume any values\u201d.\n\n\u2022  Dataset split \u2013 always \u201cvalidation\u201d as done in the paper.\n\n\u2022  Metrics \u2013 Evaluator exactly computes BLEU, ROUGE-L, BERTScore,\n   Perplexity.  EM/F1 can be added later but are not required for core\n   reproduction.\n\n\u2022  Random seed = 42 (default) to keep deterministic behaviour between\n   runs as recommended for reproducibility.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nFAILURE MODES & PLANNED EXCEPTIONS\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2022  If prompt token count > cfg.generator.encoder_max_tokens:\n       \u2013 PromptBuilder must truncate accepted_chunk list **from the\n         tail** (oldest chunk first) until length fits.\n       \u2013 main.py is NOT responsible; PromptBuilder guarantees safety.\n\n\u2022  If GPU OOM when loading Generator:\n       \u2013 main.py falls back to cpu by catching the RuntimeError and\n         re-instantiating on \u201ccpu\u201d.  This keeps the program usable on\n         laptops (slow but functional).\n\n\u2022  KeyboardInterrupt during dataset loop:\n       \u2013 Ensure open file handles are closed via try/except/finally.\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nCHECKLIST FOR IMPLEMENTATION\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2611  argparse wired with defaults from YAML but allows overrides  \n\u2611  Config singleton accessible via \u201cfrom config import Config\u201d  \n\u2611  utils.seed_everything called exactly once  \n\u2611  DatasetLoader returns list[dict] with {id, question, context, answer}  \n\u2611  SEQAPipeline.run returns dict with keys id, pred, ref (strict)  \n\u2611  No layer breaks encapsulation (e.g., main.py never calls internal\n   BayesianFilter methods directly)  \n\u2611  After loop, Evaluator.score is invoked with *all* preds/refs  \n\u2611  answers_*.jsonl and metrics_*.json written under out_dir  \n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nEND OF LOGIC ANALYSIS \u2013 main.py\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\"\"\"\n```", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746680973, "model": "o3-2025-04-16", "object": "chat.completion", "service_tier": "default", "system_fingerprint": null, "usage": {"completion_tokens": 2580, "prompt_tokens": 15817, "total_tokens": 18397, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 512, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 15616}}}]