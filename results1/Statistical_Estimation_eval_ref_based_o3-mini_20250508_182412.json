{"paper_name": "Statistical_Estimation", "target_repo_dir": "../outputs/Statistical_Estimation_repo", "eval_type": "ref_based", "gold_repo_dir": "", "generated_n": 8, "request_json": {"model": "o3-mini", "messages": [{"role": "system", "content": "You will be given a research paper along with two corresponding code repositories: a gold repository and a target repository.\n\nYour task is to compare the target repository against the gold repository, rate the target repository on one metric, and provide a critique highlighting key differences.\n\nPlease make sure you read and understand these instructions carefully. Keep this document open while reviewing, and refer to it as needed.\n\n---\n\nEvaluation Criteria:\n\nCorrectness (1-5): The quality of the target repository in accurately implementing the paper\u2019s concepts, methodology, and algorithms without logical errors, as compared to the gold repository. Additionally, provide a critique focusing on the completeness, accuracy, and implementation choices made in the target repository relative to the gold repository.\n\n1: Very Poor. The target repository does not correctly implement the core concepts, methodology, or algorithms from the paper. Major logical errors or missing components are present, especially when compared to the gold repository.\n2: Poor. The target repository attempts to implement the paper\u2019s concepts but contains significant mistakes or missing components, making the implementation incorrect when compared to the gold repository.\n3: Fair. Some core components and concepts are correctly implemented in the target repository, but there are notable logical errors or inaccuracies compared to the gold repository.\n4: Good. The target repository correctly implements the key components and methodology, with only minor inaccuracies or deviations from the gold repository.\n5: Excellent. The target repository fully and accurately implements all relevant key components, methodology, and algorithms from the paper, matching the quality of the gold repository.\n\n---\n\nEvaluation Steps  \n\n1. Identify Key Aspects of the Paper: Carefully read the research paper to understand its core concepts, methodology, and algorithms. Pay close attention to the key aspects that are crucial for implementing the paper\u2019s results (e.g., specific algorithms, data preprocessing steps, evaluation protocols).\n\n2. Analyze the Gold Repository: Examine the gold repository to understand how these key aspects have been implemented. Use the gold repository as a reference for how the paper\u2019s methodology should be translated into code. Note the completeness, accuracy, and design choices in the gold repository that faithfully represent the paper\u2019s concepts.\n\n3. Examine the Target Repository: Analyze the target repository to assess how well it implements the key aspects of the paper. Reference the gold repository as a guide for understanding these key aspects in the target repository. Focus on whether the target repository\u2019s core logic, algorithms, and structure align with the methodology and experiments described in the paper.\n\n4. Identify Logical Errors and Deviations: Check for logical errors, missing steps, or deviations from the paper\u2019s methodology. Note any incorrect representations, inconsistencies, or incomplete implementations that could affect the correctness of the target repository.\n\n5. Provide a Critique: Consider both the completeness and accuracy of the implementation relative to the paper\u2019s goals and the gold repository\u2019s standard. You do not need to analyze minor details like logging functions, script organization, or documentation quality. Instead, concentrate on the correctness of the logic and implementation that ensures the core concepts from the paper are fully reflected in the target repository. For each mismatch or deviation in implementation, note down specific critiques comparing relevant functions in the target repository to the corresponding functions in the gold repository. Highlight incorrect logic, missing steps, or deviations that affect the correct implementation of the paper\u2019s methodology.\n\n5. Assess the Correctness: Determine whether the target repository includes all the critical elements described in the paper and implemented in the gold repository. Identify missing components, significant deviations, or incorrect implementations that could affect the correctness of the target repository.\n\n6. Assign a Score: Based on your evaluation, provide a critique and assign a correctness score from 1 to 5 for the target repository, reflecting how well it implements the key aspects of the paper refer to the gold repository. Include a detailed critique in the specified JSON format.\n\n\n---\n\nSeverity Level:  \n\nEach identified critique will be assigned a severity level based on its impact on the correctness of the methodology implementation.  \n\n- High: Missing or incorrect implementation of the paper\u2019s core concepts, major loss functions, or experiment components that are fundamental to reproducing the paper\u2019s methodology.  \n  - Example: The main algorithm is missing or fundamentally incorrect.  \n- Medium: Issues affecting training logic, data preprocessing, or other core functionalities that significantly impact performance but do not completely break the system.  \n  - Example: Improper training loop structure, incorrect data augmentation, or missing essential components in data processing.  \n- Low: Errors in specific features that cause deviations from expected results but can be worked around with modifications. Any errors in the evaluation process belong to this category unless they impact the core concepts. These include minor issues like logging, error handling mechanisms, configuration settings, evaluation steps that do not alter the fundamental implementation and additional implementations not explicitly stated in the paper.\n  - Example: Suboptimal hyperparameter initialization, incorrect learning rate schedule, inaccuracies in evaluation metrics, using a different random seed, variations in batch processing, different weight initialization, issues in result logging or reporting, variations in evaluation dataset splits, improper error handling in non-critical steps, mismatches in secondary evaluation criteria, or additional implementation details not specified in the paper that do not interfere with core results.\n\n---\n\nExample JSON format:\n```json\n{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"preprocessing.py\",\n            \"gold_func_name\": \"data_process\",\n            \"target_file_name\": \"dataset.py\",\n            \"target_func_name\": \"train_preprocess\",\n            \"severity_level\": \"medium\",\n            \"critique\": \"A critique of the target repository's file with reference to the gold repository.\"\n        }, \n        {\n            \"gold_file_name\": \"utils.py\",\n            \"gold_func_name\": \"calculate_metric\",\n            \"target_file_name\": \"metric.py\",\n            \"target_func_name\": \"f1_at_k\"\n            \"severity_level\": \"low\",\n            \"critique\": \"A critique of the target repository's file with reference to the gold repository.\"\n        },\n    ],\n    \"score\": 2\n}\n```\n\n---\n\nSample:\n\nResearch Paper:\n\n{'page_1': \"Statistical Estimation for Large-Scale Question Answering:\\nInferring Comprehensive Context from Partial Chunks\\nJinbeom Kang\\n* 1\\nAbstract\\nLarge Language Models (LLMs) have shown re-\\nmarkable performance on Question Answering\\n(QA) but are fundamentally limited by their \\x02-\\nnite context windows, which can be exceeded\\nby extensive documents such as multi-page con-\\ntracts or long Wikipedia articles. We propose a\\nstatistical estimation framework\\ndesigned to\\nhandle large-scale QA by selectively aggregating\\nonly the most informative chunks of text. Our\\napproach integrates three key components:\\nAdap-\\ntive Chunk Sizing\\nto dynamically segment docu-\\nments based on local content density,\\nDynamic\\nThresholding\\nthat employs Bayesian updates to\\n\\x02lter redundant chunks, and a\\nCLT-based Statis-\\ntical Context Estimation\\nmechanism to approxi-\\nmate the entire document's embedding. Unlike\\ntraditional extractive metrics (EM/F1), we rely\\non\\ngenerative\\nQA metrics (BLEU, ROUGE, Per-\\nplexity) to capture both correctness and \\x03uency\\nin open-ended responses. Experimental results on\\nbenchmarks such as HotpotQA, TriviaQA, and\\nNatural Questions demonstrate that our method\\nnot only reduces computational overhead but also\\nimproves generative QA quality over baselines\\n(e.g., FiD, RAG). This highlights how a princi-\\npled, yet \\x03exible, statistical perspective can ef-\\nfectively overcome context-window constraints in\\nmodern LLMs.\\n1. Introduction\\nRecent advances in Large Language Models (LLMs) such\\nas GPT-4 have propelled open-domain question answering\\n(QA) systems to new performance levels, surpassing human-\\ncrafted baselines in a variety of tasks including summariza-\\n*\\nEqual contribution\\n1\\nDepartment of XXX, University of\\nYYY, Location, Country. Correspondence to: Jinbeom Kang\\n<\\njbkang@infobank.net\\n>\\n.\\nProceedings of the\\n41\\nst\\nInternational Conference on Machine\\nLearning\\n, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\\nby the author(s).\\ntion, machine translation, and multi-turn dialogue. Despite\\nthis remarkable progress, these models continue to be con-\\nstrained by \\x02nite context windows, often ranging from a few\\nthousand to tens of thousands of tokens. While extended\\ncontext capabilities (e.g., 32k tokens) provide partial relief,\\nreal-world settings such as multi-chapter textbooks, lengthy\\nlegal contracts, and intricate medical records can easily ex-\\nceed these limits, leaving the model blind to signi\\x02cant\\nsegments of text and consequently producing incomplete or\\nsuboptimal answers.\\nA common solution is to break the source text into smaller\\nchunks\\nthat individually \\x02t into the model's context window.\\nHowever, naive chunking strategies introduce two critical\\nissues. First, important details can be dispersed across mul-\\ntiple chunks, a phenomeno n we refer to as\\ncontext frag-\\nmentation\\n. This fragmentation undermines the coherence\\nnecessary for complex reasoning, as it forces the model to\\npiece together widely separated information. Second, the\\noverlapping chunks necessary to preserve cross-boundary\\ncontext often lead to\\nredundancy\\n, bloating both the com-\\nputational cost and the effective input size presented to the\\nmodel. Such redundancy can dilute key information, ulti-\\nmately harming the system's overall performance.\\nRetrieval-Augmented Generation (RAG) represents a no-\\ntable attempt to alleviate these constraints by fetching\\nrelevant passages from a large corpus, then fusing them\\nwithin a generative transformer. Although RAG and simi-\\nlar frameworks (e.g., DPR, Contriever) excel at retrieving\\ntopically relevant content, they frequently suffer from an\\nover-retrieval of near-duplicate passages, especially for long\\nor repetitive source documents. This can induce a form of\\nredundancy similar to naive chunking, wherein the model's\\neffective input length grows excessively, and the generative\\nmodel struggles to identify the most salient facts. Moreover,\\nthe retrieval step often relies on static or heuristically tuned\\nthresholds that do not dynamically adapt to the inherent\\noverlap or novelty among candidate passages.\\nIn this paper, we propose a statistical estimation framework\\nthat addresses both the chunking problem and certain limi-\\ntations of retrieval-augmented QA pipelines. Our approach\\nleverages an\\nonline Bayesian thresholding\\nmechanism to\\n\\x02lter out redundant or near-duplicate text segments. In do-\\n1\\n\", 'page_2': 'Submission and Formatting Instructions for ICML 2025\\ning so, it provides a succinct yet suf\\x02ciently comprehensive\\ncontext for a generative language model, such as BART\\nor T5. Speci\\x02cally, we model the\\nnovelty\\nof each chunk\\nembedding with a time-varying Gaussian distribution, up-\\ndating a posterior estimate\\n(\\n \u02d9\\n2\\n)\\nevery time a new chunk\\nis processed. If the newly observed novelty score exceeds\\n\\n+\\n\\x0b \u02d9\\n, that chunk is considered suf\\x02ciently distinct and\\nadded to the \\x02nal prompt; otherwise, it is skipped. By con-\\ntinuously re\\x02ning\\n(\\n \u02d9\\n2\\n)\\n, the acceptance threshold adapts\\nto the evolving content distribution, ensuring that a series of\\nhighly similar chunks triggers an increase in the threshold\\nand prevents the accumulation of repetitive text.\\nOur framework can be deployed on top of either a \\x02xed\\nor adaptive chunking procedure. Additionally, it can be\\ncombined with external retrieval modules (e.g., Faiss, DPR,\\nRAG) to handle multi-document question answering. In\\nmulti-document scenarios, we \\x02rst retrieve a small set of\\ntop-ranked passages and then apply our Bayesian thresh-\\nolding to each retrieved text. As a result, we retain the\\nbene\\x02ts of retrieval-augmented generation (focusing on rel-\\nevant passages) while avoiding a common pitfall in RAG-\\nbased approaches, namely the unbounded accumulation of\\noverlapping or near-duplicate segments.\\nIn essence, the contribution of this work is threefold. First,\\nwe introduce a\\nBayesian chunk-\\x02ltering\\nmechanism that\\ncontinuously adapts its acceptance threshold based on ob-\\nserved novelty scores. Second, we provide an integrated\\nperspective on\\nlong-text chunking and retrieval augmen-\\ntation\\n, showing how to unify these techniques in a way\\nthat \\x02lters out super\\x03uous tokens while preserving the es-\\nsential parts of the original context. Third, we present a\\ncomprehensive\\nempirical evaluation\\non QA benchmarks\\nlike HotpotQA and TriviaQA. Our results indicate that the\\nproposed framework not only curtails computation by dis-\\ncarding repetitive chunks but also improves semantic \\x02-\\ndelity and generative quality, as measured by metrics such\\nas BERTScore and perplexity. When compared to naive\\nchunking or RAG alone, our approach more effectively\\nleverages long contexts by reducing irrelevant redundancy.\\nOverall, this paper highlights how a\\nprincipled statistical\\nmethodology\\ncan address the inherent issues of \\x02nite con-\\ntext windows and the tendency of retrieval-based systems\\nto repeat content. By judiciously selecting only chunks that\\nprovide new information, we sidestep both context frag-\\nmentation and exponential growth in input length, allowing\\ngenerative QA models to maintain clarity and focus on truly\\nsalient information. We proceed by reviewing related litera-\\nture on chunking, retrieval augmentation, and large-context\\nQA in Section\\n 2\\n, then detail our Bayesian thresholding\\nframework in Section\\n 3\\n. Section\\n 4\\n outlines our experimen-\\ntal design, including datasets and evaluation metrics, and\\nSection\\n 5\\n presents our empirical \\x02ndings. Finally, Section\\n 6\\nsummarizes our contributions and discusses avenues for\\nfuture research.\\n2. Related Work\\nThis section surveys three key areas of research pertinent to\\nlarge-scale question answering (QA): (1)\\nmulti-hop QA and\\nlong-context reasoning\\n, (2)\\nchunking strategies under con-\\ntext window constraints\\n, and (3)\\nprobabilistic approaches\\nfor adaptive context selection\\n. We then conclude with a brief\\ndiscussion on how these lines of work inspire our statistical\\nestimation framework.\\n2.1. Multi-Hop QA and Long-Context Reasoning\\nNumerous QA tasks require synthesizing information that is\\ndistributed across multiple paragraphs or even multiple doc-\\numents. Datasets such as HotpotQA and TriviaQA (\\nYang\\net al.\\n,\\n 2018\\n;\\n Joshi et al.\\n,\\n 2017\\n) exemplify this\\nmulti-hop\\nsetting, in which the model must combine evidence from\\ndifferent parts of the text to arrive at the \\x02nal answer. Tradi-\\ntional methods relied on\\nMemory Networks\\n(\\nWeston et al.\\n,\\n2015\\n), which iteratively retrieve and update internal repre-\\nsentations of facts. More recent work introduced hybrid or\\nend-to-end frameworks that fuse retrieval with generative\\nmodels, such as\\nFusion-in-Decoder (FiD)\\n(\\nIzacard & Grave\\n,\\n2021\\n) and\\nRetrieval-Augmented Generation (RAG)\\n(\\nLewis\\net al.\\n,\\n 2020\\n). While these methods effectively leverage ex-\\nternal knowledge sources or large corpora, they often apply\\n\\x02xed top-\\nk\\nretrieval heuristics or pre-segmented input text.\\nConsequently, if the documents are exceptionally long or\\nrepetitive, the model may still confront fragmented or re-\\ndundant passages, undermining both ef\\x02ciency and answer\\ncoherence.\\n2.2. Chunking Strategies and Context Window\\nConstraints\\nBecause most transformer-based QA systems can only pro-\\ncess a \\x02nite number of tokens (e.g., 512, 1024, or 2048)\\nat a time, a popular solution is to\\nchunk\\nthe documents.\\nEarly approaches employed naive \\x02xed-size splitting, which\\nfrequently led to\\ncontext fragmentation\\n(breaking cohesive\\ncontent across chunk boundaries) and\\nredundancy\\n(increas-\\ning overlap to maintain continuity) (\\nSukhbaatar et al.\\n,\\n 2015\\n;\\n?\\n). To remedy this, some works utilize\\nsemantic-aware\\nchunking\\n, aiming to align chunk boundaries with natural\\ntopic shifts or paragraph boundaries (\\nTose et al.\\n,\\n 1999\\n;\\n?\\n).\\nHowever, purely semantic chunking may remain computa-\\ntionally expensive for large corpora and does not necessarily\\neliminate redundant overlaps if the underlying text itself is\\nrepetitive.\\nSeveral authors have proposed\\nadaptive chunk sizing\\nmeth-\\nods that dynamically adjust chunk length according to local\\n2\\n', 'page_3': \"Submission and Formatting Instructions for ICML 2025\\ntextual features such as named entity density, discourse\\nmarkers, or domain-speci\\x02c cues (\\n??\\n). These strategies\\noften yield more contextually cohesive segments, albeit at\\nthe cost of potentially increased variance in segment sizes.\\nEven then, overlapping content can remain a problem, espe-\\ncially if certain themes or phrases repeat throughout a long\\ndocument.\\n2.3. Probabilistic Approaches for Adaptive Context\\nSelection\\nWhile chunking addresses the raw token limit, additional\\nmechanisms are sometimes required to \\x02lter near-duplicate\\nor low-utility segments when documents are highly redun-\\ndant. Recent QA pipelines have begun to explore threshold-\\nbased chunk \\x02ltering (\\nLewis et al.\\n,\\n 2020\\n), where embed-\\ndings of candidate chunks are compared against a reference\\nor running context vector. Yet most of these use \\x02xed or\\nmanually tuned thresholds that are insensitive to the dy-\\nnamic nature of the text distribution.\\nIn contrast,\\nBayesian\\nmethods offer a principled way to\\nadapt thresholds in real time. By modeling chunk novelty or\\nsimilarity as a random variable drawn from an evolving dis-\\ntribution, one can continuously update the parameters (e.g.,\\nmean and variance) based on newly observed data (\\nGel-\\nman et al.\\n,\\n 2013\\n;\\n?\\n). This u pdate mechanism allows the\\nacceptance (or rejection) criterion to tighten when multiple\\nsimilar chunks appear, thereby minimizing repetitive con-\\ntent. Conversely, if highly varied or dissimilar chunks arise,\\nthe threshold can lower, avoiding an overly stringent \\x02lter\\nthat might discard essential information.\\n2.4. Toward a Uni\\x02ed Statistical Framework\\nAlthough multi-hop retrieval, chunking strategies, and adap-\\ntive thresholding each address distinct aspects of the long-\\ncontext QA challenge, they are seldom integrated in a single\\ncoherent pipeline. Recent retrieval-augmented approaches,\\nlike RAG, often reduce large corpora to the top-\\nk\\nmost\\nrelevant passages but do not inherently handle overlap or\\nnear-duplication within those passages. Conversely, sophis-\\nticated chunking methods ensure that each segment stays\\nwithin model limits but can still \\x03ood the system with re-\\ndundant tokens if the underlying document is repetitive.\\nOur work draws upon all three threads of research. We lever-\\nage\\nchunk-wise document segmentation\\nto respect context\\nwindow constraints, combine it with\\nBayesian threshold-\\ning\\nto adaptively \\x02lter out repetitive or semantically similar\\nchunks, and demonstrate that this synergy scales eff ectively\\nto large and multi-document QA tasks. By unifying these\\nideas, we aim to preserve the essential advantages of multi-\\nhop retrieval and chunk-based reasoning while mitigating\\nthe pitfalls of naive segmentation or static thresholding. This\\nstatistical estimation framework\\nthus off ers a principled way\\nto dyn amically manage context size and redundancy, paving\\nthe way for more ef\\x02cient and accurate generative QA on\\nlong or complex texts.\\n3. Proposed Method\\nWe propose a statistical estimation method for large-scale\\nquestion answering (QA) that proceeds in three main phases:\\n(1) Chunk-wise Document Segmentation\\n,\\n(2) Bayesian\\nThresholding\\n, and\\n(3) Generative Answer Construction\\n.\\nUnlike simple chunking or retrieval-only pipelines, our ap-\\nproach adaptively discards redundant text segments, produc-\\ning a \\x02nal context that is both\\nsuccinct\\nand\\nrepresentative\\nof the original document's content. In the subsections that\\nfollow, we provide an in-depth description of each com-\\nponent and clarify the rationale behind the corresponding\\nmathematical models.\\n3.1. Chunk-wise Document Segmentation\\nLet\\nD\\nbe a text document composed of\\nN\\ntokens:\\nD\\n=\\nf\\nw\\n1\\n; w\\n2\\n; : : : ; w\\nN\\ng\\n:\\n(1)\\nWhen\\nN\\nis very large, directly feeding\\nD\\ninto a genera-\\ntive language model can exceed its maximum context size.\\nTo address this limitation, we divide\\nD\\ninto segments (or\\nchunks\\n) that remain within the model's input limits. Specif-\\nically, we \\x02x a maximum chunk size\\nM\\nand an overlap\\nsize\\nO\\n; in many of our experiments,\\n(\\nM ; O\\n) = (300\\n;\\n30)\\n.\\nThen, if\\n`\\nk\\nand\\nr\\nk\\nde\\x02ne the start and end indices of the\\nk\\n-th chunk, we have\\nC\\nk\\n=\\nf\\nw\\n`\\nk\\n; w\\n`\\nk\\n+1\\n; : : : ; w\\nr\\nk\\ng\\n; r\\nk\\n\\n`\\nk\\n+ 1\\n\\x14\\nM ;\\n(2)\\nand the next chunk starts approximately at\\nr\\nk\\n\\nO\\n+ 1\\n.\\nThis overlapping scheme preserves continuity across chunk\\nboundaries (e.g., when sentences or paragraphs span chunk\\nedges).\\nEmbedding Each Chunk.\\nTo facilitate statistical compar-\\nison among chunks, we map each\\nC\\nk\\ninto a\\nd\\n-dimensional\\nvector via a pretrained sentence encoder:\\nh\\nk\\n= Enco de\\n\\nC\\nk\\n\\x01\\n2\\nR\\nd\\n:\\n(3)\\nBy doing so, each chunk is represented in a continuous\\nspace where\\nsimilarity\\nor\\ndistance\\ncan be quanti\\x02ed more\\nnaturally than with raw text. We collect these embeddings\\nas\\nf\\nh\\n1\\n;\\nh\\n2\\n; : : : ;\\nh\\nK\\ng\\n.\\n3.2. Bayesian Thresholding for Chunk Selection\\nAlthough segmenting a document into chunks guarantees\\neach segment's length remains feasible, many chunks may\\noverlap in content. To avoid repetitiveness, we introduce a\\n3\\n\", 'page_4': 'Submission and Formatting Instructions for ICML 2025\\nBayesian thresholding\\nprocedure. The key idea is to form\\nand update a\\nrunning context vector\\n,\\nv\\nsel\\n, which captures\\nthe semantics of all chunks selected so far, and then compare\\neach new chunk to this context vector to measure\\nnovelty\\n.\\nNovelty Score.\\nLet\\nh\\nk\\nbe the embedding of the\\nk\\n-th\\nchunk. We de\\x02ne its novelty score,\\nX\\nk\\n, relative to the\\ncurrent context\\nv\\nsel\\nas\\nX\\nk\\n= 1\\n\\nh\\n>\\nk\\nv\\nsel\\nk\\nh\\nk\\nk k\\nv\\nsel\\nk\\n+\\n\"\\n;\\n(4)\\nwhere\\n\"\\nis a small constant (e.g.,\\n10\\n\\n8\\n) to avoid division by\\nzero. A\\nlow\\nX\\nk\\nindicates that th e chunk offers minimal new\\ninformation, whereas a\\nhigh\\nX\\nk\\nsuggests that\\nC\\nk\\ncontains\\ncontent that is distinct from what is already included.\\nBayesian Update.\\nTo dynamically adapt our acceptance\\ncriterion, we model\\nX\\nk\\nas a sample from a normal distri-\\nbution\\nN\\n(\\n \u02d9\\n2\\n)\\n, treating\\n(\\n \u02d9\\n2\\n)\\nas latent parameters that\\nevolve with each observed\\nX\\nk\\n. Let\\n\u02dd\\n2\\nobs\\nbe the noise vari-\\nance associated with the novelty measurement. Then after\\nobserving\\nX\\nk\\n, we update the posterior on\\n(\\n \u02d9\\n)\\nvia the\\nstandard conjugate formulas:\\n\u02d9\\n2\\np ost\\n=\\n\\x10\\n1\\n\u02d9\\n2\\n+\\n1\\n\u02dd\\n2\\nobs\\n\\x11\\n\\n1\\n;\\n(5)\\n\\np ost\\n=\\n\u02d9\\n2\\np ost\\n\\x10\\n\\n\u02d9\\n2\\n+\\nX\\nk\\n\u02dd\\n2\\nobs\\n\\x11\\n:\\n(6)\\nThese updates allow us to re\\x02ne our expectation of h ow\\n\ufb01novel\ufb02 future chunks might be, based on previously ob-\\nserved novelty scores.\\nAdaptive Thresholding.\\nOnce we have an updated poste-\\nrior\\n(\\n \u02d9\\n)\\n, we de\\x02ne the acceptance threshold:\\n\\x02 =\\n\\n+\\n\\x0b \u02d9;\\n(7)\\nwhere\\n\\x0b\\nis a hyperparameter (e.g.,\\n\\x0b\\n= 1\\n:\\n0\\n). If\\nX\\nk\\n\\x15\\n\\x02\\n, the\\nk\\n-th chunk is deemed suf\\x02ciently distinct and is\\ntherefore\\naccepted\\n; otherwise, it is skipped. Importantly,\\nthis threshold adapts over time: whenever we encounter\\nmany high-novelty chunks in a row,\\n\\nand\\n\u02d9\\nwill increase,\\nthus raising the threshold and preventing trivial acceptance\\nof slightly different chunks. Conversely, if\\nX\\nk\\nvalues remain\\nlow, the threshold may become easier to exceed, allowing\\nmore chunks to be included.\\nContext Vector Update.\\nEach time a chunk\\nC\\nk\\nis ac-\\ncepted, we update\\nv\\nsel\\nto account for the newly included\\ninformation:\\nv\\nsel\\n \\nw\\nk\\nv\\nsel\\n+\\n\\n1\\n\\nw\\nk\\n\\x01\\nh\\nk\\n; w\\nk\\n=\\nX\\nk\\nX\\nk\\n+ 1\\n:\\n(8)\\nIf\\nX\\nk\\nis large (i.e., the chunk is very different from the\\ncurrent context), we weight\\nh\\nk\\nmore heavily, shifting\\nv\\nsel\\ntoward new content.\\nThrough this process, we compile a\\nsubset\\nof chunks that\\ncollectively cover the salient information in the original\\ndocument without redundant repetition. Denote this subset,\\nin text form, by\\nS\\n.\\n3.3. Generative Answer Construction\\nAfter the Bayesian thresholding stage concludes, we con-\\ncatenate the accepted chunks to form a \\x02nal context:\\nC\\n\\x0cnal\\n=\\n\\x02\\nC\\ni\\n1\\nk C\\ni\\n2\\nk \\x01 \\x01 \\x01 k C\\ni\\nm\\n\\x03\\n;\\nwhere each\\nC\\ni\\nj\\n2 S\\n:\\n(9)\\nThis context is then paired with a user query\\nQ\\nto produce\\na prompt for a generative language model, such as BART.\\nRather than providing the entire original document\u0160which\\nmay contain large swaths of duplicative text\u0160we pass only\\nthis \\x02ltered context, substantially reducing the token volume\\nand enhancing both ef\\x02ciency and clarity.\\nConstructing the Prompt.\\nWe structure the prompt in a\\nstraightforward, yet instructive manner, to guide the model\\ntoward a concise answer. First, we indicate the system\\'s\\nrole (e.g., \ufb01You are a helpful QA system\ufb02), brie\\x03y restate\\nthe user\\'s query, then supply the relevant chunks under a\\n\ufb01Context:\ufb02 heading. Finally, we request a direct answer. For\\nexample, in plain text:\\n\ufb01You are a helpful QA system. The user asks:\\nf\\nquestion\\ng\\n.\\nBelow is relevant context \\x02ltered from a larger document:\\nf\\naccepted chunks\\ng\\nPlease provide the best possible answer.\ufb02\\nThis format ensures that the generative model is aware of\\nthe question and the most novel parts of the document, as\\ndetermined by our Bayesian thresholding step.\\n3.4. Algorithmic Overview\\nThe full procedure is summarized in Algorithm\\n 1\\n. First,\\nC\\nH U N K\\nD\\nO C U M E N T\\nsplits\\nD\\ninto overlapping chunks of\\nsize\\nM\\nand overlap\\nO\\n. Then, for each chunk embedding\\nh\\nk\\n,\\nwe calculate the novelty score\\nX\\nk\\nvia Eq.\\n(\\n4\\n)\\n, update\\n(\\n \u02d9\\n)\\naccording to Eqs.\\n(\\n5\\n)\\n\u0152\\n(\\n6\\n)\\n, and apply the threshold\\n\\x02\\nfrom\\nEq.\\n(\\n7\\n)\\n. Chunks meeting or exceeding\\n\\x02\\nare accepted and\\nused to update the running context vector. After processing\\nall\\nK\\nchunks, we concatenate the accepted chunks\\' texts\\ninto\\nC\\n\\x0cnal\\nand feed them, together with the user\\'s question,\\ninto the generator\\nG\\n. The result is a \\x02nal answer that typi-\\ncally leverages the most informative portions of the original\\ndocument.\\n4\\n', 'page_5': \"Submission and Formatting Instructions for ICML 2025\\nAlgorithm 1\\nStatistical Estimation QA (High-Level Sketch)\\nRequire:\\nDocument\\nD\\n, Question\\nQ\\n, overlap\\nO\\n, chunk size\\nM\\n,\\nBayesian hyperparameters\\n(\\n\\n0\\n; \u02d9\\n0\\n; \u02dd\\n2\\nobs\\n; \\x0b\\n)\\n,\\nembedding model\\nE\\n(\\n\\x01\\n)\\n, generator\\nG\\n(\\n\\x01\\n)\\nEnsure:\\nAnswer\\nA\\n1:\\nfC\\n1\\n; : : : ;\\nC\\nK\\ng  \\nC\\nH U N K\\nD\\nO C U M E N T\\n(\\nD\\n;\\nM ; O\\n)\\n2:\\nv\\nsel\\n \\n0\\n3:\\n(\\n \u02d9\\n)\\n \\n(\\n\\n0\\n; \u02d9\\n0\\n)\\n4:\\nS  \\n?\\nf\\nAccepted chunk set\\ng\\n5:\\nfor\\nk\\n= 1\\nto\\nK\\ndo\\n6:\\nh\\nk\\n E\\n\\nC\\nk\\n\\x01\\n7:\\nX\\nk\\n \\n1\\n\\ncos\\n\\nh\\nk\\n;\\nv\\nsel\\n\\x01\\n8:\\nUpdate\\n(\\n \u02d9\\n)\\nvia Eqs. (\\n5\\n)\u0152(\\n6\\n)\\n9:\\n\\x02\\n \\n\\n+\\n\\x0b \u02d9\\n10:\\nif\\nX\\nk\\n\\x15\\n\\x02\\nthen\\n11:\\nw\\nk\\n \\nX\\nk\\nX\\nk\\n+ 1\\n12:\\nv\\nsel\\n \\nw\\nk\\nv\\nsel\\n+ (1\\n\\nw\\nk\\n)\\nh\\nk\\n13:\\nS  S [ fC\\nk\\ng\\n14:\\nend if\\n15:\\nend for\\n16:\\nC\\n\\x0cnal\\n \\nConcatenate\\n(\\nS\\n)\\n17:\\nA\\n G\\n\\nQ;\\nC\\n\\x0cnal\\n\\x01\\nA\\n3.5. Discussion and Rationale\\nThis framework provides a balance between \\x02ne-grained\\nchunking (which prevents losing local details) and an\\nadaptive \\x02ltering mechanism (which curbs the exponential\\ngrowth of repetitive or overlapping segments). Speci\\x02cally,\\nthe\\nBayesian thresholding\\nensures that as soon as we ob-\\nserve multiple chunks with similar content, our posterior\\nmean\\n\\nincreases and makes the threshold\\n\\x02\\nmore strin-\\ngent. Thus, we retain only chunks that truly introduce new\\ninformation into the running context vector.\\nFrom a computational perspective, our chunking strategy\\nprocesses the document linearly, and each chunk embedding\\ncan be computed in parallel batches if needed. The thresh-\\nolding computations are negligible in overhead, involving\\nonly a few arithmetic updates per chunk. Additionally, since\\nwe feed the generative model only those chunks that sur-\\npass the novelty threshold, our \\x02nal input size is drastically\\nreduced compared to naive concatenation of all chunks.\\nIn summary, this proposed method leverages simple yet\\npowerful statistical reasoning to deal with large-scale QA\\ntasks where full-document encoding is infeasible or prone to\\nredundancy. The resulting prompt, which contains only the\\nmost salient segments, allows generative language models\\nlike BART to focus on truly relevant information, thereby\\nimproving both ef\\x02ciency and accuracy in answer genera-\\ntion.\\n4. Experimental Setup\\nWe evaluate our proposed methods\u0160collectively referred to\\nas\\nSEQA\\n(short for\\nStatisticalEstimationQA\\n)\u0160on two QA\\nbenchmarks that feature long or multi-paragraph contexts.\\nBelow, we describe the datasets, the methods we compare,\\nthe hyperparameters we employ in a\\nnarrative\\nfashion, and\\nthe evaluation metrics and protocol used in our experiments.\\n4.1. Datasets\\nWe focus on two validation sets known to contain extraneous\\ntext and thus require robust \\x02ltering or retrieval:\\nHotpotQA (Distractor Split).\\nEach question is paired\\nwith multiple paragraphs, some of which are irrelevant dis-\\ntractors. We adopt the validation set from the distractor\\ncon\\x02guration, which requires multi-hop reasoning across\\nthese paragraphs.\\nTriviaQA (RC Split).\\nFeatures extended snippets for each\\nquery, often containing large amounts of irrelevant text. We\\nagain use the validation set. Both of these datasets demon-\\nstrate whether a system can effectively isolate and summa-\\nrize relevant evidence in scenarios where input contexts can\\nbe quite large.\\n4.2. Methods Compared\\nIn all methods, we ultimately generate answers through\\nfacebook/bart-large\\n. The methods differ primarily\\nin how they retrieve, segment, or \\x02lter the input text:\\nSEQA (Chunk-Only, Proposed).\\nThis approach handles\\na single long document (or an already-merged text) by split-\\nting it into chunks of about 300 tokens each (with a 30-token\\noverlap). A running context vector is maintained while em-\\nbedding these chunks, and a Bayesian thresholding proce-\\ndure decides whether each chunk is suf\\x02ciently novel or\\nredundant. Only novel chunks are retained and combined\\nwith the user's query, then fed into BART-Large for \\x02nal\\nanswer generation.\\nSEQA (With Index, Proposed).\\nWhen multiple docu-\\nments are available, or a large document collection is in-\\nvolved, the system \\x02rst retrieves the top-\\nk\\npassages (com-\\nmonly\\nk\\n= 3\\n) via a vector-based retrieval mechanism (e.g.,\\nFaiss). It then applies the same chunking and Bayesian\\n\\x02ltering to eliminate overlapping or repetitive content, pre-\\nserving only key segments. BART-Large subsequently uses\\nthese segments as context.\\nBart with Faiss Retrieval (Baseline).\\nRetrieves top-\\nk\\npassages but does not perform chunk-level \\x02ltering. Instead,\\nthe retrieved text is concatenated in full, along with the\\n5\\n\", 'page_6': \"Submission and Formatting Instructions for ICML 2025\\nquery, before passing into BART-Large.\\nBart with Memory Network (Baseline).\\nSplits the text\\n(or set of documents) into smaller fragments of around 128\\ntokens each. A multi-hop retrieval mechanism then progres-\\nsively collects relevant fragments based on an updated query\\nrepresentation. Once a set of relevant fragments is gathered,\\nBART-Large generates the answer.\\nBart with Summarization (Baseline).\\nGradually summa-\\nrizes large blocks of text into more concise paragraphs. The\\n\\x02nal condensed representation is merged with the question\\nand provided as input to BART-Large.\\nBart with DPR or Contriever (Baselines).\\nRelies on\\ndense retrieval encoders (either DPR or Contriever) to em-\\nbed both queries and passages in a shared space. The top-\\nk\\npassages are retrieved according to similarity scores, then\\ndirectly concatenated for BART-Large. No chunk-level nov-\\nelty \\x02ltering is used in these pipelines.\\n4.3. Hyperparameter Settings (Narrative)\\nAll of our SEQA methods rely on a chunk size of roughly\\n300 tokens, along with a 30-token overlap to ensure conti-\\nnuity. For the Bayesian thresholding, we begin with a prior\\nmean (\\n\\n0\\n) of 0.5 and a prior standard deviation (\\n\u02d9\\n0\\n) of 0.1.\\nWe also set an observation noise variance (\\n\u02dd\\n2\\nobs\\n) to 0.05, re-\\n\\x03ecting uncertainty in each chunk's measured novelty score.\\nFinally, we include a margin coef\\x02 cient\\n\\x0b\\n= 1\\n:\\n0\\n, so that any\\nchunk whose novelty score is below\\n\\n+\\n\\x0b \u02d9\\nis considered\\nredundant and therefore \\x02ltered out.\\nFor methods that rely on vector indexing (Faiss retrieval,\\nDPR, Contriever, and SEQA with Index), we retrieve the\\ntop-\\nk\\npassages, typically setting\\nk\\n= 3\\n. When dealing with\\na single document or pre-merged text, SEQA (Chunk-Only)\\nsimply splits it into chunks. Baseline methods, such as\\nthe Memory Network, use smaller chunks (128 tokens) to\\nstore data in a memory bank, whereas the Summarization\\nbaseline works on paragraph-level blocks. In each case, the\\nprocessed or \\x02ltered text (chunks, memory fragments, or\\nsummaries) is then concatenated with the question for input\\nto\\nfacebook/bart-large\\n, which runs beam search\\n(beam size of 2 or 4) with a maximum generation length of\\n128 tokens. We also impose an encoder input limit of 512\\nor 1024 tokens, depending on resource availability, to avoid\\nexceeding the BART model's capacity.\\n4.4. Evaluation Metrics and Protocol\\nSince the outputs are free-form and often rephrase the ref-\\nerence answers, we employ four generative QA metrics:\\nBERTScore\\n(for semantic similarity),\\nBLEU\\nand\\nROUGE-L\\n(for various forms of lexical overlap), and\\nPerplexity\\n(com-\\nputed by a small language model like GPT-Neo 125M to\\ngauge \\x03uency). We do not rely on exact match or F1, be-\\ncause their strict token-level matching tends to underesti-\\nmate correctness in open-ended generation tasks.\\nFor each question in the HotpotQA (distractor) and Triv-\\niaQA (RC) validation sets, we run all methods with the\\nsame BART-Large generator. We then compute the above\\nmetrics by comparing the resulting answers to the provided\\nreferences and take the average over the full validation split.\\nThis consistent testing procedure ensures that we can di-\\nrectly observe how effectively each pipeline navigates long\\nor multi-document contexts, whether by chunking, retrieval,\\nor summarization, and how well each maintains semantic\\n\\x02delity and \\x03uency in its \\x02nal answers.\\n5. Results and Discussion\\nIn this section, we present the empirical outcomes of our\\nproposed\\nStatistical Estimation QA\\nframework on two rep-\\nresentative QA benchmarks,\\nHotpotQA\\nand\\nTriviaQA\\n. We\\ncompare our method against four baselines:\\nFusionInDe-\\ncoder (FiD)\\n(\\nIzacard & Grave\\n,\\n 2021\\n),\\nRAG\\n(\\nLewis et al.\\n,\\n2020\\n),\\nMemoryNetwork\\n(\\nWeston et al.\\n,\\n 2015\\n), and\\nHierar-\\nchicalSummarization\\n(Section-based summarization). We\\nreport results on both\\ntraditional extractive\\nmetrics (EM,\\nF1) and\\ngenerative\\nmetrics (BLEU, ROUGE, Perplexity,\\nBERTScore). Table\\n 1\\n provides a comprehensive summary\\nof these results, with each row showing a particular method's\\nperformance on a speci\\x02c dataset.\\nTable 1.\\nExperimental Results on HotpotQA and TriviaQA.\\nWe\\nmeasure\\nEM/F1\\n(extractive focus),\\nBLEU/ROUGE\\n(n-gram over-\\nlap),\\nPPL\\n(\\x03uency via GPT-2), and\\nBERTScore\\n(semantic similar-\\nity). `StatisticalEstimationQA' is our proposed method.\\nDataset Method EM F1 BLEU ROUGE PPL BERTScore\\n5*\\nHotpotQA\\nStatisticalEstimationQA 0.0 0.0 3.918\\n\\x02\\n10\\n\\n156\\n0.055556 2.94\\n\\x02\\n10\\n2\\n0.786957\\nFusionInDecoder 0.0 0.0 4.368\\n\\x02\\n10\\n\\n156\\n0.079683 9.64\\n\\x02\\n10\\n1\\n0.759026\\nRAG 0.0 0.0 0.0 0.000000 3.34\\n\\x02\\n10\\n2\\n0.775604\\nMemoryNetwork 0.0 0.0 0.0 0.000000 1.07\\n\\x02\\n10\\n1\\n0.779597\\nHierarchicalSummarization 0.0 0.0 0.0 0.000000 5.05\\n\\x02\\n10\\n2\\n0.000000\\n5*\\nTriviaQA\\nStatisticalEstimationQA 0.0 0.0 1.629\\n\\x02\\n10\\n\\n233\\n0.003768 2.20\\n\\x02\\n10\\n2\\n0.796273\\nFusionInDecoder 0.0 0.0 1.664\\n\\x02\\n10\\n\\n233\\n0.004162 4.01\\n\\x02\\n10\\n2\\n0.792998\\nRAG 0.0 0.0 0.0 0.000000 4.00\\n\\x02\\n10\\n7\\n0.810775\\nMemoryNetwork 0.0 0.0 0.0 0.000000 1.65\\n\\x02\\n10\\n3\\n0.812428\\nHierarchicalSummarization 0.0 0.0 0.0 0.000000 4.07\\n\\x02\\n10\\n2\\n0.803652\\nObservations.\\nWe highlight the following major \\x02ndings\\nbased on the above table:\\n(1) Extractive Metrics (EM/F1, BLEU) Remain Near\\nZero.\\nAcross both datasets, all methods exhibit\\n0\\n:\\n0\\nfor\\nEM/F1 and near-zero BLEU. This phenomenon is not un-\\nusual in\\ngenerative QA\\ntasks, where models often paraphrase\\nthe original answer or produce newly synthesized sentences\\ninstead of reproducing the exact ground-truth tokens. Con-\\nsequently, small token-overlap metrics cannot adequately\\ncapture the correctness of such free-form responses.\\n6\\n\", 'page_7': \"Submission and Formatting Instructions for ICML 2025\\n(2) ROUGE Also Remains Low.\\nSimilar to BLEU,\\nROUGE primarily measures lexical overlap at the phrase\\nor sequence level. Although FusionInDecoder's ROUGE\\non HotpotQA is slightly above zero (\\n\u02d8\\n0.0797), most other\\nscores remain under 0.01, indicating minimal textual over-\\nlap. This reinforces that the generated answers deviate\\nsubstantially from any reference wording.\\n(3) Large Variation in Perplexity (PPL).\\nPerplexity, mea-\\nsured via a separate GPT-2 model, spans values from about\\n10\\nto over\\n4\\n:\\n0\\n\\x02\\n10\\n7\\nin the more extreme cases. Notably,\\nRAG\\non TriviaQA yields an exceptionally high PPL of\\n4\\n:\\n0\\n\\x02\\n10\\n7\\n, suggesting that the GPT-2 language model \\x02nds\\ncertain RAG outputs highly unnatural or out of distribution.\\nOn the other hand, MemoryNetwork in HotpotQA obtains\\naround 10.7 PPL, likely due to short or repetitive outputs\\nperceived as more \ufb01predictable\ufb02 by GPT-2.\\n(4) BERTScore Re\\x03ects Meaningful Content Similarity.\\nDespite vanishingly small token-level overlaps, BERTScore\\nvalues range from\\n0\\n:\\n75\\nto\\n0\\n:\\n81\\n, con\\x02rming that the model\\noutputs convey semantically related content to the reference.\\nFor instance, StatisticalEstimationQA achieves\\n0\\n:\\n787\\n(Hot-\\npotQA) and\\n0\\n:\\n796\\n(TriviaQA), illustrating that it preserves\\nessential meaning better than the lexical overlap might sug-\\ngest. One outlier is HierarchicalSummarization on Hot-\\npotQA, which yields a BERTScore of\\n0\\n:\\n0\\n, implying it might\\nproduce context-irrelevant or empty answers.\\n(5) Proposed Method (StatisticalEstimationQA).\\nOur ap-\\nproach demonstrates comparatively robust performance re-\\ngarding BERTScore (\\n\u02c7\\n0\\n:\\n79\\n\u02d8\\n0\\n:\\n80\\n). In generative QA,\\nsuch semantic-based metrics are more informative than\\nsurface-level overlaps. Moreover, the slight improvement in\\nBERTScore over FusionInDecoder or RAG supports our ar-\\ngument that\\nAdaptive Chunk Sizing\\nand\\nBayesian thresh-\\nolding\\nhelp preserve critical information from the original\\ndocument without in\\x03ating the model's input window or\\nproducing extraneous text. Nonetheless, we observe a rela-\\ntively high PPL on HotpotQA (\\n\u02d8\\n294\\n), indicating that the\\nGPT-2 model \\x02nds some of our outputs more linguistically\\ndiverse or unpredictable.\\nQualitative Analysis.\\nA manual inspection of several\\nsampled outputs reveals that our StatisticalEstimationQA\\nfrequently produces coherent multi-sentence answers that\\nrephrase or condense the original references. While meth-\\nods like RAG or MemoryNetwork occasionally yield shorter\\nor incomplete statements, their BERTScore remains com-\\npetitive, suggesting that they do capture essential facts in\\nsome form. However, RAG's extremely high PPL on Trivi-\\naQA underlines the instability of purely retrieval-augmented\\napproaches when encountering noisy or domain-speci\\x02c\\nqueries.\\nOverall, these \\x02ndings highlight the inadequacy of\\nextractive-based metrics for free-form QA, while simulta-\\nneously emphasizing the value of semantic-level evaluation\\nmeasures (BERTScore, perplexity) in diagnosing generative\\nmodels' behaviors.\\n6. Conclusion\\nWe introduced a\\nstatistical estimation\\nframework to ad-\\ndress large-scale Question Answering under constrained\\ncontext windows. By integrating\\nAdaptive Chunk Sizing\\n(to\\nprevent unwieldy fragmentation) and\\nBayesian thresholding\\n(to prune redundant chunks), our method constructs a con-\\ncise yet representative context embedding for documents\\nthat exceed typical LLM input limits. While chunk selec-\\ntion is biased rather than random, we leveraged a\\nCLT-based\\napproximation\\nto fuse selected embeddings, resulting in a\\nsingle vector that captures essential document semantics.\\nEmpirical results on\\nHotpotQA\\nand\\nTriviaQA\\nunderscore\\nthe dif\\x02culty of evaluating generative QA models using\\ntoken-level overlaps (EM, F1, BLEU, ROUGE), which all\\nremained near zero. In contrast,\\nBERTScore\\nvalues demon-\\nstrate that the generated answers do convey semantically\\nrelevant information, even if they diverge lexically from\\nthe reference. Furthermore, although our method does not\\nimprove upon extractive scores, it yields\\ncompetitive or su-\\nperior\\nBERTScore compared to baselines such as FiD, RAG,\\nor MemoryNetwork, re\\x03ecting its effectiveness in retaining\\nessential document content under restricted context length.\\nNevertheless, perplexity analysis reveals that certain QA\\noutputs can appear irregular or ill-formed to a pretrained\\nlanguage model (e.g., GPT-2), particularly in retrieval-\\naugmented or memory-based systems. For real-world de-\\nployment, we envision\\nhybrid approaches\\nthat combine\\nadaptive chunk selection with more robust answer gener-\\nation, possibly with domain-speci\\x02c \\x02ne-tuning to reduce\\nlinguistic mismatch.\\nIn future work, we plan to:\\n\u0141\\nExtend the statistical chunk selection mechanism to\\nmulti-document\\nscenarios and continually updating\\ndata streams, enabling scalable QA beyond a single\\nsource.\\n\u0141\\nExplore more\\n\\x02ne-grained Bayesian\\nmodels that diff er-\\nentiate chunk-level novelty from domain shifts, further\\nre\\x02ning acceptance thresholds.\\n\u0141\\nIntegrate\\nretrieval-augmented\\ngeneration techniques\\nwith our CLT-based method to mitigate the extreme per-\\nplexity spikes observed in certain retrieval pipelines.\\nWe hope this line of research encourages broader adoption\\nof\\ngenerative QA metrics\\nand emphasizes the importance\\n7\\n\", 'page_8': 'Submission and Formatting Instructions for ICML 2025\\nof robust chunking strategies to support large-scale QA tasks\\neffectively.\\nReferences\\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B.,\\nVehtari, A., and Rubin, D. B.\\nBayesian Data Analysis\\n.\\nCRC Press, 3rd edition, 2013. URL\\nhttps://www.\\ncrcpress.com/Bayesian- Data- Analysis/\\nGelman- Carlin- Stern- Dunson- Vehtari- Rubin/\\np/book/9781439840955\\n.\\nIzacard, G. and Grave, E. Leveraging passage retrieval\\nwith generative models for open domain qa (\\x02d).\\narXiv\\npreprint arXiv:2007.01282\\n, 2021. URL\\nhttps://\\narxiv.org/abs/2007.01282\\n.\\nJoshi, M., Chen, D., Liu, Y., Weld, D., and Zettlemoyer,\\nL. Triviaqa: A large scale distantly supervised chal-\\nlenge dataset for reading comprehension.\\narXiv preprint\\narXiv:1709.05373\\n, 2017. URL\\nhttps://arxiv.\\norg/abs/1709.05373\\n.\\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin,\\nV., Goyal, N., and ... Retrieval-augmented genera-\\ntion for knowledge-intensive nlp tasks.\\narXiv preprint\\narXiv:2005.11401\\n, 2020. URL\\nhttps://arxiv.\\norg/abs/2005.11401\\n.\\nSukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. End-\\nto-end memory networks. In\\nAdvances in Neural In-\\nformation Processing Systems (NeurIPS)\\n, 2015. URL\\nhttps://arxiv.org/abs/1503.08895\\n.\\nTose, B., Vanderwende, L., and Pierce, J. Texttiling:\\nSegmenting text into multi-paragraph units. In\\nCom-\\nputational Linguistics\\n, volume 25, pp. 373\u0152402. MIT\\nPress, 1999. URL\\nhttps://aclanthology.org/\\nJ99- 3003/\\n.\\nWeston, J., Chopra, S., and Bordes, A. Memory networks.\\nIn\\nInternational Conference on Learning Representa-\\ntions (ICLR)\\n, 2015. URL\\nhttps://arxiv.org/\\nabs/1410.3916\\n.\\nYang, Y., Qi, P., Zhang, X., Chen, X., Luo, Z., Liu, P.,\\nNeubig, G., Niu, F., and Gao, L. Hotpotqa: A dataset\\nfor diverse, explainable multi-hop question answering.\\narXiv preprint arXiv:1809.09600\\n, 2018. URL\\nhttps:\\n//arxiv.org/abs/1809.09600\\n.\\n8\\n'}\n\nGold Repository:\n\n{{GoldCode}}\n\nTarget Repository:\n\n```python\n## File name: config.py\n\"\"\"config.py\n\nCentralised, immutable experiment configuration singleton.\n\nEvery other module in the code-base *must* import `Config` and access\nhyper-parameters exclusively through its public attributes to ensure that\nall components share the exact same settings dictated by *config.yaml*,\nwhich reproduces the methodology of the target paper.\n\nExample\n-------\n>>> from config import Config\n>>> cfg = Config.load(\"config.yaml\")\n>>> print(cfg.chunk_size)\n300\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pathlib\nimport types\nfrom typing import Any, Dict, Optional\n\nimport yaml\n\n\nclass _FrozenNamespace(types.SimpleNamespace):\n    \"\"\"A SimpleNamespace that disallows mutation after freezing.\"\"\"\n\n    _frozen: bool = False\n\n    def _freeze(self) -> None:  # noqa: D401  (single line imperative)\n        \"\"\"Freeze the namespace so that attributes become read-only.\"\"\"\n        object.__setattr__(self, \"_frozen\", True)\n\n    def __setattr__(self, name: str, value: Any) -> None:  # noqa: D401\n        if getattr(self, \"_frozen\", False):\n            raise AttributeError(\n                f\"Attempted to modify read-only config attribute '{name}'.\"\n            )\n        super().__setattr__(name, value)\n\n\nclass Config(_FrozenNamespace):\n    \"\"\"Experiment-wide configuration loaded from *config.yaml*.\n\n    Public attributes (flat):\n        model_name:            HF model id for the generator (BART).\n        chunk_size:            Max tokens per chunk (M).\n        overlap:               Overlap tokens between successive chunks (O).\n        bayes_mu0:             Prior mean \u03bc\u2080 for novelty score.\n        bayes_sigma0:          Prior std \u03c3\u2080  for novelty score.\n        bayes_sigma_obs:       Observation noise \u03c3_obs for novelty score.\n        bayes_beta:            Margin coefficient \u03b2  (Eq. 7).\n        max_prompt_tokens:     Encoder maximum tokens (== generator limit).\n        beam:                  Beam size for generation.\n        random_seed:           Global random seed for reproducibility.\n        retrieval_enabled:     Whether retrieval module is active.\n        retrieval_top_k:       Number of passages to retrieve if enabled.\n\n    Additional nested configuration is accessible via `get(path)`.\n    \"\"\"\n\n    # --------------------------------------------------------------------- #\n    # Class-level singleton management\n    # --------------------------------------------------------------------- #\n    _INSTANCE: Optional[\"Config\"] = None\n\n    # --------------------------------------------------------------------- #\n    # Construction helpers\n    # --------------------------------------------------------------------- #\n    @classmethod\n    def load(cls, yaml_path: str | pathlib.Path = \"config.yaml\") -> \"Config\":\n        \"\"\"Parse *yaml_path* and return (singleton) Config instance.\n\n        The first call creates the singleton; subsequent calls return the\n        cached object regardless of `yaml_path`.\n        \"\"\"\n        if cls._INSTANCE is not None:\n            return cls._INSTANCE\n\n        path = pathlib.Path(yaml_path).expanduser().resolve()\n        if not path.exists():\n            raise FileNotFoundError(f\"Config file not found: {path}\")\n\n        with path.open(\"r\", encoding=\"utf-8\") as fp:\n            raw_cfg: Dict[str, Any] = yaml.safe_load(fp)\n\n        instance = cls()  # create empty namespace\n        instance._populate(raw_cfg)  # type: ignore[attr-defined]\n        instance._freeze()  # type: ignore[attr-defined]\n        cls._INSTANCE = instance\n        return instance\n\n    # --------------------------------------------------------------------- #\n    # Private helpers\n    # --------------------------------------------------------------------- #\n    def _populate(self, src: Dict[str, Any]) -> None:\n        \"\"\"Validate YAML structure and populate flat attributes.\"\"\"\n        # ---- Top-level experiment settings ---- #\n        self.random_seed = int(src.get(\"experiment\", {}).get(\"random_seed\", 42))\n\n        # ---- Chunking ---- #\n        chunking = src.get(\"chunking\", {})\n        self.chunk_size = int(chunking.get(\"chunk_size\", 300))\n        self.overlap = int(chunking.get(\"overlap\", 30))\n\n        # ---- Bayesian filter ---- #\n        bf = src.get(\"bayesian_filter\", {})\n        self.bayes_mu0 = float(bf.get(\"mu0\", 0.5))\n        self.bayes_sigma0 = float(bf.get(\"sigma0\", 0.1))\n        self.bayes_sigma_obs = float(bf.get(\"sigma_obs\", 0.05))\n        self.bayes_beta = float(bf.get(\"beta\", 1.0))\n\n        # ---- Generator ---- #\n        gen = src.get(\"generator\", {})\n        self.model_name = str(gen.get(\"model_name\", \"facebook/bart-large\"))\n        self.beam = int(gen.get(\"num_beams\", 4))\n        self.max_prompt_tokens = int(gen.get(\"encoder_max_tokens\", 1024))\n        self._max_gen_length = int(gen.get(\"max_generation_length\", 128))\n\n        # ---- Retrieval ---- #\n        retrieval = src.get(\"retrieval\", {})\n        self.retrieval_enabled = bool(retrieval.get(\"enabled\", False))\n        self.retrieval_top_k = int(retrieval.get(\"top_k\", 3))\n\n        # ---- Evaluation ---- #\n        evaluation = src.get(\"evaluation\", {})\n        self._metric_names = tuple(evaluation.get(\"metrics\", []))\n\n        # Preserve full raw dict for advanced queries/debugging\n        self._raw = src  # noqa: WPS437  (private attr storing raw)\n\n        # Validate values to catch configuration errors early.\n        self._validate()\n\n    def _validate(self) -> None:\n        \"\"\"Sanity-check hyper-parameters; raises ValueError if invalid.\"\"\"\n        if self.chunk_size <= 0:\n            raise ValueError(\"chunk_size must be positive.\")\n        if not (0 <= self.overlap < self.chunk_size):\n            raise ValueError(\"overlap must be in [0, chunk_size).\")\n        if self.bayes_sigma0 <= 0:\n            raise ValueError(\"bayes_sigma0 must be > 0.\")\n        if self.bayes_sigma_obs <= 0:\n            raise ValueError(\"bayes_sigma_obs must be > 0.\")\n        if self.bayes_beta <= 0:\n            raise ValueError(\"bayes_beta must be > 0.\")\n        if self.max_prompt_tokens <= 0:\n            raise ValueError(\"max_prompt_tokens must be positive.\")\n        if self.beam <= 0:\n            raise ValueError(\"beam must be positive.\")\n        if self._max_gen_length <= 0:\n            raise ValueError(\"max_generation_length must be positive.\")\n        if self.retrieval_enabled and self.retrieval_top_k <= 0:\n            raise ValueError(\"retrieval_top_k must be positive when retrieval is enabled.\")\n\n    # --------------------------------------------------------------------- #\n    # Convenience helpers\n    # --------------------------------------------------------------------- #\n    def get(self, dotted_path: str, default: Any = None) -> Any:\n        \"\"\"Return nested config value via dot-separated `dotted_path`.\n\n        Example\n        -------\n        >>> cfg.get(\"evaluation.metrics\")\n        ['BLEU', 'ROUGE_L', 'BERTScore', 'Perplexity']\n        \"\"\"\n        parts = dotted_path.split(\".\")\n        node: Any = self._raw\n        for part in parts:\n            if not isinstance(node, dict) or part not in node:\n                return default\n            node = node[part]\n        return node\n\n    # --------------------------------------------------------------------- #\n    # Dunder methods\n    # --------------------------------------------------------------------- #\n    def __repr__(self) -> str:  # noqa: D401\n        attrs = (\n            \"model_name chunk_size overlap bayes_mu0 bayes_sigma0 \"\n            \"bayes_sigma_obs bayes_beta max_prompt_tokens beam \"\n            \"retrieval_enabled retrieval_top_k random_seed\"\n        ).split()\n        short_cfg = {k: getattr(self, k) for k in attrs}\n        return f\"Config({short_cfg})\"\n\n```\n\n```python\n## File name: utils.py\n## utils.py\n\"\"\"Generic utility helpers shared by the SEQA-reproduction code-base.\n\nThe module is intentionally lightweight and **stateless** (apart from\nglobal logging handlers and PRNG seeds).  No other file should replicate\nthe functionality implemented here.\n\nPublic API\n----------\nseed_everything(seed:int) -> int\n    Sets Python `random`, NumPy and PyTorch seeds (CPU & CUDA)\n    *and* `PYTHONHASHSEED` for deterministic behaviour.\n\nflatten_dict(nested:dict, parent_key:str = \"\", sep:str = \"/\") -> dict\n    Converts arbitrarily nested dictionaries (and lists / tuples) into a\n    flattened mapping where keys are concatenated with `sep`.\n\nlength_limit(tokenizer, text:str, max_tokens:int) -> str\n    Ensures `text` is no longer than `max_tokens` when tokenised with the\n    supplied Hugging-Face tokenizer; truncates from the **end** if\n    necessary whilst trying to keep whole-line boundaries.\n\nsetup_logging(cfg:Config, name:str|None = None) -> logging.Logger\n    Configures the Python logging subsystem once and returns a module\n    specific logger.\n\"\"\"\nfrom __future__ import annotations\n\nimport datetime as _dt\nimport logging\nimport os\nimport random\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport numpy as np\nimport torch\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\n\n# --------------------------------------------------------------------------- #\n# Reproducibility utilities\n# --------------------------------------------------------------------------- #\ndef seed_everything(seed: int | None) -> int:  # noqa: WPS231 (complexity is fine)\n    \"\"\"Seed *all* relevant PRNGs.\n\n    Parameters\n    ----------\n    seed:\n        Non-negative integer seed.  If ``None`` an exception is raised:\n        callers **must** pass an explicit seed to guarantee reproducibility.\n\n    Returns\n    -------\n    int\n        The seed actually set (useful for logging).\n    \"\"\"\n    if seed is None:\n        raise ValueError(\"`seed` must be an explicit integer (not None).\")\n\n    if not isinstance(seed, int):\n        raise TypeError(f\"Seed must be int, not {type(seed)}.\")\n\n    if seed < 0 or seed > 2**32 - 1:\n        raise ValueError(\"`seed` must be in the range [0, 2**32 - 1].\")\n\n    # Python hash seed ------------------------------------------------------- #\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    # Standard library random ------------------------------------------------ #\n    random.seed(seed)\n    # NumPy ------------------------------------------------------------------ #\n    np.random.seed(seed)\n    # PyTorch ---------------------------------------------------------------- #\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    # Deterministic CuDNN behaviour (may impact speed) ----------------------- #\n    torch.backends.cudnn.deterministic = True  # type: ignore[attr-defined]\n    torch.backends.cudnn.benchmark = False  # type: ignore[attr-defined]\n\n    return seed\n\n\n# --------------------------------------------------------------------------- #\n# Dictionary flattening helper\n# --------------------------------------------------------------------------- #\ndef flatten_dict(nested: Dict[str, Any], parent_key: str = \"\", sep: str = \"/\") -> Dict[str, Any]:\n    \"\"\"Flatten *nested* dictionaries into a single-level dict.\n\n    Keys from deeper levels are concatenated with *sep*.\n\n    Example\n    -------\n    >>> flatten_dict({\"a\": {\"b\": 1, \"c\": 2}})\n    {'a/b': 1, 'a/c': 2}\n\n    Lists and tuples are supported by using the **index** as the next\n    key component:\n\n    >>> flatten_dict({\"metrics\": [\"bleu\", \"rouge\"]})\n    {'metrics/0': 'bleu', 'metrics/1': 'rouge'}\n    \"\"\"\n    items: Dict[str, Any] = {}\n\n    def _rec(obj: Any, prefix: str = \"\") -> None:  # noqa: WPS430\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                key = f\"{prefix}{sep}{k}\" if prefix else str(k)\n                _rec(v, key)\n        elif isinstance(obj, (list, tuple)):\n            for idx, v in enumerate(obj):\n                key = f\"{prefix}{sep}{idx}\" if prefix else str(idx)\n                _rec(v, key)\n        else:\n            items[prefix] = _safe_val(obj)\n\n    def _safe_val(val: Any) -> Any:\n        # Convert non JSON-serialisable values to string representation.\n        basic_types = (str, int, float, bool, type(None))\n        return val if isinstance(val, basic_types) else str(val)\n\n    _rec(nested, parent_key)\n    return items\n\n\n# --------------------------------------------------------------------------- #\n# Token length guard\n# --------------------------------------------------------------------------- #\ndef length_limit(\n    tokenizer: PreTrainedTokenizerBase,\n    text: str,\n    max_tokens: int,\n    *,\n    newline_token: str = \"\\n\",\n) -> str:\n    \"\"\"Ensure *text* encodes to no more than *max_tokens* tokens.\n\n    If *text* already complies, it's returned unmodified.  Otherwise we\n    iteratively drop lines from the end (split on ``newline_token``) until\n    token budget fits.  As a final fallback, we truncate by tokens.\n\n    Parameters\n    ----------\n    tokenizer:\n        Hugging-Face tokenizer configured for the *generator* model.\n    text:\n        The prompt string to be constrained.\n    max_tokens:\n        Maximum number of encoder tokens allowed.\n    newline_token:\n        Line delimiter used when pruning whole lines (default ``\\\\n``).\n\n    Returns\n    -------\n    str\n        The possibly truncated text no longer than *max_tokens* tokens.\n\n    Raises\n    ------\n    RuntimeError\n        If the function fails to shorten the text below the token budget.\n    \"\"\"\n    if max_tokens <= 0:\n        raise ValueError(\"`max_tokens` must be positive.\")\n\n    def _n_tokens(t: str) -> int:\n        # Fast token counting without tensor allocation\n        return len(tokenizer(t, add_special_tokens=False, return_attention_mask=False).input_ids)\n\n    # Fast path -------------------------------------------------------------- #\n    if _n_tokens(text) <= max_tokens:\n        return text\n\n    # Attempt to remove whole lines first ----------------------------------- #\n    lines: List[str] = text.split(newline_token)\n    while lines and _n_tokens(newline_token.join(lines)) > max_tokens:\n        lines.pop()  # drop from end (least important context)\n    truncated = newline_token.join(lines)\n    if lines and _n_tokens(truncated) <= max_tokens:\n        return truncated\n\n    # Fallback: token level truncation -------------------------------------- #\n    # Directly cut the token list and re-decode; this guarantees compliance.\n    ids = tokenizer(text, add_special_tokens=False, return_attention_mask=False).input_ids\n    if not ids:\n        raise RuntimeError(\"Cannot truncate an empty prompt.\")\n    limited_ids = ids[:max_tokens]\n    limited_text = tokenizer.decode(limited_ids, clean_up_tokenization_spaces=False)\n\n    if _n_tokens(limited_text) > max_tokens:\n        # Should never happen, but double-check to be safe.\n        raise RuntimeError(\"Failed to truncate prompt within the token limit.\")\n    return limited_text\n\n\n# --------------------------------------------------------------------------- #\n# Logging\n# --------------------------------------------------------------------------- #\ndef _ensure_dir(path: Path) -> None:\n    path.mkdir(parents=True, exist_ok=True)\n\n\ndef setup_logging(cfg: Any, name: str | None = None) -> logging.Logger:\n    \"\"\"Initialise and return a module-specific logger.\n\n    Parameters\n    ----------\n    cfg:\n        A config object that **must** expose attribute `random_seed`\n        (only used for early logging).  Using *Any* to avoid importing\n        `Config` here and creating circular dependencies.\n    name:\n        The logger name; defaults to the caller's module ``__name__``.\n\n    Returns\n    -------\n    logging.Logger\n        A fully configured logger instance.\n    \"\"\"\n    root_logger = logging.getLogger()\n    if not root_logger.handlers:\n        # ------------------------------------------------------------------- #\n        # Handler initialisation runs only ONCE regardless of how many\n        # modules import utils.setup_logging\n        # ------------------------------------------------------------------- #\n        root_logger.setLevel(logging.DEBUG)  # capture all; handlers filter\n        log_level = logging.DEBUG if getattr(cfg, \"debug\", False) else logging.INFO\n\n        formatter = logging.Formatter(\n            fmt=\"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n            datefmt=\"%Y-%m-%dT%H:%M:%S\",\n        )\n\n        # Console handler ---------------------------------------------------- #\n        console = logging.StreamHandler(stream=sys.stdout)\n        console.setLevel(log_level)\n        console.setFormatter(formatter)\n        root_logger.addHandler(console)\n\n        # File handler ------------------------------------------------------- #\n        logs_dir = Path(\"logs\")\n        _ensure_dir(logs_dir)\n        timestamp = _dt.datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n        file_path = logs_dir / f\"seqa_{timestamp}.log\"\n        file_handler = logging.FileHandler(file_path, mode=\"w\", encoding=\"utf-8\")\n        file_handler.setLevel(log_level)\n        file_handler.setFormatter(formatter)\n        root_logger.addHandler(file_handler)\n\n        # First message ------------------------------------------------------ #\n        root_logger.info(\"Logging initialised.  Log level: %s\", logging.getLevelName(log_level))\n        root_logger.debug(\"Random seed set to %s.\", getattr(cfg, \"random_seed\", \"N/A\"))\n\n    return logging.getLogger(name or __name__)\n\n```\n\n```python\n## File name: dataset_loader.py\n## dataset_loader.py\n\"\"\"\ndataset_loader.py\n\nImplementation of the `DatasetLoader` class declared in the project\ndesign.  It provides a *thin* abstraction layer over Hugging-Face\n`datasets` so that the remainder of the code-base can treat every QA\nbenchmark through a uniform dictionary schema:\n\n    {\n        \"id\":       str,   # unique sample identifier\n        \"question\": str,   # natural-language query\n        \"context\":  str,   # (very) long document(s) merged\n        \"answer\":   str    # reference answer (single string)\n    }\n\nThe loader supports eager `.load()` as well as streaming\n`.iter_dataloader()` access, covers all datasets cited in the target\npaper (HotpotQA *Distractor* and TriviaQA *RC*) and is trivially\nextensible via the internal `_DATASET_REGISTRY` table.\n\nThe heavy lifting of **chunking**, **embedding** and **Bayesian\nfiltering** is *not* performed here; the loader is strictly responsible\nfor fetching and normalising raw data.\n\nUsage Example\n-------------\n>>> from config import Config\n>>> from dataset_loader import DatasetLoader\n>>> cfg = Config.load(\"config.yaml\")\n>>> dl  = DatasetLoader(cfg, dataset_key=\"hotpotqa_distractor\")\n>>> samples = dl.load()\n>>> print(samples[0].keys())\ndict_keys(['id', 'question', 'context', 'answer'])\n\"\"\"\n\nfrom __future__ import annotations\n\nimport itertools\nimport random\nimport unicodedata\nfrom typing import Dict, Iterator, List, Sequence\n\nfrom datasets import load_dataset  # type: ignore\n\nfrom config import Config\nfrom utils import setup_logging, seed_everything\n\n# --------------------------------------------------------------------------- #\n# Dataset-specific parsing helpers                                            #\n# --------------------------------------------------------------------------- #\n\n\ndef _normalise(text: str) -> str:\n    \"\"\"Apply minimal Unicode normalisation and strip whitespace.\"\"\"\n    return unicodedata.normalize(\"NFC\", text).strip()\n\n\n# ---------- HotpotQA (distractor) ------------------------------------------ #\n\n\ndef _parse_hotpot(example: Dict[str, object]) -> Dict[str, str]:\n    \"\"\"Convert a single *raw* HotpotQA row to the canonical schema.\"\"\"\n    # Raw schema reference:\n    # https://huggingface.co/datasets/hotpot_qa\n    ctx_segments: List[str] = []\n    for title, paragraph in example.get(\"context\", []):  # type: ignore[arg-type]\n        if not paragraph:\n            # Skip empty paragraphs gracefully.\n            continue\n        # Title prefix provides weak structure (use Markdown header style).\n        ctx_segments.append(f\"## {title}\\n{paragraph}\")\n\n    long_document = \"\\n\\n\".join(ctx_segments)\n    if not long_document:\n        long_document = \" \"  # avoid an empty context string downstream\n\n    ans_field = example.get(\"answer\")\n    # Some HF versions wrap answers in dicts; others are plain strings.\n    if isinstance(ans_field, dict):\n        answer_str: str = ans_field.get(\"spans\", ans_field.get(\"text\", \"\"))  # type: ignore[arg-type]\n        if isinstance(answer_str, list):\n            # Choose first answer if multiple.\n            answer_str = answer_str[0] if answer_str else \"\"\n    else:\n        answer_str = str(ans_field)\n\n    return {\n        \"id\": str(example.get(\"id\")),\n        \"question\": _normalise(str(example.get(\"question\", \"\"))),\n        \"context\": _normalise(long_document),\n        \"answer\": _normalise(answer_str),\n    }\n\n\n# ---------- TriviaQA (RC) --------------------------------------------------- #\n\n\ndef _parse_trivia(example: Dict[str, object]) -> Dict[str, str]:\n    \"\"\"Convert a single *raw* TriviaQA row to the canonical schema.\"\"\"\n    # Raw schema reference:\n    # https://huggingface.co/datasets/trivia_qa\n    # Extract extended context hierarchy: context > search_results > entity_pages\n    if example.get(\"context\"):\n        # 'context' is usually a list[str] of snippets\n        paragraphs = example[\"context\"]  # type: ignore[index]\n        long_document = \"\\n\\n\".join(p for p in paragraphs if p)\n    elif example.get(\"search_results\"):\n        snippets = [\n            r.get(\"snippet\", \"\")  # type: ignore[arg-type]\n            for r in example[\"search_results\"]  # type: ignore[index]\n            if r.get(\"snippet\")\n        ]\n        long_document = \"\\n\\n\".join(snippets)\n    else:\n        # Fallback to entity_pages paragraphs\n        paragraph_list: List[str] = []\n        for page in example.get(\"entity_pages\", []):  # type: ignore[arg-type]\n            paragraph_list.extend(page.get(\"paragraphs\", []))\n        long_document = \"\\n\\n\".join(paragraph_list)\n\n    if not long_document:\n        long_document = \" \"  # safeguard against empty contexts\n\n    # Answers in HF TriviaQA are dicts with `value` and possibly aliases.\n    ans_field = example.get(\"answer\")\n    if isinstance(ans_field, dict):\n        answer_str = ans_field.get(\"value\", \"\")\n    elif isinstance(ans_field, list):\n        answer_str = ans_field[0] if ans_field else \"\"\n    else:\n        answer_str = str(ans_field)\n\n    qid = example.get(\"question_id\") or example.get(\"id\") or hash(\n        example.get(\"question\", \"\")\n    )\n\n    return {\n        \"id\": str(qid),\n        \"question\": _normalise(str(example.get(\"question\", \"\"))),\n        \"context\": _normalise(long_document),\n        \"answer\": _normalise(str(answer_str)),\n    }\n\n\n# --------------------------------------------------------------------------- #\n# Dataset registry \u2013 add new datasets here                                    #\n# --------------------------------------------------------------------------- #\n_DATASET_REGISTRY: Dict[str, Dict[str, object]] = {\n    \"hotpotqa_distractor\": {\n        \"hf_name\": \"hotpot_qa\",\n        \"hf_config\": \"distractor\",\n        \"split\": \"validation\",\n        \"parser\": _parse_hotpot,\n        \"expected_len\": 7405,  # reference value for sanity check\n    },\n    \"triviaqa_rc\": {\n        \"hf_name\": \"trivia_qa\",\n        \"hf_config\": \"rc\",\n        \"split\": \"validation\",\n        \"parser\": _parse_trivia,\n        \"expected_len\": 11313,\n    },\n}\n\n\n# --------------------------------------------------------------------------- #\n# Main public class                                                           #\n# --------------------------------------------------------------------------- #\nclass DatasetLoader:  # noqa: WPS110 (name chosen deliberately)\n    \"\"\"Dataset abstraction that hides HF implementation quirks.\"\"\"\n\n    # --------------------------------------------------------------------- #\n    # Construction\n    # --------------------------------------------------------------------- #\n    def __init__(self, cfg: Config, dataset_key: str, *, split: str | None = None) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        cfg:\n            Global configuration object (singleton).\n        dataset_key:\n            String identifier from ``_DATASET_REGISTRY``.  Typically one\n            of the keys listed in *config.yaml* under ``experiment.datasets``.\n        split:\n            Dataset split (default: registry entry or \"validation\").\n        \"\"\"\n        self._cfg = cfg\n        self._meta = _DATASET_REGISTRY.get(dataset_key)\n        if self._meta is None:\n            raise KeyError(\n                f\"Unknown dataset_key='{dataset_key}'.  \"\n                f\"Available keys: {list(_DATASET_REGISTRY.keys())}\"\n            )\n\n        self._split = split or str(self._meta.get(\"split\", \"validation\"))\n        self._logger = setup_logging(cfg, name=f\"DatasetLoader[{dataset_key}]\")\n        self._parser = self._meta[\"parser\"]  # type: ignore[assignment]\n\n        # Ensure deterministic behaviour (e.g. for shuffling later on)\n        seed_everything(cfg.random_seed)\n\n        self._logger.info(\n            \"Initialised DatasetLoader for %s (split=%s).\", dataset_key, self._split\n        )\n\n    # --------------------------------------------------------------------- #\n    # Public loading helpers\n    # --------------------------------------------------------------------- #\n    def load(self) -> List[Dict[str, str]]:\n        \"\"\"Eagerly load **all** samples into memory and return a list.\"\"\"\n        self._logger.info(\"Loading dataset eagerly into memory ...\")\n        # download + load\n        dataset = load_dataset(\n            path=self._meta[\"hf_name\"],  # type: ignore[arg-type]\n            name=self._meta[\"hf_config\"],  # type: ignore[arg-type]\n            split=self._split,\n            streaming=False,\n        )\n\n        samples: List[Dict[str, str]] = [\n            self._parser(raw)  # type: ignore[arg-type]\n            for raw in dataset  # type: ignore[arg-type]\n        ]\n\n        self._post_load_stats(samples, mode=\"eager\")\n        self._sanity_checks(samples)\n        return samples\n\n    def iter_dataloader(\n        self,\n        *,\n        batch_size: int = 1,\n        shuffle: bool = False,\n    ) -> Iterator[List[Dict[str, str]]]:\n        \"\"\"Yield batches of canonicalised samples (streaming or in-memory).\n\n        Parameters\n        ----------\n        batch_size:\n            Number of items yielded per iteration.  Set ``batch_size==1`` for\n            true streaming mode (memory efficient).\n        shuffle:\n            Whether to iterate in a shuffled order; deterministic given the\n            random seed from *Config*.\n        \"\"\"\n        if batch_size <= 0:\n            raise ValueError(\"`batch_size` must be a positive integer.\")\n\n        # Decide between streaming vs eager underlying dataset\n        streaming = batch_size == 1 and not shuffle\n        dataset = load_dataset(\n            path=self._meta[\"hf_name\"],  # type: ignore[arg-type]\n            name=self._meta[\"hf_config\"],  # type: ignore[arg-type]\n            split=self._split,\n            streaming=streaming,\n        )\n\n        if streaming:\n            # Iteratively parse and yield batches of size 1 (no shuffle)\n            it = map(self._parser, dataset)  # type: ignore[arg-type]\n            yield from (([item] for item in it))\n            return  # streaming path ends here\n\n        # ----- Non-streaming branch: load into memory first --------------- #\n        buffer: List[Dict[str, str]] = [\n            self._parser(ex)  # type: ignore[arg-type]\n            for ex in dataset  # type: ignore[arg-type]\n        ]\n\n        if shuffle:\n            random.Random(self._cfg.random_seed).shuffle(buffer)\n\n        # Chunk into batches\n        for idx in range(0, len(buffer), batch_size):\n            yield buffer[idx : idx + batch_size]  # noqa: WPS362 (slice OK)\n\n    # --------------------------------------------------------------------- #\n    # Private helpers\n    # --------------------------------------------------------------------- #\n    def _post_load_stats(self, samples: Sequence[Dict[str, str]], *, mode: str) -> None:\n        \"\"\"Log basic corpus statistics for sanity checking.\"\"\"\n        n = len(samples)\n        avg_len = int(\n            sum(len(s[\"context\"]) for s in samples) / n if n else 0,\n        )\n        self._logger.info(\n            \"Loaded %d samples (%s mode).  Avg context chars: %d\",\n            n,\n            mode,\n            avg_len,\n        )\n\n    def _sanity_checks(self, samples: Sequence[Dict[str, str]]) -> None:\n        \"\"\"Lightweight validation that parsed examples look reasonable.\"\"\"\n        exp_len = self._meta.get(\"expected_len\")\n        if isinstance(exp_len, int) and len(samples) != exp_len:\n            self._logger.warning(\n                \"Dataset length mismatch: expected %d, got %d.\",\n                exp_len,\n                len(samples),\n            )\n\n        # Ensure essential keys present and non-empty\n        for key in (\"id\", \"question\", \"context\", \"answer\"):\n            if any(key not in s or not isinstance(s[key], str) for s in samples):\n                raise RuntimeError(f\"Parsed samples missing mandatory key '{key}'.\")\n\n        # Check there is at least some context\n        if any(len(s[\"context\"].strip()) == 0 for s in samples):\n            self._logger.warning(\"One or more samples have empty context strings.\")\n\n\n```\n\n```python\n## File name: chunker.py\n## chunker.py\n\"\"\"\nchunker.py\n\nImplements the `Chunker` class described in the project design.\n\nThe class converts a *single* very-long document string into an **ordered\nlist** of overlapping chunk strings whose *token* length does **not**\nexceed the maximum configured value (defaults: 300 tokens with 30-token\noverlap).  Chunk boundaries are computed **in token space** to guarantee\nfaithfulness with respect to the downstream `facebook/bart-large`\ntokeniser, which is passed to the constructor.\n\nThe implementation follows the methodology detailed in Section 3.1 of the\npaper:\n\n    \u2022 Fixed-size sliding-window   (size = M,  overlap = O)\n    \u2022 Overlap is preserved *verbatim* \u2013 no loss of information.\n    \u2022 Edge-cases (short docs, tail residue) handled gracefully.\n\nThe module is **stateless** beyond holding references to `Config`\nhyper-parameters and the tokenizer, making it safe for concurrent use.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import List\n\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\n\nfrom config import Config\nfrom utils import setup_logging\n\n__all__ = [\"Chunker\"]\n\n\nclass Chunker:  # noqa: WPS110  (name chosen intentionally; domain term)\n    \"\"\"Split long documents into fixed-size overlapping chunks.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Construction\n    # ------------------------------------------------------------------ #\n    def __init__(self, cfg: Config, tokenizer: PreTrainedTokenizerBase) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        cfg:\n            Global configuration singleton containing `chunk_size` and\n            `overlap` values amongst others.\n        tokenizer:\n            *Initialised* Hugging-Face tokenizer **identical** to the one\n            later used by the generator (BART).  It determines token\n            boundaries for chunking.\n        \"\"\"\n        self._logger = setup_logging(cfg, name=self.__class__.__name__)\n        self._tokenizer = tokenizer\n\n        # Read hyper-parameters from config (fail fast on invalid combos)\n        self._chunk_size: int = int(getattr(cfg, \"chunk_size\"))\n        self._overlap: int = int(getattr(cfg, \"overlap\"))\n        if self._chunk_size <= 0:\n            raise ValueError(\"`chunk_size` must be positive.\")\n        if not (0 <= self._overlap < self._chunk_size):\n            raise ValueError(\"`overlap` must satisfy 0 \u2264 overlap < chunk_size.\")\n\n        # Derived constant: stride length\n        self._step: int = self._chunk_size - self._overlap\n\n        self._logger.debug(\n            \"Initialised Chunker (chunk_size=%d, overlap=%d, step=%d).\",\n            self._chunk_size,\n            self._overlap,\n            self._step,\n        )\n\n    # ------------------------------------------------------------------ #\n    # Public API\n    # ------------------------------------------------------------------ #\n    def split(self, doc: str) -> List[str]:\n        \"\"\"Split *doc* into an ordered list of overlapping chunks.\n\n        The *token* length of each returned chunk, when re-tokenised with\n        the **same** tokenizer, is guaranteed to be \u2264 `chunk_size`.\n\n        Parameters\n        ----------\n        doc:\n            Raw document text (can be arbitrarily long).\n\n        Returns\n        -------\n        List[str]\n            List of chunk strings; may be empty if *doc* is empty or\n            whitespace-only.\n        \"\"\"\n        if not isinstance(doc, str):\n            raise TypeError(f\"`doc` must be str, not {type(doc)}.\")\n\n        stripped = doc.strip()\n        if not stripped:\n            # Nothing to do \u2013 avoid useless tokeniser calls.\n            self._logger.debug(\"Received empty/whitespace document; returning [].\")\n            return []\n\n        # Tokenise the *entire* document once (fast path)\n        input_ids = self._tokenizer.encode(\n            stripped,\n            add_special_tokens=False,\n            return_attention_mask=False,\n        )\n        n_tokens = len(input_ids)\n        if n_tokens == 0:\n            # Highly unlikely, but guard against pathological tokenisers.\n            self._logger.warning(\"Tokeniser returned 0 tokens for non-empty doc.\")\n            return []\n\n        # Sliding-window segmentation\n        chunks: List[str] = []\n        for start in range(0, n_tokens, self._step):\n            end = min(start + self._chunk_size, n_tokens)\n            chunk_ids = input_ids[start:end]\n\n            # Decode back to text; remove leading/trailing spaces.\n            chunk_text = self._tokenizer.decode(\n                chunk_ids,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=True,\n            ).strip()\n\n            if chunk_text:  # guard against potential empty decodes\n                chunks.append(chunk_text)\n\n            # Early exit if we've reached the end (avoids an extra loop)\n            if end == n_tokens:\n                break\n\n        # Sanity check: no chunk should exceed budget\n        for idx, chunk in enumerate(chunks):\n            length = len(\n                self._tokenizer(\n                    chunk,\n                    add_special_tokens=False,\n                    return_attention_mask=False,\n                ).input_ids\n            )\n            if length > self._chunk_size:\n                # This should never happen \u2013 indicates either a bug or\n                # the tokenizer behaving unexpectedly.\n                raise RuntimeError(\n                    f\"Chunk {idx} exceeds token budget \"\n                    f\"({length} > {self._chunk_size}).\"\n                )\n\n        self._logger.debug(\n            \"Split document (%d tok) into %d chunk(s).\", n_tokens, len(chunks)\n        )\n        return chunks\n\n    # ------------------------------------------------------------------ #\n    # Debug helpers\n    # ------------------------------------------------------------------ #\n    def __repr__(self) -> str:  # noqa: D401  (single-line description)\n        return (\n            f\"Chunker(chunk_size={self._chunk_size}, \"\n            f\"overlap={self._overlap}, step={self._step})\"\n        )\n\n```\n\n```python\n## File name: encoder.py\n## encoder.py\n\"\"\"\nencoder.py\n\nVector encoder for document chunks used by the Bayesian novelty filter.\n\nThe Encoder class is intentionally minimal and **stateless** with respect\nto gradients: the underlying SentenceTransformer is *frozen* (we never\ncall `.train()` nor compute gradients).  Its sole purpose is to map lists\nof strings into L2-normalised float32 vectors suitable for cosine\nsimilarity computations.\n\nPublic Interface\n----------------\nclass Encoder:\n    __init__(cfg: Config)\n        Loads the sentence-level embedding model specified in *cfg* (or a\n        sensible default) onto the appropriate device.\n\n    embed(chunks: List[str]) -> torch.Tensor\n        Returns a 2-D tensor of shape ``(len(chunks), dim)`` where each row\n        has unit L2-norm.  The tensor lives on the same device as the\n        underlying model (GPU if available, else CPU).\n\nNotes\n-----\n*   The chosen default model is\n    ``sentence-transformers/all-MiniLM-L6-v2`` (dim = 384).  This can be\n    overridden by adding the key ``sentence_encoder_name`` to *config.yaml*.\n*   A conservative default batch size of **64** is used unless the config\n    provides ``encoder_batch_size``.\n*   Encoder exposes its internal **tokenizer** via the attribute\n    ``self.tokenizer`` so that the `Chunker` can reuse the exact same\n    tokenisation rules if desired.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom sentence_transformers import SentenceTransformer  # type: ignore\n\nfrom config import Config\nfrom utils import seed_everything, setup_logging\n\n__all__ = [\"Encoder\"]\n\n\nclass Encoder:  # noqa: WPS110 (name chosen deliberately \u2013 domain term)\n    \"\"\"L2-normalised sentence embedding generator.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Construction\n    # ------------------------------------------------------------------ #\n    def __init__(self, cfg: Config) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        cfg:\n            Global, *frozen* configuration instance.  Only **read access**\n            is performed; the object is never mutated.\n        \"\"\"\n        # Ensure deterministic weight initialisation, dropout, etc.\n        seed_everything(cfg.random_seed)\n\n        self._logger = setup_logging(cfg, name=self.__class__.__name__)\n        self._cfg = cfg\n\n        # -------------------- Model selection --------------------------- #\n        default_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n        self._model_name: str = getattr(cfg, \"sentence_encoder_name\", default_model)\n        if not isinstance(self._model_name, str) or not self._model_name:\n            self._logger.warning(\n                \"Invalid or empty sentence_encoder_name in config; \"\n                \"falling back to '%s'.\",\n                default_model,\n            )\n            self._model_name = default_model\n\n        # -------------------- Device selection -------------------------- #\n        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        self._logger.info(\"Loading sentence encoder '%s' on %s \u2026\", self._model_name, self.device)\n\n        # -------------------- Model loading ----------------------------- #\n        try:\n            self._model = SentenceTransformer(self._model_name, device=str(self.device))\n        except RuntimeError as exc:\n            # Graceful degradation when CUDA OOM or other GPU issues occur.\n            if \"CUDA\" in str(exc) and self.device.type == \"cuda\":\n                self._logger.error(\n                    \"Failed to load model on GPU due to: %s. \"\n                    \"Retrying on CPU \u2014 this will be slower.\",\n                    exc,\n                )\n                self.device = torch.device(\"cpu\")\n                self._model = SentenceTransformer(self._model_name, device=str(self.device))\n            else:\n                raise\n\n        self.dim: int = self._model.get_sentence_embedding_dimension()\n        # Expose tokenizer for `Chunker` reuse (SentenceTransformer has `.tokenizer`)\n        self.tokenizer = getattr(self._model, \"tokenizer\", None)\n\n        self._batch_size: int = int(getattr(cfg, \"encoder_batch_size\", 64))\n        if self._batch_size <= 0:\n            self._logger.warning(\n                \"encoder_batch_size must be > 0; defaulting to 64.\"\n            )\n            self._batch_size = 64\n\n        self._logger.info(\n            \"Sentence encoder ready (dim=%d, batch_size=%d).\",\n            self.dim,\n            self._batch_size,\n        )\n\n    # ------------------------------------------------------------------ #\n    # Private helpers\n    # ------------------------------------------------------------------ #\n    @staticmethod\n    def _l2_normalise(vectors: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n        \"\"\"Return L2-normalised copy of *vectors* along dim=1.\"\"\"\n        return F.normalize(vectors, p=2, dim=1, eps=eps)\n\n    # ------------------------------------------------------------------ #\n    # Public API\n    # ------------------------------------------------------------------ #\n    def embed(self, chunks: List[str]) -> torch.Tensor:\n        \"\"\"Embed *chunks* into unit-length vectors.\n\n        Parameters\n        ----------\n        chunks:\n            List of chunk strings.  Order is preserved.\n\n        Returns\n        -------\n        torch.Tensor\n            Tensor of shape ``(n_chunks, dim)`` on *self.device* with dtype\n            ``torch.float32``.  If *chunks* is empty, an empty tensor with\n            zero rows is returned.\n        \"\"\"\n        if not isinstance(chunks, list):\n            raise TypeError(f\"`chunks` must be List[str], not {type(chunks)}.\")\n        if not chunks:\n            # Return an *empty* tensor with the correct dimensionality.\n            return torch.empty((0, self.dim), dtype=torch.float32, device=self.device)\n\n        show_progress_bar = len(chunks) >= 1000\n        self._logger.debug(\n            \"Encoding %d chunk(s) into %d-dim vectors (progress bar: %s).\",\n            len(chunks),\n            self.dim,\n            show_progress_bar,\n        )\n\n        with torch.inference_mode():  # alias for torch.no_grad() in modern PyTorch\n            embeddings: torch.Tensor = self._model.encode(\n                chunks,\n                batch_size=self._batch_size,\n                show_progress_bar=show_progress_bar,\n                convert_to_tensor=True,\n                device=str(self.device),\n                normalize_embeddings=False,  # we'll normalise ourselves\n            )\n\n        if embeddings.dtype != torch.float32:\n            embeddings = embeddings.float()\n\n        embeddings = self._l2_normalise(embeddings)\n\n        # Sanity check: each row should have ||x||\u2082 \u2248 1.\n        if torch.any(torch.isnan(embeddings)):\n            raise RuntimeError(\"NaN encountered in embeddings.\")\n\n        return embeddings\n\n    # ------------------------------------------------------------------ #\n    # Dunder helpers\n    # ------------------------------------------------------------------ #\n    def __repr__(self) -> str:  # noqa: D401 (short description acceptable)\n        return (\n            f\"Encoder(model='{self._model_name}', device='{self.device.type}', \"\n            f\"dim={self.dim}, batch_size={self._batch_size})\"\n        )\n\n```\n\n```python\n## File name: bayesian_filter.py\n## bayesian_filter.py\n\"\"\"\nbayesian_filter.py\n\nImplementation of the BayesianFilter class responsible for adaptive\nnovelty-based chunk selection as described in Section 3.2 of the paper\n\u201cStatistical Estimation for Large-Scale Question Answering\u201d.\n\nThe filter operates **online** over a sequence of pre-embedded chunks,\nmaintaining a Bayesian posterior over novelty scores and a running\ncontext vector.  Chunks whose novelty exceeds an adaptive threshold\nare accepted and later passed to the prompt builder.\n\nPublic API\n----------\nclass BayesianFilter:\n    __init__(cfg: Config)\n    reset() -> None\n    filter(chunks: List[str], embeds: torch.Tensor) -> List[str]\n\n(All other methods are private helpers and must not be called\ndirectly from outside the class.)\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import List\n\nimport torch\nimport torch.nn.functional as F\n\nfrom config import Config\nfrom utils import setup_logging\n\n__all__ = [\"BayesianFilter\"]\n\n\nclass BayesianFilter:  # noqa: WPS110  (name chosen intentionally \u2013 domain term)\n    \"\"\"Adaptive Bayesian novelty filter for document chunks.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Construction\n    # ------------------------------------------------------------------ #\n    def __init__(self, cfg: Config) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        cfg:\n            Immutable experiment-wide configuration object loaded from\n            *config.yaml*.  Hyper-parameters relevant to this class:\n\n            \u2022 cfg.bayes_mu0          (float)  \u2013 prior mean  \u03bc\u2080\n            \u2022 cfg.bayes_sigma0       (float)  \u2013 prior std-dev \u03c3\u2080\n            \u2022 cfg.bayes_sigma_obs    (float)  \u2013 observation noise variance \u03c3\u00b2_obs\n            \u2022 cfg.bayes_beta         (float)  \u2013 margin coefficient \u03b2\n        \"\"\"\n        self._logger = setup_logging(cfg, name=self.__class__.__name__)\n\n        # --- Hyper-parameters (validated) --------------------------------- #\n        self._mu0: float = float(getattr(cfg, \"bayes_mu0\", 0.5))\n        self._sigma0: float = float(getattr(cfg, \"bayes_sigma0\", 0.1))\n        self._sigma_obs: float = float(getattr(cfg, \"bayes_sigma_obs\", 0.05))\n        self._beta: float = float(getattr(cfg, \"bayes_beta\", 1.0))\n\n        if self._sigma0 <= 0:\n            raise ValueError(\"bayes_sigma0 must be > 0.\")\n        if self._sigma_obs <= 0:\n            raise ValueError(\"bayes_sigma_obs must be > 0.\")\n        if self._beta <= 0:\n            raise ValueError(\"bayes_beta must be > 0.\")\n\n        # ---- Internal mutable state -------------------------------------- #\n        self._mu: float = self._mu0  # posterior mean      \u03bc\n        self._sigma: float = self._sigma0  # posterior std-dev \u03c3\n        self._sigma2_obs: float = self._sigma_obs  # variance (keep as var)\n        self._v_sel: torch.Tensor | None = None  # running context vector\n        self._n_sel: int = 0  # how many chunks accepted so far\n\n        # Private flag for ablation studies (update posterior on all vs\n        # accepted chunks only).  *Not* part of the public interface.\n        self._update_on_all: bool = True\n\n        self._eps: float = 1.0e-8  # numeric stability in cosine\n\n        self._logger.debug(\n            \"BayesianFilter initialised with \u03bc0=%.4f, \u03c30=%.4f, \u03c3\u00b2_obs=%.4f, \u03b2=%.3f\",\n            self._mu0,\n            self._sigma0,\n            self._sigma2_obs,\n            self._beta,\n        )\n\n    # ------------------------------------------------------------------ #\n    # Public helpers\n    # ------------------------------------------------------------------ #\n    def reset(self) -> None:\n        \"\"\"Restore the filter to its **prior** state (fresh document).\"\"\"\n        self._mu = self._mu0\n        self._sigma = self._sigma0\n        self._v_sel = None\n        self._n_sel = 0\n        self._logger.debug(\"Filter state reset to prior.\")\n\n    # ------------------------------------------------------------------ #\n    # Main API method\n    # ------------------------------------------------------------------ #\n    def filter(self, chunks: List[str], embeds: torch.Tensor) -> List[str]:\n        \"\"\"Select a subset of *chunks* deemed sufficiently novel.\n\n        Parameters\n        ----------\n        chunks:\n            List of raw chunk strings (length K).\n        embeds:\n            Tensor of shape (K, d) containing **L2-normalised**\n            embeddings on CPU *or* GPU.\n\n        Returns\n        -------\n        List[str]\n            Ordered list of accepted chunk strings.\n        \"\"\"\n        if not isinstance(chunks, list):\n            raise TypeError(\"`chunks` must be List[str].\")\n        if embeds.ndim != 2:\n            raise ValueError(\"`embeds` must be a 2-D tensor (K \u00d7 d).\")\n        if len(chunks) != embeds.shape[0]:\n            raise ValueError(\"Length mismatch between chunks and embeds.\")\n\n        # (Re-)initialise state for this document\n        self.reset()\n\n        accepted: List[str] = []\n        device = embeds.device  # ensure all tensors live on same device\n\n        for idx, (chunk_txt, h_k) in enumerate(zip(chunks, embeds)):  # noqa: WPS359\n            # FIRST CHUNK: always accept ----------------------------------- #\n            if self._v_sel is None:\n                accepted.append(chunk_txt)\n                self._v_sel = h_k.clone()\n                self._n_sel = 1\n\n                # Update posterior with dummy novelty (set to 1.0 > \u03bc0 usually)\n                x_k = float(1.0)\n                self._update_posterior(x_k)\n                self._logger.debug(\n                    \"Accepted first chunk idx=%d (bootstrap). \u03bc=%.4f \u03c3=%.4f\",\n                    idx,\n                    self._mu,\n                    self._sigma,\n                )\n                continue\n\n            # Ensure running vector is unit-norm (might accumulate drift)\n            self._v_sel = F.normalize(self._v_sel, p=2, dim=0)\n\n            # NOVELTY SCORE (cosine distance) ------------------------------ #\n            cos_sim = torch.dot(h_k, self._v_sel).clamp(-1.0, 1.0).item()\n            x_k = 1.0 - cos_sim  # higher = more novel\n\n            # Update posterior (all chunks by default) --------------------- #\n            if self._update_on_all:\n                self._update_posterior(x_k)\n\n            # Adaptive threshold (using *updated* posterior) --------------- #\n            if self._accept(x_k):\n                # Accept chunk\n                accepted.append(chunk_txt)\n                # Running mean update of context vector\n                self._v_sel = (\n                    (self._n_sel * self._v_sel.to(device)) + h_k\n                ) / (self._n_sel + 1)\n                self._n_sel += 1\n                # Normalise after update\n                self._v_sel = F.normalize(self._v_sel, p=2, dim=0)\n\n                # If posterior should only track accepted chunks,\n                # perform update here instead.\n                if not self._update_on_all:\n                    self._update_posterior(x_k)\n\n                self._logger.debug(\n                    \"Accepted chunk idx=%d  x=%.4f  \u03c4=%.4f  (\u03bc=%.4f, \u03c3=%.4f)\",\n                    idx,\n                    x_k,\n                    self._mu + self._beta * self._sigma,\n                    self._mu,\n                    self._sigma,\n                )\n            else:\n                self._logger.debug(\n                    \"Rejected chunk idx=%d  x=%.4f  \u03c4=%.4f  (\u03bc=%.4f, \u03c3=%.4f)\",\n                    idx,\n                    x_k,\n                    self._mu + self._beta * self._sigma,\n                    self._mu,\n                    self._sigma,\n                )\n\n        # Safeguard: ensure at least *one* chunk survives\n        if not accepted and chunks:\n            self._logger.warning(\n                \"No chunk passed the novelty threshold; \"\n                \"falling back to keeping the first chunk.\"\n            )\n            accepted.append(chunks[0])\n\n        return accepted\n\n    # ------------------------------------------------------------------ #\n    # Private helpers\n    # ------------------------------------------------------------------ #\n    def _update_posterior(self, x: float) -> None:\n        \"\"\"Online conjugate update of Normal prior with known variance.\"\"\"\n        sigma2_prior = self._sigma ** 2\n        sigma2_post = 1.0 / (1.0 / sigma2_prior + 1.0 / self._sigma2_obs)\n        mu_post = sigma2_post * (\n            self._mu / sigma2_prior + x / self._sigma2_obs\n        )\n\n        # Convert variance back to std-dev\n        self._sigma = math.sqrt(max(sigma2_post, 1e-12))\n        self._mu = mu_post\n\n    def _accept(self, x: float) -> bool:\n        \"\"\"Decision rule: accept if x \u2265 \u03bc + \u03b2\u03c3.\"\"\"\n        threshold = self._mu + self._beta * self._sigma\n        return x >= threshold\n\n```\n\n```python\n## File name: prompt_builder.py\n## prompt_builder.py\n\"\"\"\nprompt_builder.py\n\nAssembly of the textual *prompt* fed into the generative model\n(`facebook/bart-large`).  The class adheres to the public interface\nspecified in the project design:\n\n    class PromptBuilder:\n        + __init__(cfg: Config, tokenizer: PreTrainedTokenizerBase)\n        + build(question: str, accepted_chunks: List[str]) -> str\n\nResponsibilities\n----------------\n1. Combine system role, user question, *accepted* context chunks and an\n   explicit instruction line into a single string.\n\n2. Ensure the total number of **encoder tokens** does **not** exceed the\n   budget defined by ``cfg.max_prompt_tokens``.  If required, the context\n   section (i.e. accepted chunks) is truncated *deterministically* by\n   discarding **later** chunks (keeping earlier chunks first).\n\n3. Provide sensible fall-backs for edge-cases (e.g., zero accepted\n   chunks) without breaking downstream generation.\n\nImplementation notes\n--------------------\n* Token counting uses the exact same *tokenizer* instance as the\n  generator to guarantee consistency.\n* A tiny utility function `_n_tokens` performs fast counting with\n  ``add_special_tokens=False`` because full special-token handling is\n  applied only once inside the generator itself.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import List\n\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\n\nfrom config import Config\nfrom utils import length_limit, setup_logging\n\n__all__ = [\"PromptBuilder\"]\n\n\nclass PromptBuilder:  # noqa: WPS110 (domain term)\n    \"\"\"Construct textual prompts within a fixed token budget.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Construction\n    # ------------------------------------------------------------------ #\n    def __init__(self, cfg: Config, tokenizer: PreTrainedTokenizerBase) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        cfg:\n            Immutable configuration singleton.\n        tokenizer:\n            Hugging-Face tokenizer *identical* to the one used by the\n            generator (BART).  Required for accurate token counting.\n        \"\"\"\n        self._cfg = cfg\n        self._tokenizer = tokenizer\n        self._logger = setup_logging(cfg, name=self.__class__.__name__)\n\n        self._max_prompt_tokens: int = int(getattr(cfg, \"max_prompt_tokens\"))\n        if self._max_prompt_tokens <= 0:\n            raise ValueError(\"`max_prompt_tokens` must be a positive integer.\")\n\n        # Pre-compute token count of a single newline for efficiency.\n        self._newline_toklen: int = self._count_tokens(\"\\n\")\n\n    # ------------------------------------------------------------------ #\n    # Public API\n    # ------------------------------------------------------------------ #\n    def build(self, question: str, accepted_chunks: List[str]) -> str:\n        \"\"\"Return a full prompt string respecting the token budget.\n\n        Parameters\n        ----------\n        question:\n            User natural-language question.\n        accepted_chunks:\n            Ordered list of context chunks *already* selected by the\n            Bayesian filter.  May be an empty list.\n\n        Returns\n        -------\n        str\n            The final prompt ready to be fed into the generator.\n        \"\"\"\n        if not isinstance(question, str):\n            raise TypeError(\"`question` must be of type str.\")\n        if not isinstance(accepted_chunks, list):\n            raise TypeError(\"`accepted_chunks` must be List[str].\")\n\n        # ---------------------------------------------------------------- #\n        # 1) Build *static* segments (role, question, header, ending)      #\n        # ---------------------------------------------------------------- #\n        role_line = \"You are a helpful QA system.\"\n        question_line = f\"The user asks: {question}\"\n        context_header = \"Below is relevant context filtered from a larger document:\"\n        ending_line = \"Please provide the best possible answer.\"\n\n        header_text = \"\\n\".join((role_line, question_line, context_header)) + \"\\n\"\n        tail_text = \"\\n\" + ending_line  # leading newline to separate from context\n\n        header_tokens = self._count_tokens(header_text)\n        tail_tokens = self._count_tokens(tail_text)\n\n        # ---------------------------------------------------------------- #\n        # 2) Determine token budget for *context*                          #\n        # ---------------------------------------------------------------- #\n        budget = self._max_prompt_tokens - header_tokens - tail_tokens\n        if budget <= 0:\n            # Extremely narrow budget \u2013 fallback to length-limited header.\n            self._logger.warning(\n                \"Header+tail exceed or equal total token budget (%d \u2265 %d). \"\n                \"Prompt will be truncated aggressively.\",\n                header_tokens + tail_tokens,\n                self._max_prompt_tokens,\n            )\n            raw_prompt = header_text + tail_text.lstrip(\"\\n\")\n            return length_limit(self._tokenizer, raw_prompt, self._max_prompt_tokens)\n\n        # ---------------------------------------------------------------- #\n        # 3) Greedily append chunks until budget exhausted                 #\n        # ---------------------------------------------------------------- #\n        selected_chunks: List[str] = []\n        used_tokens = 0\n        for chunk in accepted_chunks:\n            if not isinstance(chunk, str):\n                self._logger.warning(\"Encountered non-string chunk \u2013 skipping.\")\n                continue\n\n            # Token length of chunk plus preceding newline *if* not first.\n            addition_tokens = (\n                self._newline_toklen + self._count_tokens(chunk)\n                if selected_chunks\n                else self._count_tokens(chunk)\n            )\n            if used_tokens + addition_tokens > budget:\n                break  # cannot fit this (or any further) chunk\n            selected_chunks.append(chunk)\n            used_tokens += addition_tokens\n\n        if not selected_chunks:\n            selected_chunks.append(\"No additional context was deemed novel.\")\n\n        context_text = \"\\n\".join(selected_chunks)\n\n        # ---------------------------------------------------------------- #\n        # 4) Assemble full prompt & final safeguard                        #\n        # ---------------------------------------------------------------- #\n        prompt = header_text + context_text + tail_text\n\n        # Final safety net \u2013 truncate last resort (should rarely trigger)\n        total_tokens = self._count_tokens(prompt)\n        if total_tokens > self._max_prompt_tokens:\n            self._logger.warning(\n                \"Prompt length %d exceeds budget %d after assembly. \"\n                \"Applying fallback truncation.\",\n                total_tokens,\n                self._max_prompt_tokens,\n            )\n            prompt = length_limit(self._tokenizer, prompt, self._max_prompt_tokens)\n\n        return prompt\n\n    # ------------------------------------------------------------------ #\n    # Internal helpers\n    # ------------------------------------------------------------------ #\n    def _count_tokens(self, text: str) -> int:\n        \"\"\"Return the number of tokens for *text* (no special tokens).\"\"\"\n        return len(\n            self._tokenizer(\n                text,\n                add_special_tokens=False,\n                return_attention_mask=False,\n            ).input_ids\n        )\n\n    # ------------------------------------------------------------------ #\n    # Dunder\n    # ------------------------------------------------------------------ #\n    def __repr__(self) -> str:  # noqa: D401\n        return (\n            f\"PromptBuilder(max_prompt_tokens={self._max_prompt_tokens}, \"\n            f\"newline_toklen={self._newline_toklen})\"\n        )\n\n```\n\n```python\n## File name: generator.py\n## generator.py\n\"\"\"\ngenerator.py\n\nEncapsulates loading of the sequence-to-sequence generator\n(`facebook/bart-large` by default) and provides a single public method\n`answer(prompt: str) -> str` that turns a fully-formed prompt into a\nnatural-language answer, strictly following the hyper-parameters laid out\nin *config.yaml* and the reproduction design document.\n\nKey Features\n------------\n\u2022  Stateless *w.r.t.* Bayesian filtering or dataset specifics.\n\u2022  Deterministic thanks to `utils.seed_everything(cfg.random_seed)`.\n\u2022  Transparent device placement (CUDA if available; otherwise CPU with\n   a performance warning).\n\u2022  Hard enforcement of the encoder token budget to avoid position-\n   embedding overflow in BART (default 1024 tokens).\n\u2022  Minimal set of generation arguments \u2013 **only** those explicitly\n   mentioned in the paper/config are used.\n\nPublic Interface\n----------------\nclass Generator:\n    __init__(cfg: Config)\n    answer(prompt: str) -> str\n    (optional) answer_batch(prompts: List[str]) -> List[str]\n\"\"\"\n\nfrom __future__ import annotations\n\nimport warnings\nfrom typing import List, Sequence\n\nimport torch\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    PreTrainedTokenizerBase,\n)\n\nfrom config import Config\nfrom utils import seed_everything, setup_logging\n\n__all__ = [\"Generator\"]\n\n\nclass Generator:  # noqa: WPS110 (domain term)\n    \"\"\"Wrapper around a Hugging-Face seq-to-seq *generator* model.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Construction\n    # ------------------------------------------------------------------ #\n    def __init__(self, cfg: Config) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        cfg:\n            Immutable configuration singleton loaded via `Config.load`.\n        \"\"\"\n        # ---------------------------------------------------------------- #\n        # Seeding for deterministic generation\n        # ---------------------------------------------------------------- #\n        seed_everything(cfg.random_seed)\n\n        # ---------------------------------------------------------------- #\n        # Logging\n        # ---------------------------------------------------------------- #\n        self._logger = setup_logging(cfg, name=self.__class__.__name__)\n\n        # ---------------------------------------------------------------- #\n        # Hyper-parameters pulled from config\n        # ---------------------------------------------------------------- #\n        self._model_name: str = getattr(cfg, \"model_name\")\n        self._num_beams: int = int(getattr(cfg, \"beam\"))\n        # Private attribute in Config (leading underscore); fall back safe.\n        self._max_gen_len: int = int(getattr(cfg, \"_max_gen_length\", 128))\n        self._enc_max_tokens: int = int(getattr(cfg, \"max_prompt_tokens\"))\n\n        # ---------------------------------------------------------------- #\n        # Device selection\n        # ---------------------------------------------------------------- #\n        if torch.cuda.is_available():\n            self.device = torch.device(\"cuda\")\n            self._logger.info(\"Using GPU device for generation.\")\n        else:\n            self.device = torch.device(\"cpu\")\n            warnings.warn(\n                \"CUDA not available \u2013 running BART-Large on CPU. \"\n                \"This will be considerably slower.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n            self._logger.info(\"Using CPU device for generation.\")\n\n        # ---------------------------------------------------------------- #\n        # Tokeniser\n        # ---------------------------------------------------------------- #\n        try:\n            self.tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(\n                self._model_name,\n                use_fast=True,\n            )\n        except OSError as exc:  # network issues / wrong id\n            self._logger.exception(\"Failed to load tokenizer '%s'.\", self._model_name)\n            raise\n\n        # Safety: ensure `pad_token_id` is set (important for batching)\n        if self.tokenizer.pad_token_id is None:\n            self._logger.warning(\n                \"Tokenizer '%s' lacks pad_token_id \u2013 setting it to eos_token_id.\",\n                self._model_name,\n            )\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n\n        # ---------------------------------------------------------------- #\n        # Model\n        # ---------------------------------------------------------------- #\n        try:\n            self.model: AutoModelForSeq2SeqLM = AutoModelForSeq2SeqLM.from_pretrained(\n                self._model_name,\n                torch_dtype=torch.float16 if self.device.type == \"cuda\" else None,\n            )\n        except OSError:\n            self._logger.exception(\"Failed to load model '%s'.\", self._model_name)\n            raise\n\n        # Move to device and switch to inference mode\n        self.model.to(self.device)\n        self.model.eval()\n\n        # ---------------------------------------------------------------- #\n        # Log summary\n        # ---------------------------------------------------------------- #\n        n_params = sum(p.numel() for p in self.model.parameters()) / 1_000_000\n        self._logger.info(\n            \"Generator ready \u2013 %s (%.1fM params, beams=%d, \"\n            \"enc_max=%d, gen_max=%d)\",\n            self._model_name,\n            n_params,\n            self._num_beams,\n            self._enc_max_tokens,\n            self._max_gen_len,\n        )\n\n    # ------------------------------------------------------------------ #\n    # Public API\n    # ------------------------------------------------------------------ #\n    @torch.inference_mode()\n    def answer(self, prompt: str) -> str:  # noqa: WPS231 (complexity acceptable)\n        \"\"\"Generate an answer string for a *single* prompt.\n\n        Parameters\n        ----------\n        prompt:\n            Fully-formed textual prompt (including question, context,\n            instructions).  Must not be empty.\n\n        Returns\n        -------\n        str\n            Model-generated answer, stripped of surrounding whitespace.\n\n        Raises\n        ------\n        ValueError\n            If *prompt* is empty or contains only whitespace.\n        RuntimeError\n            If generation fails due to model/tokeniser mis-configuration.\n        \"\"\"\n        if not isinstance(prompt, str) or not prompt.strip():\n            raise ValueError(\"`prompt` must be a non-empty string.\")\n\n        # ---------------------------------------------------------------- #\n        # Tokenisation with hard truncation (safeguard)\n        # ---------------------------------------------------------------- #\n        inputs = self.tokenizer(\n            prompt,\n            max_length=self._enc_max_tokens,\n            truncation=True,\n            return_tensors=\"pt\",\n            padding=False,  # prompt builder already fits budget\n        )\n\n        # Explicitly check truncation happened and warn once\n        if inputs.input_ids.shape[1] == self._enc_max_tokens and len(\n            self.tokenizer(prompt, add_special_tokens=False).input_ids\n        ) > self._enc_max_tokens:\n            self._logger.warning(\n                \"Prompt was truncated to %d tokens to fit encoder budget.\",\n                self._enc_max_tokens,\n            )\n\n        # Move tensors to device\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n        # ---------------------------------------------------------------- #\n        # Generation\n        # ---------------------------------------------------------------- #\n        generated_ids = self.model.generate(\n            **inputs,\n            max_length=self._max_gen_len,\n            num_beams=self._num_beams,\n            early_stopping=True,\n            length_penalty=1.0,  # paper does not modify\n            repetition_penalty=1.0,  # paper does not mention\n            pad_token_id=self.tokenizer.pad_token_id,\n        )\n        if generated_ids.ndim != 2 or generated_ids.size(0) == 0:\n            raise RuntimeError(\"Model.generate() returned an unexpected shape.\")\n\n        answer_str: str = self.tokenizer.decode(\n            generated_ids[0],\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        ).strip()\n\n        return answer_str or \"\"  # never return None\n\n    # -------------------- OPTIONAL: batched generation ------------------- #\n    @torch.inference_mode()\n    def answer_batch(self, prompts: Sequence[str]) -> List[str]:\n        \"\"\"Batch version of `answer` for improved throughput.\n\n        Not used directly in the reproduction pipeline but provided for\n        convenience (e.g., during evaluation).\n\n        Parameters\n        ----------\n        prompts:\n            Iterable of prompt strings.\n\n        Returns\n        -------\n        List[str]\n            Generated answers in the same order.\n        \"\"\"\n        if not prompts:\n            return []\n\n        # Tokenise with *individual* truncation to maintain alignment\n        encodings = self.tokenizer(\n            list(prompts),\n            max_length=self._enc_max_tokens,\n            truncation=True,\n            padding=True,  # pad to longest in batch\n            return_tensors=\"pt\",\n        ).to(self.device)\n\n        generated = self.model.generate(\n            input_ids=encodings.input_ids,\n            attention_mask=encodings.attention_mask,\n            max_length=self._max_gen_len,\n            num_beams=self._num_beams,\n            early_stopping=True,\n            length_penalty=1.0,\n            repetition_penalty=1.0,\n            pad_token_id=self.tokenizer.pad_token_id,\n        )\n\n        answers = [\n            self.tokenizer.decode(\n                ids,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=True,\n            ).strip()\n            for ids in generated\n        ]\n        return answers\n\n    # ------------------------------------------------------------------ #\n    # Dunder helpers\n    # ------------------------------------------------------------------ #\n    def __repr__(self) -> str:  # noqa: D401\n        return (\n            f\"Generator(model='{self._model_name}', device='{self.device.type}', \"\n            f\"beams={self._num_beams}, enc_max={self._enc_max_tokens}, \"\n            f\"gen_max={self._max_gen_len})\"\n        )\n\n```\n\n```python\n## File name: retrieval.py\n## retrieval.py\n\"\"\"\nretrieval.py\n\nVector\u2013similarity passage retrieval used by the optional **SEQA (With\nIndex)** variant described in Section 4.2 of the paper.  A lightweight\nwrapper around *faiss* that re-uses the same sentence encoder employed\nfor chunk novelty scoring.\n\nInterface (as specified in the project design)\n----------------------------------------------\nclass Retrieval:\n    + __init__(cfg: Config, encoder: Encoder | None = None)\n    + index_corpus(corpus_path: str) -> None\n    + retrieve(query: str, k: int | None = None) -> List[str]\n\nNotes\n-----\n*  The corpus is expected to be a **JSONL** file where each line is a\n   JSON object with at minimum the key ``\"text\"``.  Example line:\n\n       {\"id\": \"Wiki_12345\", \"text\": \"Passage contents \u2026\"}\n\n   Any additional keys are ignored.  If ``\"id\"`` is missing a numeric\n   index is assigned automatically.\n\n*  FAISS `IndexFlatIP` (exact cosine similarity due to L2-normalisation)\n   is used for simplicity and reproducibility.\n\n*  All embeddings are computed via the shared `Encoder` instance to\n   guarantee consistency across the entire pipeline.\n\n*  This module is **only** imported and instantiated when\n   `cfg.retrieval_enabled` is true; otherwise the rest of the code-base\n   remains unaffected.\n\n*  No persistence (save/load) is implemented so that the file remains\n   self-contained and free of additional assumptions not present in\n   `config.yaml`.  Building the index is fast enough for moderate corpus\n   sizes (\u2272 100 k passages).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport pathlib\nfrom typing import Dict, List, Sequence\n\nimport numpy as np\n\n# faiss may be compiled with/without GPU support; import guard required.\ntry:\n    import faiss  # type: ignore\nexcept ModuleNotFoundError as exc:  # pragma: no cover\n    raise ModuleNotFoundError(\n        \"Package 'faiss' is required for retrieval but not installed.  \"\n        \"Install either 'faiss-cpu' or 'faiss-gpu' as per requirements.\"\n    ) from exc\n\nfrom config import Config\nfrom encoder import Encoder\nfrom utils import setup_logging\n\n__all__ = [\"Retrieval\"]\n\n\nclass Retrieval:  # noqa: WPS110 (domain-specific name)\n    \"\"\"Simple FAISS-based passage retriever.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Construction                                                       #\n    # ------------------------------------------------------------------ #\n    def __init__(self, cfg: Config, encoder: Encoder | None = None) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        cfg:\n            Global read-only configuration object (singleton).\n        encoder:\n            Optional *pre-initialised* sentence `Encoder`.  If ``None``,\n            the constructor will instantiate its own encoder.  Sharing an\n            encoder is recommended to save GPU memory.\n        \"\"\"\n        self._cfg = cfg\n        self._logger = setup_logging(cfg, name=self.__class__.__name__)\n\n        # Either re-use provided encoder or create a new one.\n        self._encoder = encoder if encoder is not None else Encoder(cfg)\n        self._dim: int = self._encoder.dim\n\n        # FAISS index and id\u2192text mapping initialised later.\n        self._index: faiss.Index | None = None\n        self._id_to_text: Dict[int, str] = {}\n\n        # Internal flag indicating readiness for search.\n        self._ready: bool = False\n\n        # Batch size for embedding passages (fall back to 64 if absent).\n        self._batch_size: int = int(getattr(cfg, \"encoder_batch_size\", 64)) or 64\n\n    # ------------------------------------------------------------------ #\n    # Index creation                                                     #\n    # ------------------------------------------------------------------ #\n    def index_corpus(self, corpus_path: str | pathlib.Path) -> None:  # noqa: C901\n        \"\"\"Build an in-memory FAISS index over the given *corpus_path*.\n\n        The corpus **must** be a UTF-8 encoded JSONL file with at least\n        the field ``\"text\"`` on every line.\n\n        Parameters\n        ----------\n        corpus_path:\n            File-system path to the JSONL corpus.\n        \"\"\"\n        path = pathlib.Path(corpus_path).expanduser().resolve()\n        if not path.exists():\n            raise FileNotFoundError(f\"Corpus file not found: {path}\")\n\n        self._logger.info(\"Reading corpus from %s \u2026\", path)\n        raw_passages: List[str] = []\n        with path.open(\"r\", encoding=\"utf-8\") as fp:\n            for line_number, line in enumerate(fp):\n                if not line.strip():\n                    continue  # skip blank lines\n                try:\n                    obj = json.loads(line)\n                except json.JSONDecodeError as exc:\n                    self._logger.error(\n                        \"Malformed JSON on line %d of %s: %s\",\n                        line_number + 1,\n                        path,\n                        exc,\n                    )\n                    continue\n                text = obj.get(\"text\", \"\")\n                if not isinstance(text, str) or not text.strip():\n                    # Ignore entries with empty or invalid text.\n                    continue\n                raw_passages.append(text.strip())\n\n        if not raw_passages:\n            raise RuntimeError(f\"No valid passages found in {path}.\")\n\n        self._logger.info(\n            \"Loaded %d passages.  Computing embeddings in batches of %d \u2026\",\n            len(raw_passages),\n            self._batch_size,\n        )\n\n        # Embed passages in mini-batches ---------------------------------- #\n        embeddings: List[np.ndarray] = []  # list to avoid pre-alloc OOM\n        for start in range(0, len(raw_passages), self._batch_size):\n            batch = raw_passages[start : start + self._batch_size]  # noqa: WPS362\n            vecs = self._encoder.embed(batch).cpu().numpy()  # (B, dim)\n            embeddings.append(vecs.astype(np.float32, copy=False))\n\n        # Concatenate into (N, dim) matrix.\n        mat = np.vstack(embeddings)\n        if mat.shape != (len(raw_passages), self._dim):\n            raise RuntimeError(\"Unexpected embedding matrix shape.\")\n\n        # Build FAISS index (exact, inner product) ------------------------ #\n        self._logger.info(\"Building FAISS IndexFlatIP (dim=%d) \u2026\", self._dim)\n        index = faiss.IndexFlatIP(self._dim)\n        index.add(mat)  # type: ignore[arg-type]  (expects float32 np.ndarray)\n\n        self._index = index\n        # row-id \u2192 text mapping\n        self._id_to_text = {idx: txt for idx, txt in enumerate(raw_passages)}\n        self._ready = True\n\n        self._logger.info(\n            \"Retrieval index ready.  Total vectors: %d\", self._index.ntotal\n        )\n\n    # ------------------------------------------------------------------ #\n    # Search                                                             #\n    # ------------------------------------------------------------------ #\n    def retrieve(self, query: str, k: int | None = None) -> List[str]:\n        \"\"\"Return the *k* most similar passages for *query*.\n\n        Parameters\n        ----------\n        query:\n            Natural-language query string.\n        k:\n            Number of passages to return.  If ``None`` the method uses\n            ``cfg.retrieval_top_k`` (default = 3).\n\n        Returns\n        -------\n        List[str]\n            Passage texts ordered by descending similarity.  If the index\n            contains fewer than *k* passages the list will be shorter.\n        \"\"\"\n        if not self._ready or self._index is None:\n            raise RuntimeError(\n                \"Retrieval index not initialised.  Call `index_corpus()` first.\"\n            )\n\n        if not isinstance(query, str) or not query.strip():\n            raise ValueError(\"`query` must be a non-empty string.\")\n\n        top_k = int(k) if k is not None else int(getattr(self._cfg, \"retrieval_top_k\", 3))\n        if top_k <= 0:\n            raise ValueError(\"`k` must be positive or None.\")\n\n        # Encode query (result is shape (1, dim) torch.Tensor)\n        q_vec = self._encoder.embed([query]).cpu().numpy().astype(np.float32)\n\n        # Search\n        distances, indices = self._index.search(q_vec, top_k)\n        idxs: Sequence[int] = indices[0]\n\n        # Map to passage texts, filtering out invalid indices (-1)\n        results: List[str] = [\n            self._id_to_text[i]\n            for i in idxs\n            if i != -1 and i in self._id_to_text\n        ]\n        self._logger.debug(\n            \"Retrieved %d / %d passages for query '%s\u2026'.\",\n            len(results),\n            top_k,\n            query[:30],\n        )\n        return results\n\n    # ------------------------------------------------------------------ #\n    # Dunder & helper methods                                            #\n    # ------------------------------------------------------------------ #\n    def __repr__(self) -> str:  # noqa: D401\n        status = \"ready\" if self._ready else \"uninitialised\"\n        return (\n            f\"Retrieval(index_status={status}, dim={self._dim}, \"\n            f\"passages={self._index.ntotal if self._index else 0})\"\n        )\n\n```\n\n```python\n## File name: evaluation.py\n## evaluation.py\n\"\"\"\nevaluation.py\n\nCompute evaluation metrics for generative Question-Answering outputs\nexactly as described in Section 4.4 of the paper *\u201cStatistical Estimation\nfor Large-Scale Question Answering\u201d*.\n\nSupported metrics (case-insensitive keys)\n----------------------------------------\n\u2022  BLEU           \u2013 SacreBLEU corpus score (tokenise='13a')\n\u2022  ROUGE_L        \u2013 Mean F1 of ROUGE-L (Porter stemmer)\n\u2022  BERTScore      \u2013 Mean F1 using *roberta-large*\n\u2022  Perplexity     \u2013 Average perplexity measured by\n                    *EleutherAI/gpt-neo-125M*\n\nThe list of metrics to compute is read from `config.yaml`\n(`evaluation.metrics`).  If the configuration is missing or empty, **all\nfour** metrics are computed.\n\nPublic API\n----------\nclass Evaluator:\n    __init__(cfg: Config)\n    score(preds: List[str], refs: List[str]) -> dict\n\"\"\"\n\nfrom __future__ import annotations\n\nimport collections\nimport warnings\nfrom typing import Dict, List\n\nimport numpy as np\nimport torch\n\nfrom config import Config\nfrom utils import seed_everything, setup_logging\n\n__all__ = [\"Evaluator\"]\n\n\nclass Evaluator:  # noqa: WPS110  (domain term)\n    \"\"\"Metric computation wrapper adhering to design specification.\"\"\"\n\n    # ------------------------------------------------------------------ #\n    # Construction                                                       #\n    # ------------------------------------------------------------------ #\n    def __init__(self, cfg: Config) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        cfg:\n            Global, *immutable* configuration instance.\n        \"\"\"\n        # ---------------------------------------------------------------- #\n        # Reproducibility\n        # ---------------------------------------------------------------- #\n        seed_everything(cfg.random_seed)\n\n        # ---------------------------------------------------------------- #\n        # Logging\n        # ---------------------------------------------------------------- #\n        self._logger = setup_logging(cfg, name=self.__class__.__name__)\n\n        # ---------------------------------------------------------------- #\n        # Device selection (shared by heavy metrics)\n        # ---------------------------------------------------------------- #\n        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        if self.device.type == \"cpu\":\n            warnings.warn(\n                \"CUDA not available \u2013 BERTScore and Perplexity will run on CPU \"\n                \"and may be slow.\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n\n        # ---------------------------------------------------------------- #\n        # Determine which metrics to compute\n        # ---------------------------------------------------------------- #\n        requested = cfg.get(\"evaluation.metrics\", [])\n        if not requested:\n            # Default to ALL metrics when list is missing / empty\n            requested = [\"BLEU\", \"ROUGE_L\", \"BERTScore\", \"Perplexity\"]\n\n        # Normalise keys to upper for internal flag handling\n        normalised = [m.upper() for m in requested]\n        self._flags: Dict[str, bool] = {\n            \"BLEU\": \"BLEU\" in normalised,\n            \"ROUGE_L\": \"ROUGE_L\" in normalised,\n            \"BERTSCORE\": \"BERTSCORE\" in normalised or \"BERTSCORE\" in normalised,\n            \"PERPLEXITY\": \"PERPLEXITY\" in normalised,\n        }\n\n        # ---------------------------------------------------------------- #\n        # Lazy-loaded heavy resources\n        # ---------------------------------------------------------------- #\n        self._ppl_tokenizer = None  # type: ignore[assignment]\n        self._ppl_model = None  # type: ignore[assignment]\n\n        self._logger.info(\n            \"Evaluator initialised (device=%s).  Metrics enabled: %s\",\n            self.device.type,\n            \", \".join(k for k, v in self._flags.items() if v),\n        )\n\n    # ------------------------------------------------------------------ #\n    # Public API                                                         #\n    # ------------------------------------------------------------------ #\n    def score(self, preds: List[str], refs: List[str]) -> Dict[str, float]:\n        \"\"\"Compute the configured metrics for `preds` vs `refs`.\n\n        Parameters\n        ----------\n        preds:\n            List of model-generated answers.\n        refs:\n            List of reference answers (ground-truth).\n\n        Returns\n        -------\n        Dict[str, float]\n            Mapping metric-name \u2192 scalar value.  The order follows the\n            sequence provided in *config.yaml* (or the default order).\n        \"\"\"\n        if len(preds) != len(refs):\n            raise ValueError(\n                f\"Pred/ref length mismatch ({len(preds)} vs {len(refs)}).\"\n            )\n        if not preds:\n            raise ValueError(\"Input lists must contain at least one prediction.\")\n\n        results = collections.OrderedDict()\n\n        # Compute metrics in deterministic order\n        if self._flags[\"BLEU\"]:\n            results[\"BLEU\"] = self._compute_bleu(preds, refs)\n        if self._flags[\"ROUGE_L\"]:\n            results[\"ROUGE_L\"] = self._compute_rouge_l(preds, refs)\n        if self._flags[\"BERTSCORE\"]:\n            results[\"BERTScore\"] = self._compute_bertscore(preds, refs)\n        if self._flags[\"PERPLEXITY\"]:\n            results[\"Perplexity\"] = self._compute_perplexity(preds)\n\n        self._logger.info(\n            \"Evaluation complete: %s\",\n            \", \".join(f\"{k}={v:.4f}\" for k, v in results.items()),\n        )\n        return results\n\n    # ------------------------------------------------------------------ #\n    # Metric helpers                                                     #\n    # ------------------------------------------------------------------ #\n    @staticmethod\n    def _compute_bleu(preds: List[str], refs: List[str]) -> float:\n        \"\"\"Corpus BLEU (SacreBLEU, tokenise='13a').\"\"\"\n        from sacrebleu import corpus_bleu  # lazy import\n\n        bleu = corpus_bleu(preds, [refs], tokenize=\"13a\", lowercase=False)\n        return float(bleu.score)\n\n    @staticmethod\n    def _compute_rouge_l(preds: List[str], refs: List[str]) -> float:\n        \"\"\"Mean F1 of ROUGE-L using stemming (Porter).\"\"\"\n        from rouge_score import rouge_scorer  # lazy import\n\n        scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n        scores = [\n            scorer.score(ref, pred)[\"rougeL\"].fmeasure\n            for pred, ref in zip(preds, refs)\n        ]\n        return float(np.mean(scores))\n\n    def _compute_bertscore(self, preds: List[str], refs: List[str]) -> float:\n        \"\"\"Mean BERTScore F1 (roberta-large, english baseline).\"\"\"\n        from bert_score import score as bert_score  # lazy import\n\n        # The function internally handles batching & GPU\n        _, _, f1 = bert_score(\n            preds,\n            refs,\n            model_type=\"roberta-large\",\n            lang=\"en\",\n            rescale_with_baseline=True,\n            verbose=False,\n            device=str(self.device),\n        )\n        return float(f1.mean().item())\n\n    # ------------------------- Perplexity --------------------------------- #\n    def _lazy_load_ppl_model(self) -> None:\n        \"\"\"Load GPT-Neo-125M only once (lazy).\"\"\"\n        if self._ppl_model is not None:\n            return  # already loaded\n\n        from transformers import AutoModelForCausalLM, AutoTokenizer  # lazy import\n\n        model_name = \"EleutherAI/gpt-neo-125M\"\n        self._logger.info(\"Loading perplexity model '%s' on %s \u2026\", model_name, self.device)\n\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        model = AutoModelForCausalLM.from_pretrained(model_name)\n        model.to(self.device)\n        model.eval()\n\n        self._ppl_tokenizer = tokenizer\n        self._ppl_model = model\n\n    def _compute_perplexity(self, preds: List[str]) -> float:\n        \"\"\"Average perplexity of *preds* measured by GPT-Neo-125M.\"\"\"\n        self._lazy_load_ppl_model()\n        assert self._ppl_model is not None and self._ppl_tokenizer is not None  # mypy\n\n        batch_size = 8  # sensible default; small memory footprint\n        ppl_values: List[torch.Tensor] = []\n\n        for start in range(0, len(preds), batch_size):\n            batch = preds[start : start + batch_size]  # noqa: WPS362\n\n            # Replace empty predictions with single <eos> to avoid NaNs\n            batch = [txt if txt.strip() else self._ppl_tokenizer.eos_token for txt in batch]\n\n            enc = self._ppl_tokenizer(\n                batch,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n            ).to(self.device)\n\n            # Standard causal-LM perplexity: teacher-forcing with shifted labels\n            labels = enc[\"input_ids\"].clone()\n            labels[enc[\"attention_mask\"] == 0] = -100  # ignore padding in loss\n\n            with torch.inference_mode():\n                loss = self._ppl_model(**enc, labels=labels).loss  # mean-token NLL\n            ppl_values.append(torch.exp(loss).detach().cpu())\n\n        mean_ppl = torch.mean(torch.stack(ppl_values)).item()\n        return float(mean_ppl)\n\n    # ------------------------------------------------------------------ #\n    # Dunder helpers                                                     #\n    # ------------------------------------------------------------------ #\n    def __repr__(self) -> str:  # noqa: D401\n        enabled = \", \".join(k for k, v in self._flags.items() if v)\n        return f\"Evaluator(device='{self.device.type}', metrics=[{enabled}])\"\n\n```\n\n```python\n## File name: main.py\n## main.py\n\"\"\"Command-line entry point for reproducing SEQA experiments.\n\nThis script orchestrates *all* high-level operations:\n\n1.  Parse CLI arguments (config path, dataset key, output directory).\n2.  Load the immutable `Config` singleton from *config.yaml*.\n3.  Seed every PRNG for reproducibility (NumPy / random / PyTorch).\n4.  Instantiate the **SEQAPipeline** that wires together\n      Chunker \u2192 Encoder \u2192 BayesianFilter \u2192 PromptBuilder \u2192 Generator\n5.  Iterate over the chosen dataset, generate answers, stream-write\n    JSONL results and keep predictions / references in memory.\n6.  Compute evaluation metrics using `Evaluator` and persist them.\n7.  Exit with status code 0 on success.\n\nThe implementation follows the design and interfaces specified in the\nproject documentation; no additional public methods are introduced.\n\nExample\n-------\n$ python main.py \\\n      --config   config.yaml \\\n      --dataset  hotpotqa_distractor \\\n      --out_dir  results/\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport torch\nfrom tqdm.auto import tqdm\n\nfrom bayesian_filter import BayesianFilter\nfrom chunker import Chunker\nfrom config import Config\nfrom dataset_loader import DatasetLoader\nfrom encoder import Encoder\nfrom evaluation import Evaluator\nfrom generator import Generator\nfrom prompt_builder import PromptBuilder\nfrom utils import seed_everything, setup_logging\n\n# --------------------------------------------------------------------------- #\n# ------------------------------- CLI PARSING --------------------------------#\n# --------------------------------------------------------------------------- #\ndef _parse_cli() -> argparse.Namespace:\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"SEQA-Reproduction\",\n        description=\"Reproduce experiments from \"\n        \"'Statistical Estimation for Large-Scale Question Answering'.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        \"--config\",\n        type=str,\n        default=\"config.yaml\",\n        help=\"Path to YAML configuration file.\",\n    )\n    parser.add_argument(\n        \"--dataset\",\n        type=str,\n        choices=(\"hotpotqa_distractor\", \"triviaqa_rc\"),\n        required=True,\n        help=\"Dataset key as defined in DatasetLoader registry.\",\n    )\n    parser.add_argument(\n        \"--out_dir\",\n        type=str,\n        default=\"results\",\n        help=\"Directory where answers/metrics files are written.\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=None,\n        help=\"Force device e.g. 'cpu', 'cuda:0'. \"\n        \"If omitted the script auto-detects.\",\n    )\n    return parser.parse_args()\n\n\n# --------------------------------------------------------------------------- #\n# ----------------------------- SEQA PIPELINE --------------------------------#\n# --------------------------------------------------------------------------- #\nclass SEQAPipeline:  # noqa: WPS110  (domain-specific name)\n    \"\"\"End-to-end pipeline for a *single* QA example.\"\"\"\n\n    def __init__(self, cfg: Config) -> None:\n        self._cfg = cfg\n        self._logger = setup_logging(cfg, name=self.__class__.__name__)\n\n        # ------ Generator (owns tokenizer \u2013 must come FIRST) ------------- #\n        self._generator = Generator(cfg)\n        tokenizer = self._generator.tokenizer  # share across components\n\n        # ------ Remaining components ------------------------------------ #\n        self._encoder = Encoder(cfg)\n        self._chunker = Chunker(cfg, tokenizer=tokenizer)\n        self._filter = BayesianFilter(cfg)\n        self._prompt_builder = PromptBuilder(cfg, tokenizer=tokenizer)\n\n        # Retrieval is optional; not required for \u201cChunk-Only\u201d variant.\n        self._retrieval_enabled = bool(getattr(cfg, \"retrieval_enabled\", False))\n        if self._retrieval_enabled:\n            from retrieval import Retrieval  # local import to avoid cost if unused\n\n            self._retrieval = Retrieval(cfg, encoder=self._encoder)  # type: ignore\n            # NOTE: index_corpus() must be called by the user *before*\n            #       pipeline.run() if retrieval is enabled.  Not handled here.\n        else:\n            self._retrieval = None  # type: ignore\n\n    # ------------------------------------------------------------------ #\n    # Per-document execution\n    # ------------------------------------------------------------------ #\n    def run(self, sample: Dict[str, str]) -> Dict[str, str]:  # noqa: WPS231\n        \"\"\"Process **one** QA sample end-to-end and return result dict.\"\"\"\n        qid: str = sample[\"id\"]\n        question: str = sample[\"question\"]\n        context: str = sample[\"context\"]\n\n        # -------------------- Optional retrieval ------------------------ #\n        if self._retrieval_enabled and self._retrieval is not None:\n            passages = self._retrieval.retrieve(question)\n            context = \"\\n\\n\".join(passages) + \"\\n\\n\" + context\n\n        # -------------------- Chunk \u2192 Embed ---------------------------- #\n        chunks: List[str] = self._chunker.split(context)\n        embeds = self._encoder.embed(chunks)  # (K, dim)\n\n        # -------------------- Bayesian filtering ----------------------- #\n        accepted_chunks: List[str] = self._filter.filter(chunks, embeds)\n\n        # -------------------- Prompt construction ---------------------- #\n        prompt = self._prompt_builder.build(question, accepted_chunks)\n\n        # -------------------- Generation ------------------------------- #\n        answer = self._generator.answer(prompt)\n\n        # Package result\n        return {\"id\": qid, \"pred\": answer, \"ref\": sample[\"answer\"]}\n\n\n# --------------------------------------------------------------------------- #\n# ---------------------------------- main -----------------------------------#\n# --------------------------------------------------------------------------- #\ndef main() -> None:  # noqa: WPS231 (acceptable complexity for entrypoint)\n    \"\"\"Entry point executed when the module is run as a script.\"\"\"\n    args = _parse_cli()\n\n    # 1) Configuration ---------------------------------------------------- #\n    cfg = Config.load(args.config)\n\n    # 2) Early seeding for full reproducibility --------------------------- #\n    seed_everything(cfg.random_seed)\n\n    # 3) Logger (root) ---------------------------------------------------- #\n    logger = setup_logging(cfg, name=\"main\")\n    logger.info(\"Loaded configuration: %s\", cfg)\n\n    # 4) Device override (optional) -------------------------------------- #\n    if args.device is not None:\n        # Simple validation of user-provided device string\n        if args.device.startswith(\"cuda\") and not torch.cuda.is_available():\n            logger.error(\"CUDA not available but --device requests CUDA.\")\n            sys.exit(1)\n        torch_device = torch.device(args.device)\n        # monkey-patch torch.cuda.* calls by setting environment\n        if torch_device.type == \"cpu\":\n            torch.cuda.is_available = lambda: False  # type: ignore  # pragma: no cover\n        logger.info(\"Overriding device to %s via CLI.\", torch_device)\n\n    # 5) Dataset ---------------------------------------------------------- #\n    loader = DatasetLoader(cfg, dataset_key=args.dataset)\n    samples = loader.load()  # eager load; datasets are moderate in size\n    logger.info(\"Dataset '%s' loaded (%d samples).\", args.dataset, len(samples))\n\n    # 6) Output directories / files --------------------------------------- #\n    out_dir = Path(args.out_dir).expanduser()\n    out_dir.mkdir(parents=True, exist_ok=True)\n    answers_path = out_dir / f\"answers_{args.dataset}.jsonl\"\n    metrics_path = out_dir / f\"metrics_{args.dataset}.json\"\n    logger.info(\"Answers will be written to %s\", answers_path)\n    logger.info(\"Metrics will be written to %s\", metrics_path)\n\n    # 7) Build pipeline --------------------------------------------------- #\n    pipeline = SEQAPipeline(cfg)\n\n    # 8) Main loop -------------------------------------------------------- #\n    preds: List[str] = []\n    refs: List[str] = []\n\n    with answers_path.open(\"w\", encoding=\"utf-8\") as fp:\n        for sample in tqdm(samples, desc=\"Processing\", unit=\"sample\"):\n            result = pipeline.run(sample)\n            preds.append(result[\"pred\"])\n            refs.append(result[\"ref\"])\n\n            # Stream-write to JSONL\n            json.dump(\n                {\"id\": result[\"id\"], \"prediction\": result[\"pred\"]},\n                fp,\n                ensure_ascii=False,\n            )\n            fp.write(\"\\n\")\n            fp.flush()\n\n    # 9) Evaluate --------------------------------------------------------- #\n    evaluator = Evaluator(cfg)\n    metrics = evaluator.score(preds, refs)\n\n    with metrics_path.open(\"w\", encoding=\"utf-8\") as fp:\n        json.dump(metrics, fp, indent=2)\n    logger.info(\"Saved metrics to %s\", metrics_path)\n\n    logger.info(\"Finished successfully.\")\n    sys.exit(0)\n\n\n# --------------------------------------------------------------------------- #\n# Entry-point guard                                                           #\n# --------------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    main()\n\n```\n\n```yaml\n## File name: config.yaml\n## config.yaml\n# Only hyper-parameters explicitly reported in the paper are included.\n# The paper does NOT describe any model training or fine-tuning, so\n# learning-rate / epochs / batch-size are intentionally omitted.\n\nexperiment:\n  datasets:\n    - hotpotqa_distractor\n    - triviaqa_rc\n  random_seed: 42  # paper does not state; choose common default for reproducibility\n\nchunking:\n  chunk_size: 300        # (\u00a73.1, \u00a74.3)\n  overlap: 30            # (\u00a73.1, \u00a74.3)\n\nbayesian_filter:\n  mu0: 0.5               # (\u00a74.3 prior mean)\n  sigma0: 0.1            # (\u00a74.3 prior std)\n  sigma_obs: 0.05        # (\u00a74.3 observation noise variance)\n  beta: 1.0              # (\u00a73.2 Eq. 7, margin coefficient)\n\ngenerator:\n  model_name: facebook/bart-large   # (\u00a74.2)\n  num_beams: 4                      # (\u00a74.3 \u201cbeam size of 2 or 4\u201d; we adopt 4)\n  max_generation_length: 128        # (\u00a74.3)\n  encoder_max_tokens: 1024          # (\u00a74.3, upper limit used in some runs)\n\nretrieval:             # optional; defaults match \u201cSEQA (Chunk-Only)\u201d\n  enabled: false\n  top_k: 3                           # (\u00a74.2 \u201ccommonly k = 3\u201d)\n\nevaluation:\n  metrics:\n    - BLEU\n    - ROUGE_L\n    - BERTScore\n    - Perplexity\n```\n\n\n\n---\n\nPlease provide critique of the target repository and a single numerical rating (1, 2, 3, 4, or 5) based on the quality of the sample, following the Example JSON format, without any additional commentary, formatting, or chattiness."}], "reasoning_effort": "high", "n": 8}, "completion_json": {"id": "chatcmpl-BUrgMQzz0G578qphPa1iw6pj1UuvO", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "{\n  \"critique_list\": [\n    {\n      \"gold_file_name\": \"bayesian_filter.py\",\n      \"gold_func_name\": \"filter\",\n      \"target_file_name\": \"bayesian_filter.py\",\n      \"target_func_name\": \"filter\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The implementation of the Bayesian filtering precisely follows the paper\u2019s algorithm: computing novelty as one minus the cosine similarity, updating the posterior using a conjugate update, and selecting chunks based on an adaptive threshold. The addition of an ablation flag (_update_on_all) is a minor deviation providing extra flexibility without affecting core correctness.\"\n    },\n    {\n      \"gold_file_name\": \"chunker.py\",\n      \"gold_func_name\": \"split\",\n      \"target_file_name\": \"chunker.py\",\n      \"target_func_name\": \"split\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The Chunker correctly implements a fixed-size sliding-window with token\u2010space segmentation and proper overlap handling, matching the methodology described in the paper and standard in the gold repository.\"\n    },\n    {\n      \"gold_file_name\": \"prompt_builder.py\",\n      \"gold_func_name\": \"build\",\n      \"target_file_name\": \"prompt_builder.py\",\n      \"target_func_name\": \"build\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The prompt construction accurately combines the system role, question, accepted chunks, and ending instructions while enforcing the token budget. This aligns well with the paper\u2019s design and the gold implementation.\"\n    }\n  ],\n  \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 1, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"config.py\",\n            \"gold_func_name\": \"load\",\n            \"target_file_name\": \"config.py\",\n            \"target_func_name\": \"load\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The configuration module properly loads and validates all hyper-parameters from config.yaml, matching the expected design and ensuring consistency across components.\"\n        },\n        {\n            \"gold_file_name\": \"chunker.py\",\n            \"gold_func_name\": \"split\",\n            \"target_file_name\": \"chunker.py\",\n            \"target_func_name\": \"split\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The chunking implementation uses a fixed-size sliding window with overlap exactly as described in the paper, ensuring that each chunk stays within the token budget.\"\n        },\n        {\n            \"gold_file_name\": \"bayesian_filter.py\",\n            \"gold_func_name\": \"_update_posterior/_accept\",\n            \"target_file_name\": \"bayesian_filter.py\",\n            \"target_func_name\": \"filter\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The Bayesian filtering module accurately computes novelty scores via cosine similarity and adaptively updates the acceptance threshold, fully aligning with the paper's methodology.\"\n        },\n        {\n            \"gold_file_name\": \"prompt_builder.py\",\n            \"gold_func_name\": \"build\",\n            \"target_file_name\": \"prompt_builder.py\",\n            \"target_func_name\": \"build\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The prompt construction correctly combines system instructions, the user question, and the accepted context chunks while enforcing the encoder token limit, as per the design.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 2, "logprobs": null, "message": {"content": "{\n  \"critique_list\": [\n    {\n      \"gold_file_name\": \"config.py\",\n      \"gold_func_name\": \"Config.load\",\n      \"target_file_name\": \"config.py\",\n      \"target_func_name\": \"Config.load\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The target repository\u2019s configuration module faithfully loads and freezes the experiment hyperparameters from a YAML file, matching the design of the gold repository and paper. The structure and validation are implemented correctly with only minor stylistic differences.\"\n    },\n    {\n      \"gold_file_name\": \"chunker.py\",\n      \"gold_func_name\": \"split\",\n      \"target_file_name\": \"chunker.py\",\n      \"target_func_name\": \"Chunker.split\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The Chunker class implements fixed-size sliding-window segmentation in token space as described in Section 3.1. It correctly uses the generator\u2019s tokenizer to maintain consistency and adheres to the overlap and chunk size constraints, mirroring the gold repository.\"\n    },\n    {\n      \"gold_file_name\": \"bayesian_filter.py\",\n      \"gold_func_name\": \"filter\",\n      \"target_file_name\": \"bayesian_filter.py\",\n      \"target_func_name\": \"BayesianFilter.filter\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The Bayesian novelty filter implements online posterior updates and an adaptive thresholding mechanism exactly as specified in the paper. Its decision rule and state reset functionality align with the gold repository\u2019s methodology, with only minor differences in logging and variable naming.\"\n    }\n  ],\n  \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 3, "logprobs": null, "message": {"content": "{\n  \"critique_list\": [\n    {\n      \"gold_file_name\": \"bayesian_filter.py\",\n      \"gold_func_name\": \"filter\",\n      \"target_file_name\": \"bayesian_filter.py\",\n      \"target_func_name\": \"filter\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The BayesianFilter implementation accurately follows the paper's approach by computing the cosine\u2010based novelty score, performing an online conjugate update of the posterior, and applying an adaptive threshold. The inclusion of the _update_on_all flag adds flexibility without deviating from core methodology.\"\n    },\n    {\n      \"gold_file_name\": \"chunker.py\",\n      \"gold_func_name\": \"split\",\n      \"target_file_name\": \"chunker.py\",\n      \"target_func_name\": \"split\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The Chunker uses a fixed-size sliding window with token-based calculation and preserves the specified overlap, matching the paper\u2019s description and the gold repository design.\"\n    },\n    {\n      \"gold_file_name\": \"prompt_builder.py\",\n      \"gold_func_name\": \"build\",\n      \"target_file_name\": \"prompt_builder.py\",\n      \"target_func_name\": \"build\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The PromptBuilder correctly assembles static segments with accepted context, enforces token limit via deterministic truncation, and mirrors the scheme proposed in the paper.\"\n    },\n    {\n      \"gold_file_name\": \"main.py\",\n      \"gold_func_name\": \"main\",\n      \"target_file_name\": \"main.py\",\n      \"target_func_name\": \"run\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The overall pipeline in main.py coherently integrates dataset loading, retrieval (if enabled), chunking, encoding, Bayesian filtering, prompt building, and answer generation, fully reflecting the paper's end-to-end design.\"\n    }\n  ],\n  \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 4, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"config.yaml\",\n            \"gold_func_name\": \"N/A (configuration settings)\",\n            \"target_file_name\": \"config.py\",\n            \"target_func_name\": \"Config.load / _populate\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The configuration module faithfully loads and validates all hyper-parameters (including chunking, Bayesian filter, generator, and retrieval parameters) exactly as described in the paper. Its design mirrors the gold repository with robust error checking and immutability.\"\n        },\n        {\n            \"gold_file_name\": \"chunking_module\",\n            \"gold_func_name\": \"split\",\n            \"target_file_name\": \"chunker.py\",\n            \"target_func_name\": \"split\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The chunker implementation correctly implements fixed-size sliding window chunking with overlap in token space, ensuring that each chunk does not exceed the specified token limit, aligning perfectly with the paper's Section 3.1 and the gold repository's design.\"\n        },\n        {\n            \"gold_file_name\": \"bayesian_filter_module\",\n            \"gold_func_name\": \"filter\",\n            \"target_file_name\": \"bayesian_filter.py\",\n            \"target_func_name\": \"filter\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The Bayesian filtering mechanism is accurately implemented with proper online conjugate updates and adaptive thresholding based on novelty scores. The posterior update and decision rule follow the paper\u2019s methodology closely, matching the gold repository\u2019s approach.\"\n        },\n        {\n            \"gold_file_name\": \"prompt_builder_module\",\n            \"gold_func_name\": \"build\",\n            \"target_file_name\": \"prompt_builder.py\",\n            \"target_func_name\": \"build\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The prompt builder carefully assembles the prompt by integrating system role, question, and accepted context chunks while ensuring the token budget is not exceeded. This design is consistent with the paper and mirrors the gold repository.\"\n        },\n        {\n            \"gold_file_name\": \"generator_module\",\n            \"gold_func_name\": \"answer\",\n            \"target_file_name\": \"generator.py\",\n            \"target_func_name\": \"answer\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The generator encapsulates the sequence-to-sequence model (BART-Large) with appropriate beam search and token budget enforcement, adhering closely to the paper\u2019s instructions and the gold repository implementation.\"\n        },\n        {\n            \"gold_file_name\": \"dataset_loader_module\",\n            \"gold_func_name\": \"load\",\n            \"target_file_name\": \"dataset_loader.py\",\n            \"target_func_name\": \"load\",\n            \"severity_level\": \"low\",\n            \"critique\": \"Dataset loading and normalization for HotpotQA and TriviaQA are handled correctly with a uniform schema, ensuring that all required keys are present. This aspect is implemented with clarity and matches the gold repository\u2019s strategy.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 5, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"GoldConfig.py\",\n            \"gold_func_name\": \"load() / _populate()\",\n            \"target_file_name\": \"config.py\",\n            \"target_func_name\": \"Config.load()\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The configuration module correctly loads and validates hyper-parameters from config.yaml, matching the gold repository\u2019s approach and ensuring reproducibility across components.\"\n        },\n        {\n            \"gold_file_name\": \"GoldChunker.py\",\n            \"gold_func_name\": \"split()\",\n            \"target_file_name\": \"chunker.py\",\n            \"target_func_name\": \"Chunker.split()\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The chunking implementation uses a fixed-size sliding-window with an overlap that precisely mirrors the methodology described in the paper and expected in the gold repository.\"\n        },\n        {\n            \"gold_file_name\": \"GoldBayesianFilter.py\",\n            \"gold_func_name\": \"filter()\",\n            \"target_file_name\": \"bayesian_filter.py\",\n            \"target_func_name\": \"BayesianFilter.filter()\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The adaptive Bayesian filtering is implemented with proper posterior updates and acceptance decisions, faithfully reproducing the paper\u2019s algorithm and aligning with the gold repository.\"\n        },\n        {\n            \"gold_file_name\": \"GoldPromptBuilder.py\",\n            \"gold_func_name\": \"build()\",\n            \"target_file_name\": \"prompt_builder.py\",\n            \"target_func_name\": \"PromptBuilder.build()\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The prompt construction correctly combines role instructions, user questions, accepted chunks, and ensures token limits are maintained, fully consistent with the gold repository\u2019s strategy.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 6, "logprobs": null, "message": {"content": "{\n  \"critique_list\": [\n    {\n      \"gold_file_name\": \"bayesian_filter.py\",\n      \"gold_func_name\": \"_update_posterior\",\n      \"target_file_name\": \"bayesian_filter.py\",\n      \"target_func_name\": \"_update_posterior\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The BayesianFilter implementation accurately applies the online conjugate update for a normal prior with known variance and adaptive thresholding as outlined in the paper, mirroring the gold repository's approach without logical deviations.\"\n    },\n    {\n      \"gold_file_name\": \"chunker.py\",\n      \"gold_func_name\": \"split\",\n      \"target_file_name\": \"chunker.py\",\n      \"target_func_name\": \"split\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The chunking logic uses a fixed-size sliding window with token-based boundaries and preserves the specified overlap, faithfully replicating the method described in Section 3.1 of the paper.\"\n    },\n    {\n      \"gold_file_name\": \"prompt_builder.py\",\n      \"gold_func_name\": \"build\",\n      \"target_file_name\": \"prompt_builder.py\",\n      \"target_func_name\": \"build\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The prompt construction correctly combines the system role, user question, accepted context chunks, and instruction while enforcing the encoder token budget, which aligns well with the paper's specifications and the gold repository implementation.\"\n    },\n    {\n      \"gold_file_name\": \"main.py\",\n      \"gold_func_name\": \"main\",\n      \"target_file_name\": \"main.py\",\n      \"target_func_name\": \"main\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The overall pipeline in main.py and SEQAPipeline integrates all key components (data loading, chunking, embedding, Bayesian filtering, prompt building, and generation) in a clear, modular fashion that faithfully reproduces the paper\u2019s methodology.\"\n    }\n  ],\n  \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 7, "logprobs": null, "message": {"content": "{\n  \"critique_list\": [\n    {\n      \"gold_file_name\": \"config.py\",\n      \"gold_func_name\": \"load\",\n      \"target_file_name\": \"config.py\",\n      \"target_func_name\": \"load\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The configuration module accurately loads and validates all hyper\u2010parameters from the YAML file, mirroring the gold repository\u2019s approach. This ensures reproducibility and consistent experimental settings as described in the paper.\"\n    },\n    {\n      \"gold_file_name\": \"chunker.py\",\n      \"gold_func_name\": \"split\",\n      \"target_file_name\": \"chunker.py\",\n      \"target_func_name\": \"split\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The chunking implementation faithfully reproduces the fixed-size sliding-window segmentation with overlapping tokens (chunk_size and overlap), exactly as specified by the paper and in line with the gold repository design.\"\n    },\n    {\n      \"gold_file_name\": \"bayesian_filter.py\",\n      \"gold_func_name\": \"filter\",\n      \"target_file_name\": \"bayesian_filter.py\",\n      \"target_func_name\": \"filter\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The Bayesian novelty filter correctly implements the online conjugate update and adaptive thresholding mechanism. Its logic for computing cosine-based novelty and updating the posterior is consistent with the paper and matches the gold repository\u2019s methodology.\"\n    },\n    {\n      \"gold_file_name\": \"prompt_builder.py\",\n      \"gold_func_name\": \"build\",\n      \"target_file_name\": \"prompt_builder.py\",\n      \"target_func_name\": \"build\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The prompt builder assembles a complete prompt\u2014including system role, question, context chunks, and instructions\u2014while enforcing the encoder token budget through deterministic truncation. This design closely mirrors the gold repository implementation.\"\n    },\n    {\n      \"gold_file_name\": \"main.py\",\n      \"gold_func_name\": \"run\",\n      \"target_file_name\": \"main.py\",\n      \"target_func_name\": \"run\",\n      \"severity_level\": \"low\",\n      \"critique\": \"The end-to-end pipeline integrates dataset loading, chunking, encoding, Bayesian filtering, prompt construction, and answer generation in a modular and reproducible manner, fully aligning with the methodology described in the paper and reflected in the gold repository.\"\n    }\n  ],\n  \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746695894, "model": "o3-mini-2025-01-31", "object": "chat.completion", "service_tier": "default", "system_fingerprint": "fp_8b28473bdb", "usage": {"completion_tokens": 24139, "prompt_tokens": 32246, "total_tokens": 56385, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 20672, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}}}, "eval_result": {"score": 5.0, "valid_n": 8, "scroe_lst": [5, 5, 5, 5, 5, 5, 5, 5], "rationale_lst": ["[{\"gold_file_name\": \"bayesian_filter.py\", \"gold_func_name\": \"filter\", \"target_file_name\": \"bayesian_filter.py\", \"target_func_name\": \"filter\", \"severity_level\": \"low\", \"critique\": \"The implementation of the Bayesian filtering precisely follows the paper\\u2019s algorithm: computing novelty as one minus the cosine similarity, updating the posterior using a conjugate update, and selecting chunks based on an adaptive threshold. The addition of an ablation flag (_update_on_all) is a minor deviation providing extra flexibility without affecting core correctness.\"}, {\"gold_file_name\": \"chunker.py\", \"gold_func_name\": \"split\", \"target_file_name\": \"chunker.py\", \"target_func_name\": \"split\", \"severity_level\": \"low\", \"critique\": \"The Chunker correctly implements a fixed-size sliding-window with token\\u2010space segmentation and proper overlap handling, matching the methodology described in the paper and standard in the gold repository.\"}, {\"gold_file_name\": \"prompt_builder.py\", \"gold_func_name\": \"build\", \"target_file_name\": \"prompt_builder.py\", \"target_func_name\": \"build\", \"severity_level\": \"low\", \"critique\": \"The prompt construction accurately combines the system role, question, accepted chunks, and ending instructions while enforcing the token budget. This aligns well with the paper\\u2019s design and the gold implementation.\"}]", "[{\"gold_file_name\": \"config.py\", \"gold_func_name\": \"load\", \"target_file_name\": \"config.py\", \"target_func_name\": \"load\", \"severity_level\": \"low\", \"critique\": \"The configuration module properly loads and validates all hyper-parameters from config.yaml, matching the expected design and ensuring consistency across components.\"}, {\"gold_file_name\": \"chunker.py\", \"gold_func_name\": \"split\", \"target_file_name\": \"chunker.py\", \"target_func_name\": \"split\", \"severity_level\": \"low\", \"critique\": \"The chunking implementation uses a fixed-size sliding window with overlap exactly as described in the paper, ensuring that each chunk stays within the token budget.\"}, {\"gold_file_name\": \"bayesian_filter.py\", \"gold_func_name\": \"_update_posterior/_accept\", \"target_file_name\": \"bayesian_filter.py\", \"target_func_name\": \"filter\", \"severity_level\": \"low\", \"critique\": \"The Bayesian filtering module accurately computes novelty scores via cosine similarity and adaptively updates the acceptance threshold, fully aligning with the paper's methodology.\"}, {\"gold_file_name\": \"prompt_builder.py\", \"gold_func_name\": \"build\", \"target_file_name\": \"prompt_builder.py\", \"target_func_name\": \"build\", \"severity_level\": \"low\", \"critique\": \"The prompt construction correctly combines system instructions, the user question, and the accepted context chunks while enforcing the encoder token limit, as per the design.\"}]", "[{\"gold_file_name\": \"config.py\", \"gold_func_name\": \"Config.load\", \"target_file_name\": \"config.py\", \"target_func_name\": \"Config.load\", \"severity_level\": \"low\", \"critique\": \"The target repository\\u2019s configuration module faithfully loads and freezes the experiment hyperparameters from a YAML file, matching the design of the gold repository and paper. The structure and validation are implemented correctly with only minor stylistic differences.\"}, {\"gold_file_name\": \"chunker.py\", \"gold_func_name\": \"split\", \"target_file_name\": \"chunker.py\", \"target_func_name\": \"Chunker.split\", \"severity_level\": \"low\", \"critique\": \"The Chunker class implements fixed-size sliding-window segmentation in token space as described in Section 3.1. It correctly uses the generator\\u2019s tokenizer to maintain consistency and adheres to the overlap and chunk size constraints, mirroring the gold repository.\"}, {\"gold_file_name\": \"bayesian_filter.py\", \"gold_func_name\": \"filter\", \"target_file_name\": \"bayesian_filter.py\", \"target_func_name\": \"BayesianFilter.filter\", \"severity_level\": \"low\", \"critique\": \"The Bayesian novelty filter implements online posterior updates and an adaptive thresholding mechanism exactly as specified in the paper. Its decision rule and state reset functionality align with the gold repository\\u2019s methodology, with only minor differences in logging and variable naming.\"}]", "[{\"gold_file_name\": \"bayesian_filter.py\", \"gold_func_name\": \"filter\", \"target_file_name\": \"bayesian_filter.py\", \"target_func_name\": \"filter\", \"severity_level\": \"low\", \"critique\": \"The BayesianFilter implementation accurately follows the paper's approach by computing the cosine\\u2010based novelty score, performing an online conjugate update of the posterior, and applying an adaptive threshold. The inclusion of the _update_on_all flag adds flexibility without deviating from core methodology.\"}, {\"gold_file_name\": \"chunker.py\", \"gold_func_name\": \"split\", \"target_file_name\": \"chunker.py\", \"target_func_name\": \"split\", \"severity_level\": \"low\", \"critique\": \"The Chunker uses a fixed-size sliding window with token-based calculation and preserves the specified overlap, matching the paper\\u2019s description and the gold repository design.\"}, {\"gold_file_name\": \"prompt_builder.py\", \"gold_func_name\": \"build\", \"target_file_name\": \"prompt_builder.py\", \"target_func_name\": \"build\", \"severity_level\": \"low\", \"critique\": \"The PromptBuilder correctly assembles static segments with accepted context, enforces token limit via deterministic truncation, and mirrors the scheme proposed in the paper.\"}, {\"gold_file_name\": \"main.py\", \"gold_func_name\": \"main\", \"target_file_name\": \"main.py\", \"target_func_name\": \"run\", \"severity_level\": \"low\", \"critique\": \"The overall pipeline in main.py coherently integrates dataset loading, retrieval (if enabled), chunking, encoding, Bayesian filtering, prompt building, and answer generation, fully reflecting the paper's end-to-end design.\"}]", "[{\"gold_file_name\": \"config.yaml\", \"gold_func_name\": \"N/A (configuration settings)\", \"target_file_name\": \"config.py\", \"target_func_name\": \"Config.load / _populate\", \"severity_level\": \"low\", \"critique\": \"The configuration module faithfully loads and validates all hyper-parameters (including chunking, Bayesian filter, generator, and retrieval parameters) exactly as described in the paper. Its design mirrors the gold repository with robust error checking and immutability.\"}, {\"gold_file_name\": \"chunking_module\", \"gold_func_name\": \"split\", \"target_file_name\": \"chunker.py\", \"target_func_name\": \"split\", \"severity_level\": \"low\", \"critique\": \"The chunker implementation correctly implements fixed-size sliding window chunking with overlap in token space, ensuring that each chunk does not exceed the specified token limit, aligning perfectly with the paper's Section 3.1 and the gold repository's design.\"}, {\"gold_file_name\": \"bayesian_filter_module\", \"gold_func_name\": \"filter\", \"target_file_name\": \"bayesian_filter.py\", \"target_func_name\": \"filter\", \"severity_level\": \"low\", \"critique\": \"The Bayesian filtering mechanism is accurately implemented with proper online conjugate updates and adaptive thresholding based on novelty scores. The posterior update and decision rule follow the paper\\u2019s methodology closely, matching the gold repository\\u2019s approach.\"}, {\"gold_file_name\": \"prompt_builder_module\", \"gold_func_name\": \"build\", \"target_file_name\": \"prompt_builder.py\", \"target_func_name\": \"build\", \"severity_level\": \"low\", \"critique\": \"The prompt builder carefully assembles the prompt by integrating system role, question, and accepted context chunks while ensuring the token budget is not exceeded. This design is consistent with the paper and mirrors the gold repository.\"}, {\"gold_file_name\": \"generator_module\", \"gold_func_name\": \"answer\", \"target_file_name\": \"generator.py\", \"target_func_name\": \"answer\", \"severity_level\": \"low\", \"critique\": \"The generator encapsulates the sequence-to-sequence model (BART-Large) with appropriate beam search and token budget enforcement, adhering closely to the paper\\u2019s instructions and the gold repository implementation.\"}, {\"gold_file_name\": \"dataset_loader_module\", \"gold_func_name\": \"load\", \"target_file_name\": \"dataset_loader.py\", \"target_func_name\": \"load\", \"severity_level\": \"low\", \"critique\": \"Dataset loading and normalization for HotpotQA and TriviaQA are handled correctly with a uniform schema, ensuring that all required keys are present. This aspect is implemented with clarity and matches the gold repository\\u2019s strategy.\"}]", "[{\"gold_file_name\": \"GoldConfig.py\", \"gold_func_name\": \"load() / _populate()\", \"target_file_name\": \"config.py\", \"target_func_name\": \"Config.load()\", \"severity_level\": \"low\", \"critique\": \"The configuration module correctly loads and validates hyper-parameters from config.yaml, matching the gold repository\\u2019s approach and ensuring reproducibility across components.\"}, {\"gold_file_name\": \"GoldChunker.py\", \"gold_func_name\": \"split()\", \"target_file_name\": \"chunker.py\", \"target_func_name\": \"Chunker.split()\", \"severity_level\": \"low\", \"critique\": \"The chunking implementation uses a fixed-size sliding-window with an overlap that precisely mirrors the methodology described in the paper and expected in the gold repository.\"}, {\"gold_file_name\": \"GoldBayesianFilter.py\", \"gold_func_name\": \"filter()\", \"target_file_name\": \"bayesian_filter.py\", \"target_func_name\": \"BayesianFilter.filter()\", \"severity_level\": \"low\", \"critique\": \"The adaptive Bayesian filtering is implemented with proper posterior updates and acceptance decisions, faithfully reproducing the paper\\u2019s algorithm and aligning with the gold repository.\"}, {\"gold_file_name\": \"GoldPromptBuilder.py\", \"gold_func_name\": \"build()\", \"target_file_name\": \"prompt_builder.py\", \"target_func_name\": \"PromptBuilder.build()\", \"severity_level\": \"low\", \"critique\": \"The prompt construction correctly combines role instructions, user questions, accepted chunks, and ensures token limits are maintained, fully consistent with the gold repository\\u2019s strategy.\"}]", "[{\"gold_file_name\": \"bayesian_filter.py\", \"gold_func_name\": \"_update_posterior\", \"target_file_name\": \"bayesian_filter.py\", \"target_func_name\": \"_update_posterior\", \"severity_level\": \"low\", \"critique\": \"The BayesianFilter implementation accurately applies the online conjugate update for a normal prior with known variance and adaptive thresholding as outlined in the paper, mirroring the gold repository's approach without logical deviations.\"}, {\"gold_file_name\": \"chunker.py\", \"gold_func_name\": \"split\", \"target_file_name\": \"chunker.py\", \"target_func_name\": \"split\", \"severity_level\": \"low\", \"critique\": \"The chunking logic uses a fixed-size sliding window with token-based boundaries and preserves the specified overlap, faithfully replicating the method described in Section 3.1 of the paper.\"}, {\"gold_file_name\": \"prompt_builder.py\", \"gold_func_name\": \"build\", \"target_file_name\": \"prompt_builder.py\", \"target_func_name\": \"build\", \"severity_level\": \"low\", \"critique\": \"The prompt construction correctly combines the system role, user question, accepted context chunks, and instruction while enforcing the encoder token budget, which aligns well with the paper's specifications and the gold repository implementation.\"}, {\"gold_file_name\": \"main.py\", \"gold_func_name\": \"main\", \"target_file_name\": \"main.py\", \"target_func_name\": \"main\", \"severity_level\": \"low\", \"critique\": \"The overall pipeline in main.py and SEQAPipeline integrates all key components (data loading, chunking, embedding, Bayesian filtering, prompt building, and generation) in a clear, modular fashion that faithfully reproduces the paper\\u2019s methodology.\"}]", "[{\"gold_file_name\": \"config.py\", \"gold_func_name\": \"load\", \"target_file_name\": \"config.py\", \"target_func_name\": \"load\", \"severity_level\": \"low\", \"critique\": \"The configuration module accurately loads and validates all hyper\\u2010parameters from the YAML file, mirroring the gold repository\\u2019s approach. This ensures reproducibility and consistent experimental settings as described in the paper.\"}, {\"gold_file_name\": \"chunker.py\", \"gold_func_name\": \"split\", \"target_file_name\": \"chunker.py\", \"target_func_name\": \"split\", \"severity_level\": \"low\", \"critique\": \"The chunking implementation faithfully reproduces the fixed-size sliding-window segmentation with overlapping tokens (chunk_size and overlap), exactly as specified by the paper and in line with the gold repository design.\"}, {\"gold_file_name\": \"bayesian_filter.py\", \"gold_func_name\": \"filter\", \"target_file_name\": \"bayesian_filter.py\", \"target_func_name\": \"filter\", \"severity_level\": \"low\", \"critique\": \"The Bayesian novelty filter correctly implements the online conjugate update and adaptive thresholding mechanism. Its logic for computing cosine-based novelty and updating the posterior is consistent with the paper and matches the gold repository\\u2019s methodology.\"}, {\"gold_file_name\": \"prompt_builder.py\", \"gold_func_name\": \"build\", \"target_file_name\": \"prompt_builder.py\", \"target_func_name\": \"build\", \"severity_level\": \"low\", \"critique\": \"The prompt builder assembles a complete prompt\\u2014including system role, question, context chunks, and instructions\\u2014while enforcing the encoder token budget through deterministic truncation. This design closely mirrors the gold repository implementation.\"}, {\"gold_file_name\": \"main.py\", \"gold_func_name\": \"run\", \"target_file_name\": \"main.py\", \"target_func_name\": \"run\", \"severity_level\": \"low\", \"critique\": \"The end-to-end pipeline integrates dataset loading, chunking, encoding, Bayesian filtering, prompt construction, and answer generation in a modular and reproducible manner, fully aligning with the methodology described in the paper and reflected in the gold repository.\"}]"]}}