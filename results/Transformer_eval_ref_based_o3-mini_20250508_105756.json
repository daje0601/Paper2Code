{"paper_name": "Transformer", "target_repo_dir": "../outputs/Transformer_repo", "eval_type": "ref_based", "gold_repo_dir": "/Users/ggls03/Documents/transformers/torch", "generated_n": 8, "request_json": {"model": "o3-mini", "messages": [{"role": "system", "content": "You will be given a research paper along with two corresponding code repositories: a gold repository and a target repository.\n\nYour task is to compare the target repository against the gold repository, rate the target repository on one metric, and provide a critique highlighting key differences.\n\nPlease make sure you read and understand these instructions carefully. Keep this document open while reviewing, and refer to it as needed.\n\n---\n\nEvaluation Criteria:\n\nCorrectness (1-5): The quality of the target repository in accurately implementing the paper\u2019s concepts, methodology, and algorithms without logical errors, as compared to the gold repository. Additionally, provide a critique focusing on the completeness, accuracy, and implementation choices made in the target repository relative to the gold repository.\n\n1: Very Poor. The target repository does not correctly implement the core concepts, methodology, or algorithms from the paper. Major logical errors or missing components are present, especially when compared to the gold repository.\n2: Poor. The target repository attempts to implement the paper\u2019s concepts but contains significant mistakes or missing components, making the implementation incorrect when compared to the gold repository.\n3: Fair. Some core components and concepts are correctly implemented in the target repository, but there are notable logical errors or inaccuracies compared to the gold repository.\n4: Good. The target repository correctly implements the key components and methodology, with only minor inaccuracies or deviations from the gold repository.\n5: Excellent. The target repository fully and accurately implements all relevant key components, methodology, and algorithms from the paper, matching the quality of the gold repository.\n\n---\n\nEvaluation Steps  \n\n1. Identify Key Aspects of the Paper: Carefully read the research paper to understand its core concepts, methodology, and algorithms. Pay close attention to the key aspects that are crucial for implementing the paper\u2019s results (e.g., specific algorithms, data preprocessing steps, evaluation protocols).\n\n2. Analyze the Gold Repository: Examine the gold repository to understand how these key aspects have been implemented. Use the gold repository as a reference for how the paper\u2019s methodology should be translated into code. Note the completeness, accuracy, and design choices in the gold repository that faithfully represent the paper\u2019s concepts.\n\n3. Examine the Target Repository: Analyze the target repository to assess how well it implements the key aspects of the paper. Reference the gold repository as a guide for understanding these key aspects in the target repository. Focus on whether the target repository\u2019s core logic, algorithms, and structure align with the methodology and experiments described in the paper.\n\n4. Identify Logical Errors and Deviations: Check for logical errors, missing steps, or deviations from the paper\u2019s methodology. Note any incorrect representations, inconsistencies, or incomplete implementations that could affect the correctness of the target repository.\n\n5. Provide a Critique: Consider both the completeness and accuracy of the implementation relative to the paper\u2019s goals and the gold repository\u2019s standard. You do not need to analyze minor details like logging functions, script organization, or documentation quality. Instead, concentrate on the correctness of the logic and implementation that ensures the core concepts from the paper are fully reflected in the target repository. For each mismatch or deviation in implementation, note down specific critiques comparing relevant functions in the target repository to the corresponding functions in the gold repository. Highlight incorrect logic, missing steps, or deviations that affect the correct implementation of the paper\u2019s methodology.\n\n5. Assess the Correctness: Determine whether the target repository includes all the critical elements described in the paper and implemented in the gold repository. Identify missing components, significant deviations, or incorrect implementations that could affect the correctness of the target repository.\n\n6. Assign a Score: Based on your evaluation, provide a critique and assign a correctness score from 1 to 5 for the target repository, reflecting how well it implements the key aspects of the paper refer to the gold repository. Include a detailed critique in the specified JSON format.\n\n\n---\n\nSeverity Level:  \n\nEach identified critique will be assigned a severity level based on its impact on the correctness of the methodology implementation.  \n\n- High: Missing or incorrect implementation of the paper\u2019s core concepts, major loss functions, or experiment components that are fundamental to reproducing the paper\u2019s methodology.  \n  - Example: The main algorithm is missing or fundamentally incorrect.  \n- Medium: Issues affecting training logic, data preprocessing, or other core functionalities that significantly impact performance but do not completely break the system.  \n  - Example: Improper training loop structure, incorrect data augmentation, or missing essential components in data processing.  \n- Low: Errors in specific features that cause deviations from expected results but can be worked around with modifications. Any errors in the evaluation process belong to this category unless they impact the core concepts. These include minor issues like logging, error handling mechanisms, configuration settings, evaluation steps that do not alter the fundamental implementation and additional implementations not explicitly stated in the paper.\n  - Example: Suboptimal hyperparameter initialization, incorrect learning rate schedule, inaccuracies in evaluation metrics, using a different random seed, variations in batch processing, different weight initialization, issues in result logging or reporting, variations in evaluation dataset splits, improper error handling in non-critical steps, mismatches in secondary evaluation criteria, or additional implementation details not specified in the paper that do not interfere with core results.\n\n---\n\nExample JSON format:\n```json\n{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"preprocessing.py\",\n            \"gold_func_name\": \"data_process\",\n            \"target_file_name\": \"dataset.py\",\n            \"target_func_name\": \"train_preprocess\",\n            \"severity_level\": \"medium\",\n            \"critique\": \"A critique of the target repository's file with reference to the gold repository.\"\n        }, \n        {\n            \"gold_file_name\": \"utils.py\",\n            \"gold_func_name\": \"calculate_metric\",\n            \"target_file_name\": \"metric.py\",\n            \"target_func_name\": \"f1_at_k\"\n            \"severity_level\": \"low\",\n            \"critique\": \"A critique of the target repository's file with reference to the gold repository.\"\n        },\n    ],\n    \"score\": 2\n}\n```\n\n---\n\nSample:\n\nResearch Paper:\n\n{'paper_id': 'Transformer', 'title': 'Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. * Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\\n\u2020 Work performed while at Google Brain.\\n\u2021 Work performed while at Google Research.', 'pdf_parse': {'paper_id': 'Transformer', 'abstract': [{'text': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. * Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.', 'section': 'Abstract', 'sec_num': None}, {'text': '\u2020 Work performed while at Google Brain.', 'section': 'Abstract', 'sec_num': None}, {'text': '\u2021 Work performed while at Google Research.', 'section': 'Abstract', 'sec_num': None}], 'body_text': [{'text': 'Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5] . Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15] .', 'section': 'Introduction', 'sec_num': '1'}, {'text': 'Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h t-1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32] , while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.', 'section': 'Introduction', 'sec_num': '1'}, {'text': 'Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19] . In all but a few cases [27] , however, such attention mechanisms are used in conjunction with a recurrent network.', 'section': 'Introduction', 'sec_num': '1'}, {'text': 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.', 'section': 'Introduction', 'sec_num': '1'}, {'text': 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16] , ByteNet [18] and ConvS2S [9] , all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12] . In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.', 'section': 'Background', 'sec_num': '2'}, {'text': 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22] .', 'section': 'Background', 'sec_num': '2'}, {'text': 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34] .', 'section': 'Background', 'sec_num': '2'}, {'text': 'To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9] .', 'section': 'Background', 'sec_num': '2'}, {'text': 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35] . Here, the encoder maps an input sequence of symbol representations (x 1 , ..., x n ) to a sequence of continuous representations z = (z 1 , ..., z n ). Given z, the decoder then generates an output sequence (y 1 , ..., y m ) of symbols one element at a time. At each step the model is auto-regressive [10] , consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1 , respectively.', 'section': 'Model Architecture', 'sec_num': '3'}, {'text': 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1] . That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512.', 'section': 'Encoder and Decoder Stacks', 'sec_num': '3.1'}, {'text': 'Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.', 'section': 'Encoder and Decoder Stacks', 'sec_num': '3.1'}, {'text': 'An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum Scaled Dot-Product Attention Multi-Head Attention of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.', 'section': 'Attention', 'sec_num': '3.2'}, {'text': 'We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2 ). The input consists of queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the query with all keys, divide each by \u221a d k , and apply a softmax function to obtain the weights on the values.', 'section': 'Scaled Dot-Product Attention', 'sec_num': '3.2.1'}, {'text': 'In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:', 'section': 'Scaled Dot-Product Attention', 'sec_num': '3.2.1'}, {'text': 'EQUATION', 'section': 'Scaled Dot-Product Attention', 'sec_num': '3.2.1'}, {'text': 'The two most commonly used attention functions are additive attention [2] , and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1', 'section': 'Scaled Dot-Product Attention', 'sec_num': '3.2.1'}, {'text': '\u221a d k', 'section': 'Scaled Dot-Product Attention', 'sec_num': '3.2.1'}, {'text': '. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.', 'section': 'Scaled Dot-Product Attention', 'sec_num': '3.2.1'}, {'text': 'While for small values of d k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d k [3] . We suspect that for large values of d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4 . To counteract this effect, we scale the dot products by 1', 'section': 'Scaled Dot-Product Attention', 'sec_num': '3.2.1'}, {'text': '\u221a d k .', 'section': 'Scaled Dot-Product Attention', 'sec_num': '3.2.1'}, {'text': 'Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2 .', 'section': 'Multi-Head Attention', 'sec_num': '3.2.2'}, {'text': 'Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.', 'section': 'Multi-Head Attention', 'sec_num': '3.2.2'}, {'text': 'MultiHead(Q, K, V ) = Concat(head 1 , ..., head h )W O', 'section': 'Multi-Head Attention', 'sec_num': '3.2.2'}, {'text': 'where', 'section': 'Multi-Head Attention', 'sec_num': '3.2.2'}, {'text': 'head i = Attention(QW Q i , KW K i , V W V i )', 'section': 'Multi-Head Attention', 'sec_num': '3.2.2'}, {'text': 'Where the projections are parameter matrices', 'section': 'Multi-Head Attention', 'sec_num': '3.2.2'}, {'text': 'W Q i \u2208 R dmodel\u00d7d k , W K i \u2208 R dmodel\u00d7d k , W V i \u2208 R dmodel\u00d7dv and W O \u2208 R hdv\u00d7dmodel .', 'section': 'Multi-Head Attention', 'sec_num': '3.2.2'}, {'text': 'In this work we employ h = 8 parallel attention layers, or heads. For each of these we use', 'section': 'Multi-Head Attention', 'sec_num': '3.2.2'}, {'text': 'd k = d v = d model /h = 64.', 'section': 'Multi-Head Attention', 'sec_num': '3.2.2'}, {'text': 'Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.', 'section': 'Multi-Head Attention', 'sec_num': '3.2.2'}, {'text': 'The Transformer uses multi-head attention in three different ways:', 'section': 'Applications of Attention in our Model', 'sec_num': '3.2.3'}, {'text': '\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9] .', 'section': 'Applications of Attention in our Model', 'sec_num': '3.2.3'}, {'text': '\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.', 'section': 'Applications of Attention in our Model', 'sec_num': '3.2.3'}, {'text': '\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to -\u221e) all values in the input of the softmax which correspond to illegal connections. See Figure 2 .', 'section': 'Applications of Attention in our Model', 'sec_num': '3.2.3'}, {'text': 'In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.', 'section': 'Position-wise Feed-Forward Networks', 'sec_num': '3.3'}, {'text': 'EQUATION', 'section': 'Position-wise Feed-Forward Networks', 'sec_num': '3.3'}, {'text': 'While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.', 'section': 'Position-wise Feed-Forward Networks', 'sec_num': '3.3'}, {'text': 'The dimensionality of input and output is d model = 512, and the inner-layer has dimensionality d f f = 2048.', 'section': 'Position-wise Feed-Forward Networks', 'sec_num': '3.3'}, {'text': 'Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model . We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30] . In the embedding layers, we multiply those weights by \u221a d model . ', 'section': 'Embeddings and Softmax', 'sec_num': '3.4'}, {'text': '(n 2 \u2022 d) O(1) O(1) Recurrent O(n \u2022 d 2 ) O(n) O(n) Convolutional O(k \u2022 n \u2022 d 2 ) O(1) O(log k (n)) Self-Attention (restricted) O(r \u2022 n \u2022 d) O(1) O(n/r)', 'section': 'Embeddings and Softmax', 'sec_num': '3.4'}, {'text': 'Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9] .', 'section': 'Positional Encoding', 'sec_num': '3.5'}, {'text': 'In this work, we use sine and cosine functions of different frequencies:', 'section': 'Positional Encoding', 'sec_num': '3.5'}, {'text': 'P E (pos,2i) = sin(pos/10000 2i/dmodel ) P E (pos,2i+1) = cos(pos/10000 2i/dmodel )', 'section': 'Positional Encoding', 'sec_num': '3.5'}, {'text': 'where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u2022 2\u03c0. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P E pos+k can be represented as a linear function of P E pos .', 'section': 'Positional Encoding', 'sec_num': '3.5'}, {'text': 'We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.', 'section': 'Positional Encoding', 'sec_num': '3.5'}, {'text': 'In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1 , ..., x n ) to another sequence of equal length (z 1 , ..., z n ), with x i , z i \u2208 R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.', 'section': 'Why Self-Attention', 'sec_num': '4'}, {'text': 'One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.', 'section': 'Why Self-Attention', 'sec_num': '4'}, {'text': 'The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12] . Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.', 'section': 'Why Self-Attention', 'sec_num': '4'}, {'text': 'As noted in Table 1 , a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.', 'section': 'Why Self-Attention', 'sec_num': '4'}, {'text': 'A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log k (n)) in the case of dilated convolutions [18] , increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6] , however, decrease the complexity considerably, to O(k', 'section': 'Why Self-Attention', 'sec_num': '4'}, {'text': '\u2022 n \u2022 d + n \u2022 d 2 ).', 'section': 'Why Self-Attention', 'sec_num': '4'}, {'text': 'Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.', 'section': 'Why Self-Attention', 'sec_num': '4'}, {'text': 'As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.', 'section': 'Why Self-Attention', 'sec_num': '4'}, {'text': 'This section describes the training regime for our models.', 'section': 'Training', 'sec_num': '5'}, {'text': 'We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3] , which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38] . Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.', 'section': 'Training Data and Batching', 'sec_num': '5.1'}, {'text': 'We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3 ), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).', 'section': 'Hardware and Schedule', 'sec_num': '5.2'}, {'text': 'We used the Adam optimizer [20] with \u03b2 1 = 0.9, \u03b2 2 = 0.98 and \u03f5 = 10 -9 . We varied the learning rate over the course of training, according to the formula:', 'section': 'Optimizer', 'sec_num': '5.3'}, {'text': 'EQUATION', 'section': 'Optimizer', 'sec_num': '5.3'}, {'text': 'This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.', 'section': 'Optimizer', 'sec_num': '5.3'}, {'text': 'We employ three types of regularization during training: Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.', 'section': 'Regularization', 'sec_num': '5.4'}, {'text': 'During training, we employed label smoothing of value \u03f5 ls = 0.1 [36] . This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.', 'section': 'Label Smoothing', 'sec_num': None}, {'text': '6 Results', 'section': 'Label Smoothing', 'sec_num': None}, {'text': 'On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2 ) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3 . Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.', 'section': 'Machine Translation', 'sec_num': '6.1'}, {'text': 'On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0.1, instead of 0.3.', 'section': 'Machine Translation', 'sec_num': '6.1'}, {'text': 'For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38] . These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38] .', 'section': 'Machine Translation', 'sec_num': '6.1'}, {'text': 'Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU5 .', 'section': 'Machine Translation', 'sec_num': '6.1'}, {'text': 'To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the . We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3 .', 'section': 'Model Variations', 'sec_num': '6.2'}, {'text': 'In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.', 'section': 'Model Variations', 'sec_num': '6.2'}, {'text': 'In Table 3 rows (B), we observe that reducing the attention key size d k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9] , and observe nearly identical results to the base model.', 'section': 'Model Variations', 'sec_num': '6.2'}, {'text': 'To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37] .', 'section': 'English Constituency Parsing', 'sec_num': '6.3'}, {'text': 'We trained a 4-layer transformer with d model = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25] , about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37] . We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.', 'section': 'English Constituency Parsing', 'sec_num': '6.3'}, {'text': 'We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3 for both WSJ only and the semi-supervised setting.', 'section': 'English Constituency Parsing', 'sec_num': '6.3'}, {'text': 'Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8] .', 'section': 'English Constituency Parsing', 'sec_num': '6.3'}, {'text': 'In contrast to RNN sequence-to-sequence models [37] , the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.', 'section': 'English Constituency Parsing', 'sec_num': '6.3'}, {'text': 'In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.', 'section': 'Conclusion', 'sec_num': '7'}, {'text': 'For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.', 'section': 'Conclusion', 'sec_num': '7'}, {'text': 'We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.', 'section': 'Conclusion', 'sec_num': '7'}, {'text': 'The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor. ', 'section': 'Conclusion', 'sec_num': '7'}, {'text': \"The Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\", 'section': 'Input-Input Layer5', 'sec_num': None}, {'text': 'The ', 'section': 'Input-Input Layer5', 'sec_num': None}, {'text': 'To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q \u2022 k = d k i=1 qiki, has mean 0 and variance d k .', 'section': '', 'sec_num': None}, {'text': 'We used values of 2.8, 3.7,', 'section': '', 'sec_num': None}, {'text': '6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.', 'section': '', 'sec_num': None}], 'back_matter': [{'text': 'Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.', 'section': 'acknowledgement', 'sec_num': None}], 'ref_entries': {'FIGREF0': {'text': 'Figure 1: The Transformer -model architecture.', 'num': None, 'type_str': 'figure', 'fig_num': '1', 'uris': None}, 'FIGREF1': {'text': 'Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.', 'num': None, 'type_str': 'figure', 'fig_num': '2', 'uris': None}, 'FIGREF2': {'text': \"Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.\", 'num': None, 'type_str': 'figure', 'fig_num': '3', 'uris': None}, 'FIGREF3': {'text': \"Figure4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\", 'num': None, 'type_str': 'figure', 'fig_num': '4', 'uris': None}, 'FIGREF4': {'text': 'Figure5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.', 'num': None, 'type_str': 'figure', 'fig_num': '5', 'uris': None}, 'TABREF0': {'text': 'Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.', 'num': None, 'type_str': 'table', 'html': None, 'content': '<table><tr><td>Layer Type</td><td>Complexity per Layer Sequential Maximum Path Length</td></tr><tr><td/><td>Operations</td></tr><tr><td>Self-Attention</td><td>O</td></tr></table>'}, 'TABREF1': {'text': 'The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.', 'num': None, 'type_str': 'table', 'html': None, 'content': '<table><tr><td>Model</td><td colspan=\"2\">BLEU EN-DE EN-FR</td><td>Training Cost (FLOPs) EN-DE EN-FR</td></tr><tr><td>ByteNet [18]</td><td>23.75</td><td/><td/></tr><tr><td>Deep-Att + PosUnk [39]</td><td/><td>39.2</td><td>1.0 \u2022 10 20</td></tr><tr><td>GNMT + RL [38]</td><td>24.6</td><td>39.92</td><td>2.3 \u2022 10 19 1.4 \u2022 10 20</td></tr><tr><td>ConvS2S [9]</td><td>25.16</td><td>40.46</td><td>9.6 \u2022 10 18 1.5 \u2022 10 20</td></tr><tr><td>MoE [32]</td><td>26.03</td><td>40.56</td><td>2.0 \u2022 10 19 1.2 \u2022 10 20</td></tr><tr><td>Deep-Att + PosUnk Ensemble [39]</td><td/><td>40.4</td><td>8.0 \u2022 10 20</td></tr><tr><td>GNMT + RL Ensemble [38]</td><td>26.30</td><td>41.16</td><td>1.8 \u2022 10 20 1.1 \u2022 10 21</td></tr><tr><td>ConvS2S Ensemble [9]</td><td>26.36</td><td>41.29</td><td>7.7 \u2022 10 19 1.2 \u2022 10 21</td></tr><tr><td>Transformer (base model)</td><td>27.3</td><td>38.1</td><td>3.3 \u2022 10 18</td></tr><tr><td>Transformer (big)</td><td>28.4</td><td>41.8</td><td>2.3 \u2022 10 19</td></tr></table>'}, 'TABREF2': {'text': 'Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.', 'num': None, 'type_str': 'table', 'html': None, 'content': '<table><tr><td/><td colspan=\"2\">N d model</td><td>d ff</td><td>h</td><td>d k</td><td>d v</td><td colspan=\"2\">P drop \u03f5 ls</td><td>train steps (dev) (dev) PPL BLEU params \u00d710 6</td></tr><tr><td colspan=\"2\">base 6</td><td>512</td><td colspan=\"2\">2048 8</td><td>64</td><td>64</td><td>0.1</td><td colspan=\"2\">0.1 100K 4.92</td><td>25.8</td><td>65</td></tr><tr><td/><td/><td/><td/><td colspan=\"3\">1 512 512</td><td/><td/><td>5.29</td><td>24.9</td></tr><tr><td>(A)</td><td/><td/><td/><td colspan=\"3\">4 128 128 16 32 32</td><td/><td/><td>5.00 4.91</td><td>25.5 25.8</td></tr><tr><td/><td/><td/><td/><td colspan=\"2\">32 16</td><td>16</td><td/><td/><td>5.01</td><td>25.4</td></tr><tr><td>(B)</td><td/><td/><td/><td/><td>16 32</td><td/><td/><td/><td>5.16 5.01</td><td>25.1 25.4</td><td>58 60</td></tr><tr><td/><td>2</td><td/><td/><td/><td/><td/><td/><td/><td>6.11</td><td>23.7</td><td>36</td></tr><tr><td/><td>4</td><td/><td/><td/><td/><td/><td/><td/><td>5.19</td><td>25.3</td><td>50</td></tr><tr><td/><td>8</td><td/><td/><td/><td/><td/><td/><td/><td>4.88</td><td>25.5</td><td>80</td></tr><tr><td>(C)</td><td/><td>256</td><td/><td/><td>32</td><td>32</td><td/><td/><td>5.75</td><td>24.5</td><td>28</td></tr><tr><td/><td/><td>1024</td><td/><td/><td colspan=\"2\">128 128</td><td/><td/><td>4.66</td><td>26.0</td><td>168</td></tr><tr><td/><td/><td/><td>1024</td><td/><td/><td/><td/><td/><td>5.12</td><td>25.4</td><td>53</td></tr><tr><td/><td/><td/><td>4096</td><td/><td/><td/><td/><td/><td>4.75</td><td>26.2</td><td>90</td></tr><tr><td/><td/><td/><td/><td/><td/><td/><td>0.0</td><td/><td>5.77</td><td>24.6</td></tr><tr><td>(D)</td><td/><td/><td/><td/><td/><td/><td>0.2</td><td>0.0</td><td>4.95 4.67</td><td>25.5 25.3</td></tr><tr><td/><td/><td/><td/><td/><td/><td/><td/><td>0.2</td><td>5.47</td><td>25.7</td></tr><tr><td>(E)</td><td/><td colspan=\"7\">positional embedding instead of sinusoids</td><td>4.92</td><td>25.7</td></tr><tr><td>big</td><td>6</td><td colspan=\"3\">1024 4096 16</td><td/><td/><td>0.3</td><td/><td>300K 4.33</td><td>26.4</td><td>213</td></tr><tr><td colspan=\"4\">development set, newstest2013</td><td/><td/><td/><td/><td/></tr></table>'}, 'TABREF3': {'text': 'The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)', 'num': None, 'type_str': 'table', 'html': None, 'content': '<table><tr><td>Parser</td><td>Training</td><td>WSJ 23 F1</td></tr><tr><td colspan=\"2\">Vinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative</td><td>88.3</td></tr><tr><td>Petrov et al. (2006) [29]</td><td>WSJ only, discriminative</td><td>90.4</td></tr><tr><td>Zhu et al. (2013) [40]</td><td>WSJ only, discriminative</td><td>90.4</td></tr><tr><td>Dyer et al. (2016) [8]</td><td>WSJ only, discriminative</td><td>91.7</td></tr><tr><td>Transformer (4 layers)</td><td>WSJ only, discriminative</td><td>91.3</td></tr><tr><td>Zhu et al. (2013) [40]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>Huang &amp; Harper (2009) [14]</td><td>semi-supervised</td><td>91.3</td></tr><tr><td>McClosky et al. (2006) [26]</td><td>semi-supervised</td><td>92.1</td></tr><tr><td>Vinyals &amp; Kaiser el al. (2014) [37]</td><td>semi-supervised</td><td>92.1</td></tr><tr><td>Transformer (4 layers)</td><td>semi-supervised</td><td>92.7</td></tr><tr><td>Luong et al. (2015) [23]</td><td>multi-task</td><td>93.0</td></tr><tr><td>Dyer et al. (2016) [8]</td><td>generative</td><td>93.3</td></tr></table>'}}}}\n\nGold Repository:\n\n```## File name: decoder.py\nimport torch\nimport torch.nn as nn\n\nfrom utils import PositionalEncoding, FullyConnected, LayerNorm\nfrom mha import MultiHeadAttention\n\nclass DecoderLayer(nn.Module):\n    \"\"\"\n    The decoder layer is composed by an masked multi-head self-attention mechanism,\n    followed by a multi-head attention mechanism to the output of the encoder and a \n    simple, position-wise fully connected feed-forward network. This architecture \n    includes residual connections around all of the three sub-layers, followed by \n    layer normalization.\n    \"\"\"\n    def __init__(self, num_heads, d_model, fully_connected_dim,\n                 dropout_rate=0.1, layernorm_eps=1e-6, **kwargs):\n        super(DecoderLayer, self).__init__(**kwargs)\n\n        self.masked_mha = MultiHeadAttention(num_heads=num_heads,\n                                             d_model=d_model,\n                                             dropout_rate=dropout_rate)\n\n        self.mha = MultiHeadAttention(num_heads=num_heads,\n                                      d_model=d_model,\n                                      dropout_rate=dropout_rate)\n\n        self.ffn = FullyConnected(embedding_dim=d_model,\n                                  fully_connected_dim=fully_connected_dim)\n\n        self.layernorm1 = LayerNorm(d_model, eps=layernorm_eps)\n        self.layernorm2 = LayerNorm(d_model, eps=layernorm_eps)\n        self.layernorm3 = LayerNorm(d_model, eps=layernorm_eps)\n\n        self.dropout_ffn = nn.Dropout(dropout_rate)\n\n    def forward(self, x, encoder_output, look_ahead_mask=None, padding_mask=None):\n        \"\"\"\n        Forward pass for the Decoder Layer\n        \n        Arguments:\n            x -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n            encoder_output -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n            look_ahead_mask -- Mask to ensure that the decoder does not attend to subsequent positions\n            padding_mask -- Boolean mask to ensure that the padding is not treated as part of the input\n        Returns:\n            decoder_layer_out -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n            attn_weights_block1 -- Tensor of shape (batch_size, num_heads, target_seq_len, target_seq_len)\n            attn_weights_block2 -- Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n        \"\"\"\n        # masked multi-head self-attention\n        mha_output1, attn_weights_block1 = self.masked_mha(x, x, x, look_ahead_mask) # (batch_size, target_seq_len, fully_connected_dim)\n\n        # skip connection\n        skip_x_attention1 = self.layernorm1(x + mha_output1)\n\n        # multi-head attention on encoder output\n        mha_output2, attn_weights_block2 = self.mha(skip_x_attention1, encoder_output, encoder_output, padding_mask) # (batch_size, target_seq_len, fully_connected_dim)\n\n        # skip connection\n        skip_x_attention2 = self.layernorm2(skip_x_attention1 + mha_output2)\n\n        # pass the output of the multi-head attention layer through a ffn\n        ffn_output = self.ffn(skip_x_attention2)\n\n        # apply dropout layer to ffn output during training\n        ffn_output = self.dropout_ffn(ffn_output)\n\n        # apply layer normalization on sum of the output from multi-head attention (skip connection) and ffn output to get the output of the decoder layer\n        decoder_layer_out = self.layernorm3(skip_x_attention2 + ffn_output)\n\n        return decoder_layer_out, attn_weights_block1, attn_weights_block2\n\nclass Decoder(nn.Module):\n    \"\"\"\n    The Decoder consists of N layers of DecoderLayer.\n    \"\"\"\n    def __init__(self, num_layers, num_heads, d_model, fully_connected_dim, target_vocab_size,\n                 maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(Decoder, self).__init__()\n        \n        self.num_layers = num_layers\n        self.d_model = d_model\n        \n        self.embedding = nn.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, maximum_position_encoding)\n        \n        self.decoder_layers = nn.ModuleList([\n            DecoderLayer(num_heads=num_heads,\n                         d_model=d_model,\n                         fully_connected_dim=fully_connected_dim,\n                         dropout_rate=dropout_rate,\n                         layernorm_eps=layernorm_eps)\n            for _ in range(num_layers)\n        ])\n        \n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n        \"\"\"\n        Forward pass for the Decoder.\n        \n        Arguments:\n            x -- Tensor of shape (batch_size, target_seq_len)\n            enc_output -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n            look_ahead_mask -- Boolean mask for the target sequence\n            padding_mask -- Boolean mask for the input sequence\n            \n        Returns:\n            decoder_output -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\n            attention_weights -- Dictionary of attention weights for each decoder layer\n        \"\"\"\n        x = self.embedding(x) # (batch_size, target_seq_len, d_model)\n        x *= torch.sqrt(torch.tensor(self.d_model).float())\n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n        \n        attention_weights = {}\n        \n        for i in range(self.num_layers):\n            x, attn_wt_1,  attn_wt_2 = self.decoder_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n            attention_weights[f\"decoder_layer{i + 1}_block1_self_att\"] = attn_wt_1\n            attention_weights[f\"decoder_layer{i + 1}_block2_decenc_att\"] = attn_wt_2\n        \n        return x, attention_weights\n\n```\n\n```## File name: __init__.py\nfrom .transformer import Transformer\nfrom .utils import create_padding_mask, create_look_ahead_mask\n\n__all__ = ['Transformer', 'create_padding_mask', 'create_look_ahead_mask']\n\n```\n\n```## File name: encoder.py\nimport torch.nn as nn\nimport math\n\nfrom utils import PositionalEncoding, FullyConnected, LayerNorm\nfrom mha import MultiHeadAttention\n\nclass EncoderLayer(nn.Module):\n    \"\"\"\n    The encoder layer is composed of a multi-head self-attention mechanism,\n    followed by a simple, position-wise fully connected feed-forward network. \n    This architecture includes a residual connection around each of the two \n    sub-layers, followed by layer normalization.\n    \"\"\"\n    def __init__(self, num_heads, d_model, fully_connected_dim,\n                 dropout_rate=0.1, layernorm_eps=1e-6):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(num_heads=num_heads,\n                                      d_model=d_model,\n                                      dropout_rate=dropout_rate)\n\n        self.ffn = FullyConnected(embedding_dim=d_model,\n                                  fully_connected_dim=fully_connected_dim)\n\n        self.layernorm1 = LayerNorm(d_model, eps=layernorm_eps)\n        self.layernorm2 = LayerNorm(d_model, eps=layernorm_eps)\n\n        self.dropout_ffn = nn.Dropout(dropout_rate)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Forward pass for the Encoder Layer\n        \n        Arguments:\n            x -- Tensor of shape (batch_size, input_seq_len, d_model)\n            mask -- Boolean mask to ensure that the padding is not \n                    treated as part of the input\n        Returns:\n            encoder_layer_out -- Tensor of shape (batch_size, input_seq_len, d_model)\n        \"\"\"\n        # calculate Self-Attention using Multi-Head Attention\n        mha_output, _ = self.mha(x, x, x, mask)  # Self attention (batch_size, input_seq_len, d_model)\n\n        # skip connection\n        # apply layer normalization on sum of the input and the attention output to get the output of the multi-head attention layer\n        skip_x_attention = self.layernorm1(x + mha_output)\n\n        # pass the output of the multi-head attention layer through a ffn\n        ffn_output = self.ffn(skip_x_attention)\n\n        # apply dropout layer to ffn output during training\n        ffn_output = self.dropout_ffn(ffn_output)\n\n        # apply layer normalization on sum of the output from multi-head attention (skip connection) and ffn output to get the output of the encoder layer\n        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)\n\n        return encoder_layer_out\n\nclass Encoder(nn.Module):\n    \"\"\"\n    The encoder is composed by a stack of identical layers (EncoderLayers).\n    \"\"\"\n    def __init__(self, num_layers, num_heads, d_model, fully_connected_dim,\n                 input_vocab_size, maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(Encoder, self).__init__()\n        \n        self.num_layers = num_layers\n        self.d_model = d_model\n        \n        self.embedding = nn.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, maximum_position_encoding)\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        \n        self.enc_layers = nn.ModuleList([EncoderLayer(num_heads, d_model, fully_connected_dim,\n                                                       dropout_rate=dropout_rate, layernorm_eps=layernorm_eps)\n                                          for _ in range(num_layers)])\n        \n    def forward(self, x, mask=None):\n        \"\"\"\n        Forward pass for the Encoder\n        \n        Arguments:\n            x -- Tensor of shape (batch_size, input_seq_len)\n            mask -- Boolean mask to ensure that the padding is not \n                    treated as part of the input\n        Returns:\n            encoder_out -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n        \"\"\"\n        seq_len = x.size(1)\n        \n        # Embedding lookup and scaling\n        x = self.embedding(x) * math.sqrt(self.d_model)\n        \n        # Add position encoding to the input\n        x = self.pos_encoding(x)\n        \n        # Apply dropout to the input\n        x = self.dropout(x)\n        \n        # Pass the input through each encoder layer\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, mask)\n        \n        encoder_out = x\n        \n        return encoder_out\n\n```\n\n```## File name: utils.py\nimport torch\nimport torch.nn as nn\nimport math\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass FullyConnected(nn.Module):\n    def __init__(self, embedding_dim, fully_connected_dim):\n        super(FullyConnected, self).__init__()\n        self.fc1 = nn.Linear(embedding_dim, fully_connected_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(fully_connected_dim, embedding_dim)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        \n        return self.fc2(x)\n\nclass LayerNorm(torch.nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.hidden_size = hidden_size\n        self.eps = eps\n        \n        self.scale = torch.nn.Parameter(torch.ones(self.hidden_size))\n        self.bias = torch.nn.Parameter(torch.zeros(self.hidden_size))\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        normalized = (x - mean) / (std + self.eps)\n        \n        return self.scale * normalized + self.bias\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.d_model = d_model\n        self.pe = self.get_positional_encoding(max_seq_len)\n\n    def get_positional_encoding(self, max_seq_len):\n        # Generate positional encoding matrix\n        pe = torch.zeros(max_seq_len, self.d_model)\n        position = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / self.d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        \n        return pe.to(device)\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        pe = self.pe[:, :seq_len, :]\n        x = x * math.sqrt(self.d_model)\n        x = x + pe\n        \n        return x\n\ndef create_padding_mask(decoder_token_ids):\n    \"\"\"\n    Creates a matrix mask for the padding cells\n    \n    Arguments:\n        decoder_token_ids -- (n, m) matrix\n    \n    Returns:\n        mask -- (n, 1, m) binary tensor\n    \"\"\"\n    # Create a matrix mask for the padding cells\n    seq = 1 - torch.eq(decoder_token_ids, 0).float()\n  \n    # Add extra dimensions to add the padding to the attention logits\n    return seq.unsqueeze(1).unsqueeze(2)\n\ndef create_look_ahead_mask(sequence_length):\n    \"\"\"\n    Returns a lower triangular matrix filled with ones\n    \n    Arguments:\n        sequence_length -- matrix size\n    \n    Returns:\n        mask -- (size, size) tensor\n    \"\"\"\n    # Create a lower triangular matrix filled with ones\n    mask = torch.tril(torch.ones(sequence_length, sequence_length))\n    \n    return mask\n\n```\n\n```## File name: transformer.py\nimport torch\nimport torch.nn as nn\n\nfrom encoder import Encoder\nfrom decoder import Decoder\n\nclass Transformer(nn.Module):\n    def __init__(self, num_encoder_layers, num_decoder_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, max_seq_len, dropout_rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_encoder_layers, num_heads, d_model, dff, input_vocab_size, max_seq_len, dropout_rate)\n\n        self.decoder = Decoder(num_decoder_layers, num_heads, d_model, dff, target_vocab_size, max_seq_len, dropout_rate)\n\n        self.final_layer = nn.Linear(d_model, target_vocab_size)\n\n    def forward(self, inp, tar, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n        enc_output = self.encoder(inp, enc_padding_mask)\n\n        dec_output, attention_weights = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)\n\n        return final_output, attention_weights\n\n```\n\n```## File name: test_transformer.py\nimport torch\nimport unittest\nfrom transformer import Transformer\nfrom utils import create_look_ahead_mask, create_padding_mask\n\nclass TestTransformer(unittest.TestCase):\n    \n    def setUp(self):\n        self.num_layer = 6\n        self.input_vocab_size=100\n        self.target_vocab_size=100\n        self.max_seq_len=100\n        self.batch_size = 3\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.transformer = Transformer(\n            num_encoder_layers=self.num_layer,\n            num_decoder_layers=self.num_layer,\n            d_model=512,\n            num_heads=8,\n            dff=2048,\n            input_vocab_size=self.input_vocab_size,\n            target_vocab_size=self.target_vocab_size,\n            max_seq_len=self.max_seq_len,\n            dropout_rate=0.1\n        )\n        \n    def test_forward_pass(self):\n        x = torch.randint(0, self.input_vocab_size, (self.batch_size, self.max_seq_len)).to(self.device)\n        y = torch.randint(0, self.target_vocab_size, (self.batch_size, self.max_seq_len)).to(self.device)\n        \n        enc_padding_mask = create_padding_mask(x).to(self.device)\n        dec_padding_mask = create_padding_mask(y).to(self.device)\n        look_ahead_mask = create_look_ahead_mask(self.max_seq_len).to(self.device)\n        \n        output, _ = self.transformer(x, y, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n        \n        self.assertEqual(output.shape, (self.batch_size, self.max_seq_len, self.target_vocab_size))\n        \nif __name__ == \"__main__\":\n    unittest.main()\n\n```\n\n```## File name: mha.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout_rate, **kwargs):\n        super(ScaledDotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, queries, keys, values, mask=None):\n        # Calculate dot products between the query and the key\n        matmul_qk = torch.matmul(queries, keys.transpose(-2, -1))\n\n        dk = keys.size()[-1]\n        scaled_attention_logits = matmul_qk / (dk ** 0.5)\n\n        if mask is not None:\n            scaled_attention_logits += (1. - mask) * -1e9 \n        \n        # Apply softmax function to obtain attention weights\n        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n\n        # Apply dropout to the attention weights\n        attention_weights_dropout = self.dropout(attention_weights)\n\n        output = torch.matmul(attention_weights_dropout, values)\n\n        return output, attention_weights\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, d_model, dropout_rate=0.1):\n        super(MultiHeadAttention, self).__init__()\n\n        assert d_model % num_heads == 0\n\n        self.attention = ScaledDotProductAttention(dropout_rate)\n\n        self.num_heads = num_heads\n        self.d_head = d_model // num_heads # key_dim = value_dim = d_head\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # Apply linear transformations to the inputs\n        query = self.W_q(query)\n        key = self.W_k(key)\n        value = self.W_v(value)\n\n        # Reshape the inputs for appropriate attention calculation\n        query = query.view(batch_size, -1, self.num_heads, self.d_head).transpose(1, 2)  # (batch_size, num_heads, query_seq_len, d_head)\n        key = key.view(batch_size, -1, self.num_heads, self.d_head).transpose(1, 2)  # (batch_size, num_heads, key_seq_len, d_head)\n        value = value.view(batch_size, -1, self.num_heads, self.d_head).transpose(1, 2)  # (batch_size, num_heads, value_seq_len, d_head)\n\n        context, attention_weights = self.attention(query, key, value, mask)\n\n        # Concatenate the output of the heads and apply a linear transformation\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_head)  # (batch_size, query_seq_len, d_model)\n        output = self.W_o(context)  # (batch_size, query_seq_len, d_model)\n\n        return output, attention_weights\n\n```\n\n\n\nTarget Repository:\n\n```python\n## File name: config.py\n\"\"\"config.py\n\nThis module defines the central configuration for the project.\nIt reads from a YAML file \"config.yaml\" (if available) and exposes configuration\nparameters in a structured and type\u2010safe manner. All other modules (dataset_loader.py,\nmodel.py, trainer.py, evaluation.py, utils.py, main.py) import this file to ensure\nconsistency and reproducibility.\n\nThe configuration includes training parameters, model hyperparameters,\ninference settings for machine translation, parsing experiment settings, dataset details,\nand hardware specifications.\n\"\"\"\n\nimport os\nimport yaml\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any\n\n# ---------------------------\n# Training Configuration\n# ---------------------------\n@dataclass(frozen=True)\nclass AdamConfig:\n    beta1: float = 0.9\n    beta2: float = 0.98\n    epsilon: float = 1e-9\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'AdamConfig':\n        return AdamConfig(\n            beta1=float(data.get('beta1', 0.9)),\n            beta2=float(data.get('beta2', 0.98)),\n            epsilon=float(data.get('epsilon', 1e-9))\n        )\n\n@dataclass(frozen=True)\nclass TrainingConfig:\n    total_steps: int = 100000\n    warmup_steps: int = 4000\n    optimizer: str = \"adam\"\n    adam: AdamConfig = field(default_factory=AdamConfig)\n    learning_rate_schedule: str = \"d_model^-0.5 * min(step^-0.5, step * warmup_steps^-1.5)\"\n    dropout_rate: float = 0.1\n    batch_tokens: int = 25000\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'TrainingConfig':\n        adam_data = data.get('adam', {})\n        return TrainingConfig(\n            total_steps=int(data.get('total_steps', 100000)),\n            warmup_steps=int(data.get('warmup_steps', 4000)),\n            optimizer=str(data.get('optimizer', \"adam\")),\n            adam=AdamConfig.from_dict(adam_data),\n            learning_rate_schedule=str(data.get('learning_rate_schedule',\n                                                  \"d_model^-0.5 * min(step^-0.5, step * warmup_steps^-1.5)\")),\n            dropout_rate=float(data.get('dropout_rate', 0.1)),\n            batch_tokens=int(data.get('batch_tokens', 25000))\n        )\n\n# ---------------------------\n# Model Configuration\n# ---------------------------\n@dataclass(frozen=True)\nclass ModelConfig:\n    type: str = \"transformer_base\"\n    d_model: int = 512\n    num_layers: int = 6\n    d_ff: int = 2048\n    num_heads: int = 8\n    d_k: int = 64\n    d_v: int = 64\n    positional_encoding: str = \"sinusoidal\"\n    share_embedding: bool = True\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'ModelConfig':\n        return ModelConfig(\n            type=str(data.get('type', \"transformer_base\")),\n            d_model=int(data.get('d_model', 512)),\n            num_layers=int(data.get('num_layers', 6)),\n            d_ff=int(data.get('d_ff', 2048)),\n            num_heads=int(data.get('num_heads', 8)),\n            d_k=int(data.get('d_k', 64)),\n            d_v=int(data.get('d_v', 64)),\n            positional_encoding=str(data.get('positional_encoding', \"sinusoidal\")),\n            share_embedding=bool(data.get('share_embedding', True))\n        )\n\n# ---------------------------\n# Inference (Translation) Configuration\n# ---------------------------\n@dataclass(frozen=True)\nclass InferenceConfig:\n    beam_size: int = 4\n    length_penalty: float = 0.6\n    max_output_length_offset: int = 50\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'InferenceConfig':\n        return InferenceConfig(\n            beam_size=int(data.get('beam_size', 4)),\n            length_penalty=float(data.get('length_penalty', 0.6)),\n            max_output_length_offset=int(data.get('max_output_length_offset', 50))\n        )\n\n# ---------------------------\n# Parsing Experiment Configuration\n# ---------------------------\n@dataclass(frozen=True)\nclass ParsingModelConfig:\n    type: str = \"transformer_parsing\"\n    num_layers: int = 4\n    d_model: int = 1024\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'ParsingModelConfig':\n        return ParsingModelConfig(\n            type=str(data.get('type', \"transformer_parsing\")),\n            num_layers=int(data.get('num_layers', 4)),\n            d_model=int(data.get('d_model', 1024))\n        )\n\n@dataclass(frozen=True)\nclass ParsingVocabConfig:\n    wsj: int = 16000\n    semi_supervised: int = 32000\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'ParsingVocabConfig':\n        return ParsingVocabConfig(\n            wsj=int(data.get('wsj', 16000)),\n            semi_supervised=int(data.get('semi_supervised', 32000))\n        )\n\n@dataclass(frozen=True)\nclass ParsingInferenceConfig:\n    beam_size: int = 21\n    length_penalty: float = 0.3\n    max_output_length_offset: int = 300\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'ParsingInferenceConfig':\n        return ParsingInferenceConfig(\n            beam_size=int(data.get('beam_size', 21)),\n            length_penalty=float(data.get('length_penalty', 0.3)),\n            max_output_length_offset=int(data.get('max_output_length_offset', 300))\n        )\n\n@dataclass(frozen=True)\nclass ParsingConfig:\n    model: ParsingModelConfig = field(default_factory=ParsingModelConfig)\n    vocab_size: ParsingVocabConfig = field(default_factory=ParsingVocabConfig)\n    inference: ParsingInferenceConfig = field(default_factory=ParsingInferenceConfig)\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'ParsingConfig':\n        model_data = data.get('model', {})\n        vocab_data = data.get('vocab_size', {})\n        inference_data = data.get('inference', {})\n        return ParsingConfig(\n            model=ParsingModelConfig.from_dict(model_data),\n            vocab_size=ParsingVocabConfig.from_dict(vocab_data),\n            inference=ParsingInferenceConfig.from_dict(inference_data)\n        )\n\n# ---------------------------\n# Data Configuration\n# ---------------------------\n@dataclass(frozen=True)\nclass WMTConfig:\n    dataset_size: str = \"4.5M sentence pairs\"\n    tokenizer: str = \"BPE\"\n    vocab_size: int = 37000\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any], default_tokenizer: str, default_vocab: int) -> 'WMTConfig':\n        return WMTConfig(\n            dataset_size=str(data.get('dataset_size', \"4.5M sentence pairs\")),\n            tokenizer=str(data.get('tokenizer', default_tokenizer)),\n            vocab_size=int(data.get('vocab_size', default_vocab))\n        )\n\n@dataclass(frozen=True)\nclass WSJConfig:\n    dataset: str = \"Penn Treebank WSJ\"\n    training_size: str = \"40K sentences\"\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'WSJConfig':\n        return WSJConfig(\n            dataset=str(data.get('dataset', \"Penn Treebank WSJ\")),\n            training_size=str(data.get('training_size', \"40K sentences\"))\n        )\n\n@dataclass(frozen=True)\nclass SemiSupervisedParsingConfig:\n    dataset: str = \"BerkleyParser and high-confidence corpus\"\n    training_size: str = \"17M sentences\"\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'SemiSupervisedParsingConfig':\n        return SemiSupervisedParsingConfig(\n            dataset=str(data.get('dataset', \"BerkleyParser and high-confidence corpus\")),\n            training_size=str(data.get('training_size', \"17M sentences\"))\n        )\n\n@dataclass(frozen=True)\nclass DataConfig:\n    wmt_2014_en_de: WMTConfig = field(default_factory=lambda: WMTConfig())\n    wmt_2014_en_fr: WMTConfig = field(default_factory=lambda: WMTConfig(tokenizer=\"wordpiece\", vocab_size=32000))\n    wsj: WSJConfig = field(default_factory=WSJConfig)\n    semi_supervised_parsing: SemiSupervisedParsingConfig = field(default_factory=SemiSupervisedParsingConfig)\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'DataConfig':\n        wmt_de_data = data.get('wmt_2014_en_de', {})\n        wmt_fr_data = data.get('wmt_2014_en_fr', {})\n        wsj_data = data.get('wsj', {})\n        semi_data = data.get('semi_supervised_parsing', {})\n        wmt_2014_en_de = WMTConfig.from_dict(wmt_de_data, default_tokenizer=\"BPE\", default_vocab=37000)\n        wmt_2014_en_fr = WMTConfig.from_dict(wmt_fr_data, default_tokenizer=\"wordpiece\", default_vocab=32000)\n        wsj_config = WSJConfig.from_dict(wsj_data)\n        semi_config = SemiSupervisedParsingConfig.from_dict(semi_data)\n        return DataConfig(\n            wmt_2014_en_de=wmt_2014_en_de,\n            wmt_2014_en_fr=wmt_2014_en_fr,\n            wsj=wsj_config,\n            semi_supervised_parsing=semi_config\n        )\n\n# ---------------------------\n# Hardware Configuration\n# ---------------------------\n@dataclass(frozen=True)\nclass HardwareConfig:\n    gpus: int = 8\n    gpu_type: str = \"NVIDIA P100\"\n    base_model_training_time: str = \"12 hours\"\n    big_model_training_time: str = \"3.5 days\"\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'HardwareConfig':\n        return HardwareConfig(\n            gpus=int(data.get('gpus', 8)),\n            gpu_type=str(data.get('gpu_type', \"NVIDIA P100\")),\n            base_model_training_time=str(data.get('base_model_training_time', \"12 hours\")),\n            big_model_training_time=str(data.get('big_model_training_time', \"3.5 days\"))\n        )\n\n# ---------------------------\n# Global Configuration\n# ---------------------------\n@dataclass(frozen=True)\nclass Config:\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n    model: ModelConfig = field(default_factory=ModelConfig)\n    inference: InferenceConfig = field(default_factory=InferenceConfig)\n    parsing: ParsingConfig = field(default_factory=ParsingConfig)\n    data: DataConfig = field(default_factory=DataConfig)\n    hardware: HardwareConfig = field(default_factory=HardwareConfig)\n\n    @staticmethod\n    def from_dict(data: Dict[str, Any]) -> 'Config':\n        training_data = data.get('training', {})\n        model_data = data.get('model', {})\n        inference_data = data.get('inference', {})\n        parsing_data = data.get('parsing', {})\n        data_section = data.get('data', {})\n        hardware_data = data.get('hardware', {})\n\n        training_cfg = TrainingConfig.from_dict(training_data)\n        model_cfg = ModelConfig.from_dict(model_data)\n        inference_cfg = InferenceConfig.from_dict(inference_data)\n        parsing_cfg = ParsingConfig.from_dict(parsing_data)\n        data_cfg = DataConfig.from_dict(data_section)\n        hardware_cfg = HardwareConfig.from_dict(hardware_data)\n\n        config_instance = Config(\n            training=training_cfg,\n            model=model_cfg,\n            inference=inference_cfg,\n            parsing=parsing_cfg,\n            data=data_cfg,\n            hardware=hardware_cfg\n        )\n\n        # Validate that model.d_model equals num_heads * d_k.\n        if config_instance.model.d_model != config_instance.model.num_heads * config_instance.model.d_k:\n            raise ValueError(\n                f\"Invalid model configuration: d_model ({config_instance.model.d_model}) != \"\n                f\"num_heads ({config_instance.model.num_heads}) * d_k ({config_instance.model.d_k}).\"\n            )\n        return config_instance\n\ndef load_config(config_path: str = \"config.yaml\") -> Config:\n    \"\"\"\n    Loads configuration parameters from a YAML file and returns a Config instance.\n    If the file does not exist or is empty, default values are used.\n    \"\"\"\n    if os.path.exists(config_path):\n        with open(config_path, \"r\") as f:\n            yaml_data = yaml.safe_load(f)\n            if yaml_data is None:\n                yaml_data = {}\n    else:\n        yaml_data = {}\n    return Config.from_dict(yaml_data)\n\n# Global configuration instance used by other modules.\nCONFIG: Config = load_config()\n\nif __name__ == \"__main__\":\n    # For debugging: print the loaded configuration.\n    print(\"Loaded Configuration:\")\n    print(CONFIG)\n\n```\n\n```python\n## File name: utils.py\n\"\"\"\nutils.py\n\nThis module provides utility functions for logging, checkpoint management, tokenization, \nand miscellaneous file/directory operations. All functions rely on configuration \nparameters sourced from the central config.py to ensure consistency and reproducibility.\n\"\"\"\n\nimport logging\nimport os\nimport time\nfrom typing import Any, Dict, List, Optional\n\nimport torch\nfrom tokenizers import Tokenizer, models, pre_tokenizers\n\n# Import configuration from config.py\nfrom config import CONFIG\n\n\ndef initialize_logger(log_level: str = \"INFO\", log_file: Optional[str] = None) -> logging.Logger:\n    \"\"\"\n    Initialize and configure a logger instance.\n\n    Args:\n        log_level (str): Logging level as a string (e.g., \"INFO\", \"DEBUG\"). Default is \"INFO\".\n        log_file (Optional[str]): Optional file path for file logging. If provided,\n                                  logs will also be written to this file.\n\n    Returns:\n        logging.Logger: Configured logger instance.\n    \"\"\"\n    logger = logging.getLogger(\"transformer_logger\")\n    # Set logging level based on provided log_level or default to INFO.\n    logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n    \n    # Prevent adding duplicate handlers if logger is already configured.\n    if not logger.handlers:\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n        \n        # Console handler for standard output.\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n        console_handler.setFormatter(formatter)\n        logger.addHandler(console_handler)\n        \n        # File handler, if a log file path is provided.\n        if log_file:\n            # Ensure the directory for the log file exists.\n            log_dir = os.path.dirname(log_file)\n            if log_dir:\n                create_dir_if_not_exists(log_dir)\n            file_handler = logging.FileHandler(log_file)\n            file_handler.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n            file_handler.setFormatter(formatter)\n            logger.addHandler(file_handler)\n            \n    return logger\n\n\n# Global logger instance to be used by all modules.\nLOGGER: logging.Logger = initialize_logger()\n\n\ndef log_metrics(step: int, metrics: Dict[str, Any], logger: logging.Logger = LOGGER) -> None:\n    \"\"\"\n    Log training progress metrics in a standardized format.\n\n    Args:\n        step (int): The current training step/epoch.\n        metrics (Dict[str, Any]): Dictionary containing metric names and their corresponding values.\n        logger (logging.Logger): Logger instance to use. Defaults to the global LOGGER.\n    \"\"\"\n    metrics_str = \", \".join(\n        [f\"{key}: {value:.4f}\" if isinstance(value, float) else f\"{key}: {value}\" for key, value in metrics.items()]\n    )\n    logger.info(f\"Step {step}: {metrics_str}\")\n\n\ndef save_checkpoint(\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    step: int,\n    checkpoint_dir: str,\n    additional_state: Optional[Dict[str, Any]] = None\n) -> None:\n    \"\"\"\n    Save the current training state to a checkpoint file.\n\n    Args:\n        model (torch.nn.Module): The model whose state is to be saved.\n        optimizer (torch.optim.Optimizer): The optimizer whose state is to be saved.\n        step (int): The current training step.\n        checkpoint_dir (str): Directory where checkpoint files are to be saved.\n        additional_state (Optional[Dict[str, Any]]): Any additional state information (e.g., scheduler state).\n    \"\"\"\n    try:\n        create_dir_if_not_exists(checkpoint_dir)\n        checkpoint = {\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"step\": step\n        }\n        if additional_state:\n            checkpoint.update(additional_state)\n        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_step_{step}.pth\")\n        torch.save(checkpoint, checkpoint_path)\n        LOGGER.info(f\"Checkpoint saved at step {step} to {checkpoint_path}\")\n    except Exception as e:\n        LOGGER.error(f\"Failed to save checkpoint at step {step}: {str(e)}\")\n\n\ndef load_checkpoint(filepath: str, model: torch.nn.Module, optimizer: torch.optim.Optimizer) -> Dict[str, Any]:\n    \"\"\"\n    Load a checkpoint from the disk and update the model and optimizer states.\n\n    Args:\n        filepath (str): Path to the checkpoint file.\n        model (torch.nn.Module): Model whose state is to be loaded.\n        optimizer (torch.optim.Optimizer): Optimizer whose state is to be loaded.\n\n    Returns:\n        Dict[str, Any]: The checkpoint dictionary containing additional state information (if any).\n    \"\"\"\n    try:\n        checkpoint = torch.load(filepath, map_location=\"cpu\")\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        step = checkpoint.get(\"step\", 0)\n        LOGGER.info(f\"Checkpoint loaded from {filepath} at step {step}\")\n        return checkpoint\n    except Exception as e:\n        LOGGER.error(f\"Error loading checkpoint from {filepath}: {str(e)}\")\n        return {}\n\n\ndef load_tokenizer(tokenizer_type: str, vocab_file: Optional[str] = None) -> Any:\n    \"\"\"\n    Load and initialize a tokenizer based on the specified tokenizer type.\n\n    Args:\n        tokenizer_type (str): Type of tokenizer to load (\"BPE\" or \"wordpiece\").\n        vocab_file (Optional[str]): Optional path to a tokenizer/vocabulary file.\n                                    If None, a default file is used based on the tokenizer type.\n\n    Returns:\n        Any: An initialized tokenizer instance.\n    \"\"\"\n    default_vocab_file: str = \"\"\n    if tokenizer_type.lower() == \"bpe\":\n        default_vocab_file = \"bpe_tokenizer.json\"\n    elif tokenizer_type.lower() == \"wordpiece\":\n        default_vocab_file = \"wordpiece_tokenizer.json\"\n    else:\n        LOGGER.error(f\"Unsupported tokenizer type: {tokenizer_type}. Falling back to basic whitespace tokenizer.\")\n    \n    # Use the provided vocabulary file if available; otherwise, use the default.\n    vocab_file_to_use = vocab_file if vocab_file is not None else default_vocab_file\n    \n    try:\n        if vocab_file_to_use and os.path.isfile(vocab_file_to_use):\n            tokenizer = Tokenizer.from_file(vocab_file_to_use)\n            LOGGER.info(f\"Loaded tokenizer from {vocab_file_to_use}\")\n        else:\n            # If a vocabulary file is not found, initialize a basic whitespace tokenizer.\n            LOGGER.warning(f\"Tokenizer file '{vocab_file_to_use}' not found. Initializing basic whitespace tokenizer.\")\n            from tokenizers.models import WordLevel\n            tokenizer = Tokenizer(WordLevel({\"[UNK]\": 0}, unk_token=\"[UNK]\"))\n            tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n        return tokenizer\n    except Exception as e:\n        LOGGER.error(f\"Failed to load tokenizer: {str(e)}\")\n        raise e\n\n\ndef encode_text(text: str, tokenizer: Any) -> List[int]:\n    \"\"\"\n    Encode a text string into a sequence of token IDs using the provided tokenizer.\n\n    Args:\n        text (str): Input text to encode.\n        tokenizer (Any): Tokenizer instance that supports the encode() method.\n\n    Returns:\n        List[int]: The list of token IDs produced by the tokenizer.\n    \"\"\"\n    try:\n        encoding = tokenizer.encode(text)\n        # The tokenizer's encode method typically returns an object with an 'ids' attribute.\n        if hasattr(encoding, \"ids\"):\n            return encoding.ids\n        return encoding\n    except Exception as e:\n        LOGGER.error(f\"Error encoding text: {str(e)}\")\n        return []\n\n\ndef decode_tokens(token_ids: List[int], tokenizer: Any) -> str:\n    \"\"\"\n    Decode a sequence of token IDs back into a text string using the provided tokenizer.\n\n    Args:\n        token_ids (List[int]): List of token IDs to decode.\n        tokenizer (Any): Tokenizer instance that supports the decode() method.\n\n    Returns:\n        str: The decoded text string.\n    \"\"\"\n    try:\n        decoded_text = tokenizer.decode(token_ids)\n        return decoded_text\n    except Exception as e:\n        LOGGER.error(f\"Error decoding tokens: {str(e)}\")\n        return \"\"\n\n\ndef create_dir_if_not_exists(path: str) -> None:\n    \"\"\"\n    Ensure that a directory exists; if not, create it.\n\n    Args:\n        path (str): Path of the directory to create.\n    \"\"\"\n    try:\n        if path and not os.path.isdir(path):\n            os.makedirs(path, exist_ok=True)\n            LOGGER.info(f\"Directory created at: {path}\")\n    except Exception as e:\n        LOGGER.error(f\"Error creating directory '{path}': {str(e)}\")\n\n\ndef format_time(seconds: float) -> str:\n    \"\"\"\n    Format a time duration in seconds to a string in HH:MM:SS format.\n\n    Args:\n        seconds (float): Number of seconds.\n\n    Returns:\n        str: Time formatted as a string (HH:MM:SS).\n    \"\"\"\n    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n\n```\n\n```python\n## File name: dataset_loader.py\n\"\"\"dataset_loader.py\n\nThis module defines the DatasetLoader class, responsible for reading raw text data\n(from both machine translation and constituency parsing sources), performing tokenization \n(using Hugging Face tokenizers), building vocabularies, splitting data into training, \ndevelopment, and test sets, and grouping tokenized sequences into batches based on an \napproximate token count (as specified in the configuration).\n\nThe DatasetLoader class follows the design interface:\n    + __init__(config: dict)\n    + load_data() -> Tuple[Dataset, Dataset, Dataset]\n\nThe returned dataset for each split is a list of batch dictionaries, where each batch \ncontains tokenized sequences (and their lengths) appropriate for subsequent training steps.\n\"\"\"\n\nimport os\nfrom typing import Any, Dict, List, Tuple, Optional\n\nimport torch\n\n# Import configuration and utility functions.\nfrom config import CONFIG, Config\nfrom utils import LOGGER, load_tokenizer, encode_text, create_dir_if_not_exists\n\n\nclass DatasetLoader:\n    \"\"\"\n    DatasetLoader is responsible for loading, tokenizing, splitting, and batching data for both\n    machine translation and constituency parsing experiments based on the provided configuration.\n    \"\"\"\n\n    def __init__(self, config: Config, task: str = \"translation\") -> None:\n        \"\"\"\n        Initialize the DatasetLoader with configuration and task type.\n\n        Args:\n            config (Config): Configuration instance loaded from config.yaml via config.py.\n            task (str): Task type; either \"translation\" or \"parsing\". Defaults to \"translation\".\n        \"\"\"\n        self.config: Config = config\n        self.task: str = task.lower()\n        self.batch_tokens: int = self.config.training.batch_tokens\n\n        # Set default file paths based on task.\n        if self.task == \"translation\":\n            # For machine translation, use the WMT 2014 English-to-German dataset by default.\n            # Expected file paths (one for source and one for target) are in the 'data' directory.\n            self.translation_dataset: str = \"wmt_2014_en_de\"\n            self.src_file: str = os.path.join(\"data\", \"wmt_2014_en_de.src\")\n            self.tgt_file: str = os.path.join(\"data\", \"wmt_2014_en_de.tgt\")\n        elif self.task == \"parsing\":\n            # For constituency parsing, use the WSJ dataset by default.\n            self.parsing_dataset: str = \"wsj\"\n            self.wsj_file: str = os.path.join(\"data\", \"wsj.txt\")\n        else:\n            LOGGER.error(f\"Unsupported task type: {self.task}. Defaulting to 'translation'.\")\n            self.task = \"translation\"\n            self.translation_dataset = \"wmt_2014_en_de\"\n            self.src_file = os.path.join(\"data\", \"wmt_2014_en_de.src\")\n            self.tgt_file = os.path.join(\"data\", \"wmt_2014_en_de.tgt\")\n\n        LOGGER.info(f\"DatasetLoader initialized for task '{self.task}' with batch token limit {self.batch_tokens}.\")\n\n    def _read_lines(self, file_path: str) -> List[str]:\n        \"\"\"\n        Reads a file from the given file path and returns a list of stripped text lines.\n\n        Args:\n            file_path (str): Path to the text file.\n\n        Returns:\n            List[str]: List of text lines.\n        \"\"\"\n        if not os.path.isfile(file_path):\n            LOGGER.error(f\"File not found: {file_path}\")\n            return []\n        try:\n            with open(file_path, mode=\"r\", encoding=\"utf-8\") as f:\n                lines: List[str] = [line.strip() for line in f if line.strip()]\n            LOGGER.info(f\"Read {len(lines)} lines from {file_path}.\")\n            return lines\n        except Exception as e:\n            LOGGER.error(f\"Error reading file {file_path}: {str(e)}\")\n            return []\n\n    def _tokenize_translation(\n        self, src_lines: List[str], tgt_lines: List[str]\n    ) -> List[Tuple[List[int], List[int]]]:\n        \"\"\"\n        Tokenizes parallel source and target sentence lists using the configured tokenizer.\n\n        If the model configuration indicates shared embeddings (share_embedding = True),\n        the same tokenizer is used for both source and target; otherwise, separate tokenizers \n        are loaded based on the dataset configuration.\n\n        Args:\n            src_lines (List[str]): List of source language sentences.\n            tgt_lines (List[str]): List of target language sentences.\n\n        Returns:\n            List[Tuple[List[int], List[int]]]: List of tuples containing tokenized source and\n                                                target sentences as lists of token IDs.\n        \"\"\"\n        # Select dataset configuration for translation.\n        # Here we use the wmt_2014_en_de configuration.\n        dataset_config = self.config.data.wmt_2014_en_de\n        tokenizer_type: str = dataset_config.tokenizer  # e.g., \"BPE\"\n        vocab_size: int = dataset_config.vocab_size\n\n        # Load tokenizer. If shared embedding is True, use one tokenizer for both language sides.\n        tokenizer = load_tokenizer(tokenizer_type)\n        try:\n            # Attempt to get vocabulary size from tokenizer.\n            vocab = tokenizer.get_vocab()  # Assumes the tokenizer provides get_vocab()\n            LOGGER.info(f\"Tokenizer loaded with vocabulary size: {len(vocab)}. (Expected target: {vocab_size})\")\n        except Exception as e:\n            LOGGER.warning(f\"Unable to obtain vocabulary from tokenizer: {str(e)}\")\n\n        tokenized_pairs: List[Tuple[List[int], List[int]]] = []\n        num_sentences: int = min(len(src_lines), len(tgt_lines))\n        for idx in range(num_sentences):\n            src_sentence: str = src_lines[idx]\n            tgt_sentence: str = tgt_lines[idx]\n            src_tokens: List[int] = encode_text(src_sentence, tokenizer)\n            tgt_tokens: List[int] = encode_text(tgt_sentence, tokenizer)\n            tokenized_pairs.append((src_tokens, tgt_tokens))\n            if idx < 3:  # Log first few examples for debugging.\n                LOGGER.debug(f\"Example {idx}: src tokens ({len(src_tokens)} tokens), tgt tokens ({len(tgt_tokens)} tokens).\")\n        LOGGER.info(f\"Tokenized {len(tokenized_pairs)} translation sentence pairs.\")\n        return tokenized_pairs\n\n    def _tokenize_parsing(self, lines: List[str]) -> List[List[int]]:\n        \"\"\"\n        Tokenizes a list of sentences for constituency parsing experiments.\n        For parsing, a default tokenizer (e.g., BPE) is used.\n\n        Args:\n            lines (List[str]): List of raw sentences.\n\n        Returns:\n            List[List[int]]: List of tokenized sentences (each sentence is a list of token IDs).\n        \"\"\"\n        # For parsing, we assume default tokenization using BPE.\n        tokenizer = load_tokenizer(\"BPE\")\n        try:\n            vocab = tokenizer.get_vocab()\n            LOGGER.info(f\"Parsing tokenizer loaded with vocabulary size: {len(vocab)}.\")\n        except Exception as e:\n            LOGGER.warning(f\"Unable to obtain vocabulary for parsing tokenizer: {str(e)}\")\n\n        tokenized_sentences: List[List[int]] = []\n        for idx, sentence in enumerate(lines):\n            tokens: List[int] = encode_text(sentence, tokenizer)\n            tokenized_sentences.append(tokens)\n            if idx < 3:\n                LOGGER.debug(f\"Parsing example {idx}: {len(tokens)} tokens.\")\n        LOGGER.info(f\"Tokenized {len(tokenized_sentences)} parsing sentences.\")\n        return tokenized_sentences\n\n    def _split_data(self, examples: List[Any]) -> Tuple[List[Any], List[Any], List[Any]]:\n        \"\"\"\n        Splits the list of examples into training, development, and test sets using a simple\n        80/10/10 ratio.\n\n        Args:\n            examples (List[Any]): List of tokenized examples (either sentence pairs or single sentences).\n\n        Returns:\n            Tuple[List[Any], List[Any], List[Any]]: (train_set, dev_set, test_set)\n        \"\"\"\n        total: int = len(examples)\n        train_end: int = int(total * 0.8)\n        dev_end: int = train_end + int(total * 0.1)\n        train_set: List[Any] = examples[:train_end]\n        dev_set: List[Any] = examples[train_end:dev_end]\n        test_set: List[Any] = examples[dev_end:]\n        LOGGER.info(f\"Data split into {len(train_set)} training, {len(dev_set)} dev, and {len(test_set)} test examples.\")\n        return train_set, dev_set, test_set\n\n    def _create_batches(self, examples: List[Any], is_pair: bool = True) -> List[Dict[str, Any]]:\n        \"\"\"\n        Groups tokenized examples into batches based on the approximate token count defined in\n        the configuration. For translation tasks (is_pair=True), both source and target tokens are\n        considered; for parsing (is_pair=False), only one sequence per example is used.\n\n        Args:\n            examples (List[Any]): List of tokenized examples. For translation, each example is a tuple\n                                  (src_tokens, tgt_tokens). For parsing, each example is a list of token IDs.\n            is_pair (bool): Indicates if examples contain paired sequences (True for translation). Defaults to True.\n\n        Returns:\n            List[Dict[str, Any]]: List of batch dictionaries containing the tokenized sequences.\n                                  For translation: {\"src\": List[List[int]], \"tgt\": List[List[int]]}.\n                                  For parsing: {\"inputs\": List[List[int]]}.\n        \"\"\"\n        batches: List[Dict[str, Any]] = []\n        if is_pair:\n            current_batch: Dict[str, List[List[int]]] = {\"src\": [], \"tgt\": []}\n            current_src_count: int = 0\n            current_tgt_count: int = 0\n            for src_tokens, tgt_tokens in examples:\n                src_len: int = len(src_tokens)\n                tgt_len: int = len(tgt_tokens)\n                # If adding this example exceeds the batch token limit for either side, start a new batch.\n                if (current_src_count + src_len > self.batch_tokens) or (current_tgt_count + tgt_len > self.batch_tokens):\n                    if current_batch[\"src\"]:\n                        batches.append(current_batch)\n                    current_batch = {\"src\": [], \"tgt\": []}\n                    current_src_count = 0\n                    current_tgt_count = 0\n                current_batch[\"src\"].append(src_tokens)\n                current_batch[\"tgt\"].append(tgt_tokens)\n                current_src_count += src_len\n                current_tgt_count += tgt_len\n            if current_batch[\"src\"]:\n                batches.append(current_batch)\n            LOGGER.info(f\"Created {len(batches)} batches for translation data.\")\n        else:\n            # For parsing tasks.\n            current_batch: Dict[str, List[List[int]]] = {\"inputs\": []}\n            current_token_count: int = 0\n            for tokens in examples:\n                token_len: int = len(tokens)\n                if current_token_count + token_len > self.batch_tokens:\n                    if current_batch[\"inputs\"]:\n                        batches.append(current_batch)\n                    current_batch = {\"inputs\": []}\n                    current_token_count = 0\n                current_batch[\"inputs\"].append(tokens)\n                current_token_count += token_len\n            if current_batch[\"inputs\"]:\n                batches.append(current_batch)\n            LOGGER.info(f\"Created {len(batches)} batches for parsing data.\")\n        return batches\n\n    def load_data(self) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n        \"\"\"\n        Loads raw text data from files, tokenizes the data, splits it into training, development,\n        and test sets, and constructs batches based on token count.\n\n        Returns:\n            Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n                (train_batches, dev_batches, test_batches)\n        \"\"\"\n        if self.task == \"translation\":\n            # Read source and target files.\n            src_lines: List[str] = self._read_lines(self.src_file)\n            tgt_lines: List[str] = self._read_lines(self.tgt_file)\n            if not src_lines or not tgt_lines:\n                LOGGER.error(\"Translation data files are empty or could not be read.\")\n                return [], [], []\n            if len(src_lines) != len(tgt_lines):\n                LOGGER.warning(\n                    f\"Source and target file line counts differ: {len(src_lines)} vs. {len(tgt_lines)}. \"\n                    \"Proceeding with minimum count.\"\n                )\n            tokenized_examples: List[Tuple[List[int], List[int]]] = self._tokenize_translation(src_lines, tgt_lines)\n            train_examples, dev_examples, test_examples = self._split_data(tokenized_examples)\n            train_batches: List[Dict[str, Any]] = self._create_batches(train_examples, is_pair=True)\n            dev_batches: List[Dict[str, Any]] = self._create_batches(dev_examples, is_pair=True)\n            test_batches: List[Dict[str, Any]] = self._create_batches(test_examples, is_pair=True)\n            LOGGER.info(\"Translation data loaded and batched successfully.\")\n            return train_batches, dev_batches, test_batches\n\n        elif self.task == \"parsing\":\n            # Read WSJ file.\n            wsj_lines: List[str] = self._read_lines(self.wsj_file)\n            if not wsj_lines:\n                LOGGER.error(\"WSJ parsing data file is empty or could not be read.\")\n                return [], [], []\n            # Note: Here we simply tokenize sentences. In practice, tree structures should be\n            # linearized (e.g., using a bracket-based representation). This implementation uses raw sentences.\n            tokenized_examples: List[List[int]] = self._tokenize_parsing(wsj_lines)\n            train_examples, dev_examples, test_examples = self._split_data(tokenized_examples)\n            train_batches: List[Dict[str, Any]] = self._create_batches(train_examples, is_pair=False)\n            dev_batches: List[Dict[str, Any]] = self._create_batches(dev_examples, is_pair=False)\n            test_batches: List[Dict[str, Any]] = self._create_batches(test_examples, is_pair=False)\n            LOGGER.info(\"Parsing data loaded and batched successfully.\")\n            return train_batches, dev_batches, test_batches\n\n        else:\n            LOGGER.error(f\"Unsupported task type during data loading: {self.task}\")\n            return [], [], []\n\n\n# For debugging and testing purposes.\nif __name__ == \"__main__\":\n    # Create a DatasetLoader for translation and print dataset statistics.\n    loader_translation = DatasetLoader(CONFIG, task=\"translation\")\n    train_data, dev_data, test_data = loader_translation.load_data()\n    LOGGER.info(f\"Translation: {len(train_data)} training batches, {len(dev_data)} dev batches, {len(test_data)} test batches.\")\n\n    # Create a DatasetLoader for parsing and print dataset statistics.\n    loader_parsing = DatasetLoader(CONFIG, task=\"parsing\")\n    p_train, p_dev, p_test = loader_parsing.load_data()\n    LOGGER.info(f\"Parsing: {len(p_train)} training batches, {len(p_dev)} dev batches, {len(p_test)} test batches.\")\n\n```\n\n```python\n## File name: model.py\n\"\"\"model.py\n\nThis module implements the Transformer model architecture as described in \n\"Attention Is All You Need\". It defines all necessary submodules including \nPositionalEncoding, MultiHeadAttention, PositionWiseFeedForward, EncoderLayer, \nDecoderLayer, Encoder, Decoder, and the main TransformerModel class. The model \nsupports both encoder and decoder stacks, shared token embeddings, and integrates \na training step with label smoothing.\n\nAll configuration values are read from the centralized config (config.yaml via config.py).\n\nAuthor: [Your Name]\nDate: [Current Date]\n\"\"\"\n\nimport math\nfrom typing import Dict, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom config import CONFIG\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Injects absolute positional information into token embeddings using sinusoidal functions.\"\"\"\n\n    def __init__(self, d_model: int, dropout_rate: float, max_len: int = 5000) -> None:\n        \"\"\"\n        Args:\n            d_model: Dimensionality of token embeddings.\n            dropout_rate: Dropout rate to apply after adding the positional encoding.\n            max_len: Maximum input sequence length.\n        \"\"\"\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: Input embeddings of shape (batch_size, seq_len, d_model).\n        Returns:\n            Tensor of the same shape with positional encodings added.\n        \"\"\"\n        x = x + self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Implements multi-head scaled dot-product attention.\"\"\"\n\n    def __init__(self, d_model: int, num_heads: int, d_k: int, d_v: int, dropout_rate: float) -> None:\n        \"\"\"\n        Args:\n            d_model: Dimensionality of input embeddings.\n            num_heads: Number of attention heads.\n            d_k: Dimensionality of queries and keys per head.\n            d_v: Dimensionality of values per head.\n            dropout_rate: Dropout rate to apply to attention weights.\n        \"\"\"\n        super().__init__()\n        self.num_heads = num_heads\n        self.d_k = d_k\n        self.d_v = d_v\n        self.linear_q = nn.Linear(d_model, num_heads * d_k)\n        self.linear_k = nn.Linear(d_model, num_heads * d_k)\n        self.linear_v = nn.Linear(d_model, num_heads * d_v)\n        self.fc_out = nn.Linear(num_heads * d_v, d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        mask: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            query: Tensor of shape (batch_size, query_len, d_model).\n            key: Tensor of shape (batch_size, key_len, d_model).\n            value: Tensor of shape (batch_size, value_len, d_model).\n            mask: Optional mask tensor broadcastable to (batch_size, num_heads, query_len, key_len).\n        Returns:\n            Output tensor of shape (batch_size, query_len, d_model).\n        \"\"\"\n        batch_size = query.size(0)\n\n        # Linear projections and reshape for multi-head attention.\n        Q = self.linear_q(query)  # (batch_size, query_len, num_heads * d_k)\n        K = self.linear_k(key)      # (batch_size, key_len, num_heads * d_k)\n        V = self.linear_v(value)    # (batch_size, value_len, num_heads * d_v)\n\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, query_len, d_k)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, key_len, d_k)\n        V = V.view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)  # (batch_size, num_heads, value_len, d_v)\n\n        # Scaled dot-product attention.\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch_size, num_heads, query_len, key_len)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        attn_output = torch.matmul(attn_weights, V)  # (batch_size, num_heads, query_len, d_v)\n\n        # Concatenate attention heads.\n        attn_output = attn_output.transpose(1, 2).contiguous()  # (batch_size, query_len, num_heads, d_v)\n        attn_output = attn_output.view(batch_size, -1, self.num_heads * self.d_v)  # (batch_size, query_len, d_model)\n        output = self.fc_out(attn_output)  # (batch_size, query_len, d_model)\n        return output\n\n\nclass PositionWiseFeedForward(nn.Module):\n    \"\"\"Implements a two-layer feed-forward network applied position-wise.\"\"\"\n\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float) -> None:\n        \"\"\"\n        Args:\n            d_model: Input and output dimensionality.\n            d_ff: Dimensionality of the inner feed-forward layer.\n            dropout_rate: Dropout rate to apply.\n        \"\"\"\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: Tensor of shape (batch_size, seq_len, d_model).\n        Returns:\n            Tensor of the same shape with the feed-forward transformation applied.\n        \"\"\"\n        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n\n\nclass EncoderLayer(nn.Module):\n    \"\"\"Constructs an encoder layer with self-attention and feed-forward sub-layers.\"\"\"\n\n    def __init__(\n        self, d_model: int, num_heads: int, d_ff: int, d_k: int, d_v: int, dropout_rate: float\n    ) -> None:\n        \"\"\"\n        Args:\n            d_model: Model dimensionality.\n            num_heads: Number of attention heads.\n            d_ff: Feed-forward inner layer dimensionality.\n            d_k: Dimensionality of queries/keys per head.\n            d_v: Dimensionality of values per head.\n            dropout_rate: Dropout rate to apply.\n        \"\"\"\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, d_k, d_v, dropout_rate)\n        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout_rate)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, seq_len, d_model).\n            mask: Optional mask for self-attention.\n        Returns:\n            Output tensor of shape (batch_size, seq_len, d_model).\n        \"\"\"\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\n\nclass DecoderLayer(nn.Module):\n    \"\"\"Constructs a decoder layer with masked self-attention, encoder-decoder attention, and feed-forward sub-layers.\"\"\"\n\n    def __init__(\n        self, d_model: int, num_heads: int, d_ff: int, d_k: int, d_v: int, dropout_rate: float\n    ) -> None:\n        \"\"\"\n        Args:\n            d_model: Model dimensionality.\n            num_heads: Number of attention heads.\n            d_ff: Feed-forward inner layer dimensionality.\n            d_k: Dimensionality of queries/keys per head.\n            d_v: Dimensionality of values per head.\n            dropout_rate: Dropout rate to apply.\n        \"\"\"\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, d_k, d_v, dropout_rate)\n        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, d_k, d_v, dropout_rate)\n        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout_rate)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        enc_output: torch.Tensor,\n        tgt_mask: Optional[torch.Tensor] = None,\n        src_mask: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: Target embeddings of shape (batch_size, tgt_seq_len, d_model).\n            enc_output: Encoder output of shape (batch_size, src_seq_len, d_model).\n            tgt_mask: Mask for target self-attention (causal mask).\n            src_mask: Mask for encoder-decoder attention.\n        Returns:\n            Tensor of shape (batch_size, tgt_seq_len, d_model).\n        \"\"\"\n        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(self_attn_output))\n        enc_dec_attn_output = self.enc_dec_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(enc_dec_attn_output))\n        ffn_output = self.ffn(x)\n        x = self.norm3(x + self.dropout(ffn_output))\n        return x\n\n\nclass Encoder(nn.Module):\n    \"\"\"Stack of encoder layers that processes source embeddings into continuous representations.\"\"\"\n\n    def __init__(\n        self, num_layers: int, d_model: int, num_heads: int, d_ff: int, d_k: int, d_v: int, dropout_rate: float\n    ) -> None:\n        \"\"\"\n        Args:\n            num_layers: Number of encoder layers.\n            d_model: Model dimensionality.\n            num_heads: Number of attention heads.\n            d_ff: Feed-forward inner layer dimensionality.\n            d_k: Dimensionality per attention head for queries/keys.\n            d_v: Dimensionality per attention head for values.\n            dropout_rate: Dropout rate.\n        \"\"\"\n        super().__init__()\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, d_k, d_v, dropout_rate)\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: Input tensor of shape (batch_size, src_seq_len, d_model).\n            mask: Optional mask tensor for self-attention.\n        Returns:\n            Output tensor of the same shape.\n        \"\"\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\n\nclass Decoder(nn.Module):\n    \"\"\"Stack of decoder layers for auto-regressive target prediction with encoder context.\"\"\"\n\n    def __init__(\n        self, num_layers: int, d_model: int, num_heads: int, d_ff: int, d_k: int, d_v: int, dropout_rate: float\n    ) -> None:\n        \"\"\"\n        Args:\n            num_layers: Number of decoder layers.\n            d_model: Model dimensionality.\n            num_heads: Number of attention heads.\n            d_ff: Feed-forward inner layer dimensionality.\n            d_k: Dimensionality per attention head for queries/keys.\n            d_v: Dimensionality per attention head for values.\n            dropout_rate: Dropout rate.\n        \"\"\"\n        super().__init__()\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, d_k, d_v, dropout_rate)\n            for _ in range(num_layers)\n        ])\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        enc_output: torch.Tensor,\n        tgt_mask: Optional[torch.Tensor] = None,\n        src_mask: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: Target embeddings of shape (batch_size, tgt_seq_len, d_model).\n            enc_output: Encoder output of shape (batch_size, src_seq_len, d_model).\n            tgt_mask: Target mask for self-attention.\n            src_mask: Source mask for encoder-decoder attention.\n        Returns:\n            Tensor of shape (batch_size, tgt_seq_len, d_model).\n        \"\"\"\n        for layer in self.layers:\n            x = layer(x, enc_output, tgt_mask, src_mask)\n        return x\n\n\nclass TransformerModel(nn.Module):\n    \"\"\"\n    The TransformerModel class encapsulates the full encoder-decoder architecture.\n    It applies shared token embeddings (if configured), adds positional encoding, and\n    passes inputs through stacked encoder and decoder layers. It also provides a train_step\n    method that performs a full training iteration with label smoothing.\n    \"\"\"\n\n    def __init__(self, config: object) -> None:\n        \"\"\"\n        Args:\n            config: Configuration object loaded from config.yaml via config.py.\n        \"\"\"\n        super().__init__()\n        self.config = config\n        self.d_model = config.model.d_model\n        self.num_layers = config.model.num_layers\n        self.d_ff = config.model.d_ff\n        self.num_heads = config.model.num_heads\n        self.d_k = config.model.d_k\n        self.d_v = config.model.d_v\n        self.dropout_rate = config.training.dropout_rate\n        self.pos_enc_type = config.model.positional_encoding\n        self.share_embedding = config.model.share_embedding\n\n        # Determine vocabulary size based on model type.\n        if config.model.type == \"transformer_base\":\n            self.vocab_size = config.data.wmt_2014_en_de.vocab_size\n        elif config.model.type == \"transformer_parsing\":\n            self.vocab_size = config.parsing.vocab_size.wsj\n        else:\n            self.vocab_size = config.data.wmt_2014_en_de.vocab_size\n\n        # Token embedding layer.\n        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n        nn.init.xavier_uniform_(self.embedding.weight)\n\n        # Positional Encoding (currently only sinusoidal is implemented).\n        if self.pos_enc_type == \"sinusoidal\":\n            self.positional_encoding = PositionalEncoding(self.d_model, self.dropout_rate)\n        else:\n            self.positional_encoding = PositionalEncoding(self.d_model, self.dropout_rate)\n\n        # Stacked Encoder and Decoder.\n        self.encoder = Encoder(self.num_layers, self.d_model, self.num_heads, self.d_ff, self.d_k, self.d_v, self.dropout_rate)\n        self.decoder = Decoder(self.num_layers, self.d_model, self.num_heads, self.d_ff, self.d_k, self.d_v, self.dropout_rate)\n\n        # Final linear layer projecting to vocabulary logits.\n        self.final_linear = nn.Linear(self.d_model, self.vocab_size)\n\n        # Tie weights if configured.\n        if self.share_embedding:\n            self.final_linear.weight = self.embedding.weight\n\n        self.dropout = nn.Dropout(self.dropout_rate)\n\n    def forward(\n        self,\n        src: torch.Tensor,\n        tgt: torch.Tensor,\n        src_mask: Optional[torch.Tensor] = None,\n        tgt_mask: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the Transformer model.\n        \n        Args:\n            src: Source token tensor of shape (batch_size, src_seq_len).\n            tgt: Target token tensor of shape (batch_size, tgt_seq_len).\n            src_mask: Optional source mask tensor.\n            tgt_mask: Optional target mask tensor (used for causal masking in decoder).\n        Returns:\n            Logits tensor of shape (batch_size, tgt_seq_len, vocab_size).\n        \"\"\"\n        batch_size, src_seq_len = src.size()\n        batch_size_t, tgt_seq_len = tgt.size()\n        assert batch_size == batch_size_t, \"Batch size of source and target must match.\"\n\n        # Embed and scale source tokens; add positional encoding.\n        src_embeddings = self.embedding(src) * math.sqrt(self.d_model)\n        src_embeddings = self.positional_encoding(src_embeddings)\n        src_embeddings = self.dropout(src_embeddings)\n\n        # Pass through encoder stack.\n        enc_output = self.encoder(src_embeddings, src_mask)\n\n        # Embed and scale target tokens; add positional encoding.\n        tgt_embeddings = self.embedding(tgt) * math.sqrt(self.d_model)\n        tgt_embeddings = self.positional_encoding(tgt_embeddings)\n        tgt_embeddings = self.dropout(tgt_embeddings)\n\n        # Pass through decoder stack.\n        dec_output = self.decoder(tgt_embeddings, enc_output, tgt_mask, src_mask)\n\n        # Final projection to vocabulary logits.\n        logits = self.final_linear(dec_output)\n        return logits\n\n    def _generate_subsequent_mask(self, size: int) -> torch.Tensor:\n        \"\"\"\n        Generates a causal mask to prevent attention to future tokens.\n\n        Args:\n            size: Length of the target sequence.\n        Returns:\n            A mask tensor of shape (1, size, size) with ones in allowed positions.\n        \"\"\"\n        mask = torch.tril(torch.ones((size, size), dtype=torch.uint8))\n        return mask.unsqueeze(0)\n\n    def train_step(self, batch: Dict[str, torch.Tensor], label_smoothing: float = 0.1) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Executes a single training iteration: computes logits and loss with label smoothing.\n\n        Args:\n            batch: Dictionary containing \"src\" and \"tgt\" tensors.\n                   \"src\": (batch_size, src_seq_len),\n                   \"tgt\": (batch_size, tgt_seq_len) where tgt tokens include start tokens.\n            label_smoothing: Label smoothing factor (default 0.1).\n        Returns:\n            A dictionary with keys:\n              \"loss\": Scalar loss tensor.\n              \"logits\": Logits tensor from the forward pass.\n        \"\"\"\n        src = batch[\"src\"]\n        tgt = batch[\"tgt\"]\n\n        tgt_seq_len = tgt.size(1)\n        tgt_mask = self._generate_subsequent_mask(tgt_seq_len).to(tgt.device)\n\n        logits = self.forward(src, tgt, src_mask=None, tgt_mask=tgt_mask)\n\n        # Shift logits and target for loss computation.\n        logits = logits[:, :-1, :]  # (batch_size, tgt_seq_len - 1, vocab_size)\n        target = tgt[:, 1:]         # (batch_size, tgt_seq_len - 1)\n\n        loss = self._label_smoothing_loss(logits, target, label_smoothing)\n        return {\"loss\": loss, \"logits\": logits}\n\n    def _label_smoothing_loss(self, logits: torch.Tensor, target: torch.Tensor, smoothing: float) -> torch.Tensor:\n        \"\"\"\n        Computes the label-smoothed cross entropy loss.\n\n        Args:\n            logits: Logits tensor of shape (batch_size, seq_len, vocab_size).\n            target: Ground truth indices of shape (batch_size, seq_len).\n            smoothing: Label smoothing factor.\n        Returns:\n            Scalar loss tensor.\n        \"\"\"\n        batch_size, seq_len, vocab_size = logits.size()\n        logits = logits.view(-1, vocab_size)  # (batch_size*seq_len, vocab_size)\n        target = target.view(-1)              # (batch_size*seq_len)\n\n        log_probs = torch.log_softmax(logits, dim=-1)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_probs)\n            true_dist.fill_(smoothing / (vocab_size - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - smoothing)\n\n        loss = torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n        return loss\n\n```\n\n```python\n## File name: trainer.py\n\"\"\"trainer.py\n\nThis module defines the Trainer class, which manages the training loop for the Transformer model.\nIt uses the Adam optimizer with a custom learning rate scheduler (including warmup and inverse square-root decay),\napplies gradient clipping, logs training metrics, and periodically saves checkpoints.\n\nAll hyperparameters are read from the centralized configuration (config.yaml via config.py).\nThe Trainer class is used by the main experiment driver (main.py) to launch training.\n\"\"\"\n\nimport time\nimport math\nimport itertools\nfrom typing import Any, Dict, List\n\nimport torch\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tqdm import tqdm\n\nfrom config import CONFIG  # Global configuration (instance of Config)\nfrom model import TransformerModel\nfrom utils import LOGGER, save_checkpoint, log_metrics, create_dir_if_not_exists\n\n\nclass Trainer:\n    \"\"\"\n    Trainer handles the training loop of the Transformer model.\n    It receives the model, training data, development data, and configuration parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: TransformerModel,\n        train_data: List[Dict[str, Any]],\n        dev_data: List[Dict[str, Any]],\n        config: Any = CONFIG,\n    ) -> None:\n        \"\"\"\n        Initialize the Trainer.\n\n        Args:\n            model (TransformerModel): An instantiated Transformer model.\n            train_data (List[Dict[str, Any]]): Training dataset batches.\n            dev_data (List[Dict[str, Any]]): Development/validation dataset batches.\n            config (Any): Configuration object loaded from config.yaml via config.py.\n        \"\"\"\n        self.model: TransformerModel = model\n        self.train_data: List[Dict[str, Any]] = train_data\n        self.dev_data: List[Dict[str, Any]] = dev_data\n        self.config = config\n\n        # Set device: use GPU if available.\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n\n        # Set up the Adam optimizer with hyperparameters from configuration.\n        initial_lr: float = self.get_learning_rate(1)\n        self.optimizer = optim.Adam(\n            self.model.parameters(),\n            lr=initial_lr,\n            betas=(self.config.training.adam.beta1, self.config.training.adam.beta2),\n            eps=self.config.training.adam.epsilon,\n        )\n\n        # Training hyperparameters from configuration.\n        self.total_steps: int = self.config.training.total_steps\n        self.warmup_steps: int = self.config.training.warmup_steps\n        self.d_model: int = self.config.model.d_model\n\n        # Global training step counter starting at 1.\n        self.current_step: int = 1\n\n        # Set checkpoint saving parameters.\n        self.checkpoint_interval: int = 10000  # Save checkpoint every 10000 steps.\n        self.checkpoint_dir: str = \"checkpoints\"\n        create_dir_if_not_exists(self.checkpoint_dir)\n\n        # Create a cyclic iterator for training batches.\n        self.train_iter = itertools.cycle(self.train_data)\n\n        # Set gradient clipping norm (optional safeguard).\n        self.grad_clip: float = 1.0\n\n        # Define logging interval (in steps).\n        self.log_interval: int = 100\n\n        LOGGER.info(\n            f\"Trainer initialized on device {self.device}. Total training steps: {self.total_steps}.\"\n        )\n\n    def get_learning_rate(self, step: int) -> float:\n        \"\"\"\n        Compute the learning rate based on the current training step using the formula:\n            lr = (d_model)^(-0.5) * min(step^(-0.5), step * (warmup_steps)^(-1.5))\n\n        Args:\n            step (int): Current training step.\n\n        Returns:\n            float: The computed learning rate.\n        \"\"\"\n        lr = (self.d_model ** -0.5) * min(\n            step ** -0.5, step * (self.warmup_steps ** -1.5)\n        )\n        return lr\n\n    def save_checkpoint(self) -> None:\n        \"\"\"\n        Saves the current state of the model, optimizer, and training step to a checkpoint file.\n        Uses the save_checkpoint helper function from utils.py.\n        \"\"\"\n        save_checkpoint(\n            self.model, self.optimizer, self.current_step, self.checkpoint_dir\n        )\n\n    def pad_batch(self, sequences: List[List[int]]) -> torch.Tensor:\n        \"\"\"\n        Pads a batch of sequences to have uniform length using zero as the padding value.\n\n        Args:\n            sequences (List[List[int]]): List of sequences (each sequence is a list of token IDs).\n\n        Returns:\n            torch.Tensor: Padded tensor of shape (batch_size, max_seq_len).\n        \"\"\"\n        tensor_seqs = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n        padded_batch = pad_sequence(tensor_seqs, batch_first=True, padding_value=0)\n        return padded_batch\n\n    def train(self) -> None:\n        \"\"\"\n        Runs the complete training loop until the global training step reaches total_steps.\n\n        For each training step:\n            - Retrieves a batch from the training data.\n            - Moves the batch to the appropriate device.\n            - Performs a forward pass and computes label-smoothed loss via model.train_step().\n            - Backpropagates the loss and updates model parameters.\n            - Dynamically updates the learning rate according to the scheduling formula.\n            - Logs training metrics periodically.\n            - Saves checkpoints at predefined intervals.\n        \"\"\"\n        self.model.train()  # Enable training mode.\n        start_time = time.time()\n        progress_bar = tqdm(total=self.total_steps, desc=\"Training Steps\", unit=\"step\")\n\n        while self.current_step <= self.total_steps:\n            # Retrieve next batch from cyclic iterator.\n            batch = next(self.train_iter)\n\n            # Convert batch lists to padded tensors and move to device.\n            # For translation tasks, expect keys \"src\" and \"tgt\".\n            # For parsing tasks, expect key \"inputs\".\n            if \"src\" in batch and \"tgt\" in batch:\n                src_tensor = self.pad_batch(batch[\"src\"]).to(self.device)\n                tgt_tensor = self.pad_batch(batch[\"tgt\"]).to(self.device)\n                batch = {\"src\": src_tensor, \"tgt\": tgt_tensor}\n            elif \"inputs\" in batch:\n                inputs_tensor = self.pad_batch(batch[\"inputs\"]).to(self.device)\n                batch = {\"inputs\": inputs_tensor}\n            else:\n                LOGGER.error(\n                    \"Batch format not recognized. Expected keys 'src'/'tgt' or 'inputs'.\"\n                )\n                continue\n\n            # Forward pass: compute logits and loss with label smoothing.\n            output: Dict[str, torch.Tensor] = self.model.train_step(batch)\n            loss: torch.Tensor = output.get(\"loss\")\n            if loss is None:\n                LOGGER.error(\"Loss not computed during training step.\")\n                continue\n\n            # Backpropagation.\n            self.optimizer.zero_grad()\n            loss.backward()\n\n            # Apply gradient clipping.\n            clip_grad_norm_(self.model.parameters(), self.grad_clip)\n\n            # Optimizer step.\n            self.optimizer.step()\n\n            # Update learning rate according to schedule.\n            current_lr: float = self.get_learning_rate(self.current_step)\n            for param_group in self.optimizer.param_groups:\n                param_group[\"lr\"] = current_lr\n\n            # Logging: log every self.log_interval steps.\n            if self.current_step % self.log_interval == 0:\n                elapsed_time = time.time() - start_time\n                metrics = {\n                    \"loss\": loss.item(),\n                    \"lr\": current_lr,\n                    \"step\": self.current_step,\n                    \"elapsed_time_sec\": elapsed_time,\n                }\n                log_metrics(self.current_step, metrics)\n\n            # Checkpointing: save checkpoint every checkpoint_interval steps.\n            if self.current_step % self.checkpoint_interval == 0:\n                self.save_checkpoint()\n\n            self.current_step += 1\n            progress_bar.update(1)\n\n        progress_bar.close()\n        LOGGER.info(\"Training completed.\")\n\n```\n\n```python\n## File name: evaluation.py\n\"\"\"evaluation.py\n\nThis module implements the Evaluation class for assessing the performance of the trained \nTransformer model. It provides two primary evaluation methods:\n  \u2022 evaluate_mt() for computing BLEU scores on machine translation tasks.\n  \u2022 evaluate_parsing() for computing F1 (and associated precision and recall) scores \n    on constituency parsing tasks.\n\nThe evaluation routines use beam search decoding with parameters (beam size, length penalty,\nand maximum output length offset) specified in the central configuration (config.yaml). \nToken decoding and other utility functions are imported from utils.py. For constituency parsing,\nlinearized tree outputs are converted to bracketed tree structures and evaluated by comparing \ntheir constituent spans.\n\nAll configuration values and hyperparameters are read from the central config.yaml file \n(via config.py). Default values are set where necessary.\n\"\"\"\n\nimport math\nimport torch\nimport torch.nn.functional as F\nfrom typing import Any, Dict, List, Set, Tuple\nfrom tqdm import tqdm\n\n# Import configuration and utilities.\nfrom config import CONFIG\nfrom model import TransformerModel\nfrom utils import LOGGER, decode_tokens, load_tokenizer\n\n# ---------------------------------------------------------------------------\n# Helper Functions for Beam Search Decoding and Metric Computation\n# ---------------------------------------------------------------------------\n\ndef _beam_search_decode(\n    model: TransformerModel,\n    src: torch.Tensor,\n    device: torch.device,\n    beam_size: int,\n    length_penalty: float,\n    max_output_length: int,\n    start_token: int = 1,\n    end_token: int = 2,\n) -> List[int]:\n    \"\"\"\n    Performs beam search decoding for a single source instance.\n    \n    Args:\n        model: Trained TransformerModel instance.\n        src: Source tensor of shape (1, src_seq_len).\n        device: Torch device.\n        beam_size: Beam search width.\n        length_penalty: Exponent for length penalty.\n        max_output_length: Maximum target sequence length (source length + offset).\n        start_token: Token ID for begin-of-sentence (default: 1).\n        end_token: Token ID for end-of-sentence (default: 2).\n    \n    Returns:\n        A list of token IDs representing the best decoded sequence.\n    \"\"\"\n    # Each beam candidate is a tuple: (sequence, cumulative_log_prob)\n    beam: List[Tuple[List[int], float]] = [([start_token], 0.0)]\n    completed_candidates: List[Tuple[List[int], float]] = []\n    \n    # Set model to evaluation and disable gradient computation.\n    model.eval()\n    with torch.no_grad():\n        for _ in range(max_output_length):\n            new_beam: List[Tuple[List[int], float]] = []\n            for seq, cum_log_prob in beam:\n                # If last token is end_token, add candidate to new beam unchanged.\n                if seq[-1] == end_token:\n                    new_beam.append((seq, cum_log_prob))\n                    continue\n\n                # Prepare target tensor for current candidate.\n                tgt_tensor = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(0)  # (1, seq_len)\n                # Generate causal mask for target.\n                tgt_len = tgt_tensor.size(1)\n                # The model has a helper _generate_subsequent_mask, so we call it.\n                tgt_mask = model._generate_subsequent_mask(tgt_len).to(device)\n                \n                # Forward pass: compute logits. Note: We only need the last time-step.\n                logits = model(src, tgt_tensor, src_mask=None, tgt_mask=tgt_mask)  # (1, seq_len, vocab_size)\n                logits_last = logits[0, -1, :]  # (vocab_size)\n                log_probs = F.log_softmax(logits_last, dim=-1)  # (vocab_size)\n                \n                # Get top beam_size candidates.\n                topk_log_probs, topk_indices = torch.topk(log_probs, beam_size)\n                topk_log_probs = topk_log_probs.cpu().tolist()\n                topk_indices = topk_indices.cpu().tolist()\n\n                for token_id, token_log_prob in zip(topk_indices, topk_log_probs):\n                    new_seq = seq + [token_id]\n                    new_log_prob = cum_log_prob + token_log_prob\n                    new_beam.append((new_seq, new_log_prob))\n            \n            # If no candidates found, break.\n            if not new_beam:\n                break\n\n            # Apply length penalty and select top beam_size candidates.\n            def score_fn(candidate: Tuple[List[int], float]) -> float:\n                seq, log_prob = candidate\n                # Length penalty: (len(seq))**length_penalty\n                return log_prob / (len(seq) ** length_penalty)\n            \n            new_beam = sorted(new_beam, key=score_fn, reverse=True)[:beam_size]\n            beam = new_beam\n\n            # Check if all candidates have ended.\n            if all(seq[-1] == end_token for seq, _ in beam):\n                break\n        \n        # Select the candidate with the highest score from the final beam.\n        best_candidate = max(beam, key=score_fn)[0]\n    \n    return best_candidate\n\ndef compute_corpus_bleu(references: List[str], hypotheses: List[str]) -> float:\n    \"\"\"\n    Computes a corpus-level BLEU score for machine translation.\n    This is a simplified BLEU implementation using up to 4-gram matching.\n    \n    Args:\n        references: List of reference sentences (strings).\n        hypotheses: List of hypothesis sentences (strings).\n    \n    Returns:\n        BLEU score as a float.\n    \"\"\"\n    import math\n\n    def ngram_counts(tokens: List[str], n: int) -> Dict[Tuple[str, ...], int]:\n        counts = {}\n        for i in range(len(tokens) - n + 1):\n            ngram = tuple(tokens[i:i+n])\n            counts[ngram] = counts.get(ngram, 0) + 1\n        return counts\n    \n    total_matches = {n: 0 for n in range(1, 5)}\n    total_ngrams = {n: 0 for n in range(1, 5)}\n    ref_length = 0\n    hyp_length = 0\n    \n    for ref_sentence, hyp_sentence in zip(references, hypotheses):\n        ref_tokens = ref_sentence.split()\n        hyp_tokens = hyp_sentence.split()\n        ref_length += len(ref_tokens)\n        hyp_length += len(hyp_tokens)\n        for n in range(1, 5):\n            ref_counts = ngram_counts(ref_tokens, n)\n            hyp_counts = ngram_counts(hyp_tokens, n)\n            total_ngrams[n] += max(len(hyp_tokens) - n + 1, 0)\n            for ngram in hyp_counts:\n                total_matches[n] += min(hyp_counts[ngram], ref_counts.get(ngram, 0))\n    \n    precisions = []\n    for n in range(1, 5):\n        if total_ngrams[n] == 0:\n            precisions.append(0)\n        else:\n            precisions.append(total_matches[n] / total_ngrams[n])\n    \n    # Avoid log(0)\n    if min(precisions) == 0:\n        geo_mean = 0\n    else:\n        geo_mean = math.exp(sum(math.log(p) for p in precisions) / 4)\n    \n    # Brevity penalty\n    bp = 1.0 if hyp_length > ref_length else math.exp(1 - ref_length / hyp_length) if hyp_length > 0 else 0.0\n    bleu = bp * geo_mean\n    return bleu\n\ndef _parse_tree(tokens: List[str], index: int, word_index: int) -> Tuple[Set[Tuple[int, int]], int, int]:\n    \"\"\"\n    Recursively parses a bracketed tree represented as a list of tokens and returns \n    the set of constituent spans along with the updated index and the current word position.\n    \n    Args:\n        tokens: List of tokens (strings) from the linearized tree.\n        index: Current index in the tokens list.\n        word_index: Current word position (integer).\n    \n    Returns:\n        A tuple (spans, new_index, new_word_index) where:\n            spans: Set of tuples indicating constituent spans (start, end).\n            new_index: Updated index after parsing this node.\n            new_word_index: Updated word position after processing subtree.\n    \"\"\"\n    spans: Set[Tuple[int, int]] = set()\n    assert tokens[index] == '(', \"Expected '(' at token index {}.\".format(index)\n    index += 1  # Skip '('\n    # Skip the label token.\n    label = tokens[index]\n    index += 1\n    start = word_index\n    # Process children until encountering ')'\n    while tokens[index] != ')':\n        if tokens[index] == '(':\n            child_spans, index, word_index = _parse_tree(tokens, index, word_index)\n            spans.update(child_spans)\n        else:\n            # Leaf word encountered.\n            word_index += 1\n            index += 1\n    # Current node span from start to current word_index\n    end = word_index\n    spans.add((start, end))\n    index += 1  # Skip ')'\n    return spans, index, word_index\n\ndef tree_to_spans(tree_str: str) -> Set[Tuple[int, int]]:\n    \"\"\"\n    Converts a bracketed tree string into a set of constituent spans.\n    A span is defined as a tuple (start, end) with start inclusive and end exclusive.\n    \n    Args:\n        tree_str: String representing the linearized parse tree in bracketed notation.\n    \n    Returns:\n        A set of tuples representing the spans in the tree.\n    \"\"\"\n    tokens = tree_str.replace('(', ' ( ').replace(')', ' ) ').split()\n    spans, _, _ = _parse_tree(tokens, 0, 0)\n    return spans\n\ndef compute_parsing_f1(gold_trees: List[str], pred_trees: List[str]) -> Dict[str, float]:\n    \"\"\"\n    Computes F1 score (with precision and recall) for constituency parsing by comparing \n    the spans extracted from the gold-standard and predicted bracketed tree strings.\n    \n    Args:\n        gold_trees: List of gold parse trees as bracketed strings.\n        pred_trees: List of predicted parse trees as bracketed strings.\n    \n    Returns:\n        A dictionary with keys \"F1\", \"precision\", and \"recall\".\n    \"\"\"\n    total_gold = 0\n    total_pred = 0\n    total_overlap = 0\n    \n    for gold_tree, pred_tree in zip(gold_trees, pred_trees):\n        gold_spans = tree_to_spans(gold_tree)\n        pred_spans = tree_to_spans(pred_tree)\n        total_gold += len(gold_spans)\n        total_pred += len(pred_spans)\n        total_overlap += len(gold_spans.intersection(pred_spans))\n    \n    precision = total_overlap / total_pred if total_pred > 0 else 0.0\n    recall = total_overlap / total_gold if total_gold > 0 else 0.0\n    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n    return {\"F1\": f1, \"precision\": precision, \"recall\": recall}\n\ndef linearization_to_tree(linear_str: str) -> str:\n    \"\"\"\n    Converts a linearized tree string into a bracketed tree string.\n    Placeholder implementation: This function currently assumes the linearized representation\n    is already in bracketed form. Update this function if a different conversion strategy is needed.\n    \n    Args:\n        linear_str: Linearized string representation of the parse tree.\n    \n    Returns:\n        A bracketed tree string.\n    \"\"\"\n    return linear_str  # Identity transformation (placeholder)\n\n# ---------------------------------------------------------------------------\n# Evaluation Class\n# ---------------------------------------------------------------------------\n\nclass Evaluation:\n    \"\"\"\n    Evaluation class for assessing Transformer model performance. It supports\n    evaluation for both machine translation (BLEU score) and constituency parsing (F1 score).\n\n    Attributes:\n        model: Trained TransformerModel instance.\n        test_data: Test dataset (list of batch dictionaries).\n        config: Configuration parameters loaded from config.yaml.\n        task: Task type (\"translation\" or \"parsing\").\n        tokenizer: Tokenizer instance for decoding token IDs into text.\n        device: Torch device.\n    \"\"\"\n\n    def __init__(self, model: TransformerModel, test_data: List[Dict[str, Any]], config: Any, task: str = \"translation\") -> None:\n        \"\"\"\n        Initializes the Evaluation instance.\n        \n        Args:\n            model: Trained TransformerModel instance.\n            test_data: Test dataset (list of batches). For translation, each batch should have keys \"src\" and \"tgt\".\n                       For parsing, each batch should have key \"inputs\".\n            config: Central configuration object loaded from config.yaml.\n            task: Task type; either \"translation\" or \"parsing\". Defaults to \"translation\".\n        \"\"\"\n        self.model = model\n        self.test_data = test_data\n        self.config = config\n        self.task = task.lower()\n        self.model.eval()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n        # Load tokenizer based on task settings.\n        if self.task == \"translation\":\n            tokenizer_type = self.config.data.wmt_2014_en_de.tokenizer\n        else:\n            tokenizer_type = \"BPE\"\n        self.tokenizer = load_tokenizer(tokenizer_type)\n        LOGGER.info(f\"Evaluation initialized for task '{self.task}' using device {self.device}.\")\n\n    def evaluate_mt(self) -> Dict[str, Any]:\n        \"\"\"\n        Evaluates the Transformer model on a machine translation test dataset.\n        Uses beam search decoding with parameters:\n          - beam_size from config.inference.beam_size (default 4)\n          - length_penalty from config.inference.length_penalty (default 0.6)\n          - max_output_length set to (input_length + config.inference.max_output_length_offset, default offset 50)\n        \n        Returns:\n            A dictionary with the final BLEU score and additional statistics.\n        \"\"\"\n        # Retrieve inference parameters from configuration.\n        beam_size: int = self.config.inference.beam_size\n        length_penalty: float = self.config.inference.length_penalty\n        max_offset: int = self.config.inference.max_output_length_offset\n\n        all_references: List[str] = []\n        all_hypotheses: List[str] = []\n        total_sentences: int = 0\n\n        LOGGER.info(\"Starting machine translation evaluation...\")\n        for batch in tqdm(self.test_data, desc=\"Evaluating MT\", unit=\"batch\"):\n            # Expect batch to have keys \"src\" and \"tgt\", each a list of token ID lists.\n            src_batch: List[List[int]] = batch.get(\"src\", [])\n            tgt_batch: List[List[int]] = batch.get(\"tgt\", [])\n            for src_tokens, tgt_tokens in zip(src_batch, tgt_batch):\n                # Prepare source tensor (batch size = 1).\n                src_tensor = torch.tensor(src_tokens, dtype=torch.long, device=self.device).unsqueeze(0)\n                # Determine maximum output length.\n                max_output_length = src_tensor.size(1) + max_offset\n\n                # Perform beam search decoding.\n                pred_token_ids = _beam_search_decode(\n                    self.model,\n                    src_tensor,\n                    self.device,\n                    beam_size,\n                    length_penalty,\n                    max_output_length\n                )\n                # Decode predicted and reference tokens to strings.\n                hypothesis: str = decode_tokens(pred_token_ids, self.tokenizer)\n                reference: str = decode_tokens(tgt_tokens, self.tokenizer)\n\n                all_hypotheses.append(hypothesis)\n                all_references.append(reference)\n                total_sentences += 1\n\n        bleu_score: float = compute_corpus_bleu(all_references, all_hypotheses)\n        LOGGER.info(f\"Machine Translation Evaluation: {total_sentences} sentences evaluated, BLEU = {bleu_score:.4f}\")\n        return {\"BLEU\": bleu_score, \"num_sentences\": total_sentences}\n\n    def evaluate_parsing(self) -> Dict[str, Any]:\n        \"\"\"\n        Evaluates the Transformer model on a constituency parsing test dataset.\n        Uses beam search decoding with parsing-specific parameters:\n          - beam_size from config.parsing.inference.beam_size (default 21)\n          - length_penalty from config.parsing.inference.length_penalty (default 0.3)\n          - max_output_length set to (input_length + config.parsing.inference.max_output_length_offset, default offset 300)\n        \n        After decoding, the linearized outputs are converted to bracketed tree strings and compared\n        to the gold-standard trees to compute F1, precision, and recall.\n        \n        Returns:\n            A dictionary with F1, precision, recall, and sentence count.\n        \"\"\"\n        # Retrieve parsing inference parameters.\n        beam_size: int = self.config.parsing.inference.beam_size\n        length_penalty: float = self.config.parsing.inference.length_penalty\n        max_offset: int = self.config.parsing.inference.max_output_length_offset\n\n        gold_trees: List[str] = []\n        pred_trees: List[str] = []\n        total_sentences: int = 0\n\n        LOGGER.info(\"Starting constituency parsing evaluation...\")\n        for batch in tqdm(self.test_data, desc=\"Evaluating Parsing\", unit=\"batch\"):\n            # Expect batch to have key \"inputs\": list of token ID lists.\n            inputs_batch: List[List[int]] = batch.get(\"inputs\", [])\n            for input_tokens in inputs_batch:\n                # For parsing, the same sequence is used as gold reference.\n                # In practice, the gold tree string should be available.\n                gold_linear: str = decode_tokens(input_tokens, self.tokenizer)\n                gold_tree: str = linearization_to_tree(gold_linear)\n\n                # Prepare source tensor (batch size = 1).\n                src_tensor = torch.tensor(input_tokens, dtype=torch.long, device=self.device).unsqueeze(0)\n                max_output_length = src_tensor.size(1) + max_offset\n\n                # Beam search decoding.\n                pred_token_ids = _beam_search_decode(\n                    self.model,\n                    src_tensor,\n                    self.device,\n                    beam_size,\n                    length_penalty,\n                    max_output_length\n                )\n                pred_linear: str = decode_tokens(pred_token_ids, self.tokenizer)\n                pred_tree: str = linearization_to_tree(pred_linear)\n\n                gold_trees.append(gold_tree)\n                pred_trees.append(pred_tree)\n                total_sentences += 1\n\n        parsing_metrics: Dict[str, float] = compute_parsing_f1(gold_trees, pred_trees)\n        LOGGER.info(\n            f\"Parsing Evaluation: {total_sentences} sentences evaluated, F1 = {parsing_metrics['F1']:.4f}, \"\n            f\"Precision = {parsing_metrics['precision']:.4f}, Recall = {parsing_metrics['recall']:.4f}\"\n        )\n        result: Dict[str, Any] = {\n            \"F1\": parsing_metrics[\"F1\"],\n            \"precision\": parsing_metrics[\"precision\"],\n            \"recall\": parsing_metrics[\"recall\"],\n            \"num_sentences\": total_sentences,\n        }\n        return result\n\n```\n\n```python\n## File name: main.py\n\"\"\"main.py\n\nEntry point for the Transformer experiment workflow.\nThis file orchestrates configuration loading, data preparation, model initialization,\ntraining, and evaluation for both machine translation and constituency parsing tasks,\nfollowing the \"Attention Is All You Need\" paper's methodology and experimental setup.\n\nUsage:\n    python main.py --task [translation|parsing|both]\n\nAuthor: [Your Name]\nDate: [Current Date]\n\"\"\"\n\nimport argparse\nfrom copy import deepcopy\nfrom dataclasses import replace\nimport logging\n\n# Import centralized configuration and global instance.\nfrom config import CONFIG, Config\n\n# Import DatasetLoader, TransformerModel, Trainer, and Evaluation classes.\nfrom dataset_loader import DatasetLoader\nfrom model import TransformerModel\nfrom trainer import Trainer\nfrom evaluation import Evaluation\n\n# Import global logger from utils.\nfrom utils import LOGGER\n\n\ndef run_translation_experiment() -> None:\n    \"\"\"\n    Run the machine translation experiment using the transformer base model.\n    Loads WMT data (using the translation configuration), trains the model,\n    and evaluates performance using BLEU score.\n    \"\"\"\n    LOGGER.info(\"Starting Machine Translation experiment...\")\n\n    # Initialize DatasetLoader for translation.\n    translation_loader = DatasetLoader(CONFIG, task=\"translation\")\n    train_data, dev_data, test_data = translation_loader.load_data()\n    \n    # Instantiate the Transformer model using the configured (base) settings.\n    model = TransformerModel(CONFIG)\n    LOGGER.info(\"Transformer model for translation created.\")\n\n    # Initialize Trainer with model, training data and dev data.\n    trainer = Trainer(model=model, train_data=train_data, dev_data=dev_data, config=CONFIG)\n    LOGGER.info(\"Starting training for machine translation...\")\n    trainer.train()\n    \n    # After training, evaluate using the Evaluation module.\n    evaluator = Evaluation(model=model, test_data=test_data, config=CONFIG, task=\"translation\")\n    metrics = evaluator.evaluate_mt()\n    LOGGER.info(f\"Machine Translation Evaluation Metrics: {metrics}\")\n\n\ndef run_parsing_experiment() -> None:\n    \"\"\"\n    Run the constituency parsing experiment using a transformer configured for parsing.\n    Loads WSJ parsing data, trains the model with parsing-specific hyperparameters,\n    and evaluates performance using F1, precision, and recall metrics.\n    \"\"\"\n    LOGGER.info(\"Starting Constituency Parsing experiment...\")\n    \n    # Initialize DatasetLoader for parsing.\n    parsing_loader = DatasetLoader(CONFIG, task=\"parsing\")\n    train_data, dev_data, test_data = parsing_loader.load_data()\n    \n    # Since the global CONFIG is set for translation (transformer_base), create a new configuration\n    # for parsing by overriding the model configuration with parsing-specific settings.\n    new_model_config = replace(CONFIG.model, type=\"transformer_parsing\", d_model=CONFIG.parsing.model.d_model, num_layers=CONFIG.parsing.model.num_layers)\n    # Create a new Config instance for parsing experiments.\n    parsing_config = Config(\n        training=CONFIG.training,\n        model=new_model_config,\n        inference=CONFIG.inference,\n        parsing=CONFIG.parsing,\n        data=CONFIG.data,\n        hardware=CONFIG.hardware\n    )\n    LOGGER.info(\"Parsing configuration created with model type 'transformer_parsing'.\")\n\n    # Instantiate Transformer model with parsing configuration.\n    model = TransformerModel(parsing_config)\n    LOGGER.info(\"Transformer model for parsing created.\")\n\n    # Initialize Trainer for parsing.\n    trainer = Trainer(model=model, train_data=train_data, dev_data=dev_data, config=parsing_config)\n    LOGGER.info(\"Starting training for constituency parsing...\")\n    trainer.train()\n    \n    # Evaluate the parsing model.\n    evaluator = Evaluation(model=model, test_data=test_data, config=parsing_config, task=\"parsing\")\n    metrics = evaluator.evaluate_parsing()\n    LOGGER.info(f\"Constituency Parsing Evaluation Metrics: {metrics}\")\n\n\ndef main() -> None:\n    \"\"\"\n    Main function to run experiments. It parses command-line arguments to decide\n    which experiment to run: machine translation, constituency parsing, or both.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Transformer Experiment Runner\")\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        choices=[\"translation\", \"parsing\", \"both\"],\n        default=\"translation\",\n        help=\"Specify which experiment to run: translation, parsing, or both (default: translation).\"\n    )\n    args = parser.parse_args()\n\n    LOGGER.info(\"Experiment started with configuration from config.yaml\")\n    \n    if args.task == \"translation\":\n        run_translation_experiment()\n    elif args.task == \"parsing\":\n        run_parsing_experiment()\n    elif args.task == \"both\":\n        run_translation_experiment()\n        run_parsing_experiment()\n    else:\n        LOGGER.error(f\"Unsupported task type provided: {args.task}\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n```yaml\n## File name: config.yaml\n## config.yaml\ntraining:\n  total_steps: 100000        # For base model training (12 hours on 8 x P100 GPUs)\n  warmup_steps: 4000         # Warmup steps as described in the paper\n  optimizer: adam\n  adam:\n    beta1: 0.9\n    beta2: 0.98\n    epsilon: 1e-9\n  learning_rate_schedule: \"d_model^-0.5 * min(step^-0.5, step * warmup_steps^-1.5)\"\n  dropout_rate: 0.1          # Dropout rate applied to sub-layer outputs and embeddings\n  batch_tokens: 25000        # Approximate number of source tokens (and target tokens) per batch\n\nmodel:\n  type: transformer_base\n  d_model: 512               # Dimensionality of model embeddings (for base model)\n  num_layers: 6              # Number of encoder and decoder layers\n  d_ff: 2048                 # Dimensionality of the inner feed-forward layer\n  num_heads: 8               # Number of parallel attention heads\n  d_k: 64                    # Dimension per attention head (d_model / num_heads)\n  d_v: 64                    # Dimension per attention head (d_model / num_heads)\n  positional_encoding: sinusoidal  # Use sinusoidal positional encoding as described in the paper\n  share_embedding: true      # Share embedding weights with pre-softmax linear transformation\n\ninference:\n  beam_size: 4              # Beam search size for translation experiments\n  length_penalty: 0.6       # Length penalty for the beam search\n  max_output_length_offset: 50  # Maximum output length = input length + 50\n\nparsing:\n  model:\n    type: transformer_parsing\n    num_layers: 4                   # 4-layer Transformer for constituency parsing\n    d_model: 1024                   # Model dimensionality for parsing experiments\n  vocab_size:\n    wsj: 16000                    # Vocabulary size for WSJ-only training\n    semi_supervised: 32000        # Vocabulary size for semi-supervised training\n  inference:\n    beam_size: 21\n    length_penalty: 0.3\n    max_output_length_offset: 300     # Maximum output length = input length + 300\n\ndata:\n  wmt_2014_en_de:\n    dataset_size: \"4.5M sentence pairs\"\n    tokenizer: BPE\n    vocab_size: 37000\n  wmt_2014_en_fr:\n    dataset_size: \"36M sentence pairs\"\n    tokenizer: wordpiece\n    vocab_size: 32000\n  wsj:\n    dataset: \"Penn Treebank WSJ\"\n    training_size: \"40K sentences\"\n  semi_supervised_parsing:\n    dataset: \"BerkleyParser and high-confidence corpus\"\n    training_size: \"17M sentences\"\n\nhardware:\n  gpus: 8\n  gpu_type: \"NVIDIA P100\"\n  base_model_training_time: \"12 hours\"\n  big_model_training_time: \"3.5 days\"\n```\n\n\n\n---\n\nPlease provide critique of the target repository and a single numerical rating (1, 2, 3, 4, or 5) based on the quality of the sample, following the Example JSON format, without any additional commentary, formatting, or chattiness."}], "reasoning_effort": "high", "n": 8}, "completion_json": {"id": "chatcmpl-BUkmzSFd2xOGeXvKkHPofpo8H2w1U", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"mha.py\",\n            \"gold_func_name\": \"MultiHeadAttention.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"MultiHeadAttention.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"In the gold implementation, MultiHeadAttention returns both the output and the attention weights for further inspection, whereas the target repository returns only the output. This deviation is minor and does not impact the core training functionality, but may limit interpretability if attention weights are needed.\"\n        },\n        {\n            \"gold_file_name\": \"transformer.py\",\n            \"gold_func_name\": \"Transformer.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"TransformerModel.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target repository organizes the Transformer components (encoder, decoder, positional encoding, etc.) in a single module with an integrated training step and additional infrastructure (configuration, dataset loading, training, evaluation). Although structured differently from the gold repository, the implementation faithfully adheres to the paper's core concepts and methodologies.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 1, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"mha.py\",\n            \"gold_func_name\": \"forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"MultiHeadAttention.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"In the gold repository the MultiHeadAttention function returns both the transformed output and the attention weights, which facilitates inspection of the attention distribution. The target repository\u2019s version returns only the output, which is a minor deviation that does not affect the overall model correctness but removes one avenue for interpretability.\"\n        },\n        {\n            \"gold_file_name\": \"decoder.py\",\n            \"gold_func_name\": \"forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"DecoderLayer.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The gold repository separates the encoder and decoder implementations into distinct modules for clarity, while the target repository consolidates most components within a single model.py file. Although this consolidation is a design choice that adds extra training, evaluation, and configuration functionality, it slightly reduces modularity but does not compromise the core architectural implementation.\"\n        },\n        {\n            \"gold_file_name\": \"transformer.py\",\n            \"gold_func_name\": \"forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target repository faithfully implements the core Transformer components\u2014positional encoding, multi-head attention, feed-forward networks, residual connections, and layer normalization\u2014while also adding comprehensive modules for training, evaluation, and configuration management. This complete experimental pipeline extends the gold repository\u2019s scope and matches the paper\u2019s methodology without introducing logical errors.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 2, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"decoder.py\",\n            \"gold_func_name\": \"DecoderLayer.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"TransformerModel.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target repository\u2019s decoder construction and overall forward pass closely mirror the gold repository, incorporating all core sub-layers with residual connections and layer normalization. Additionally, it integrates extra features (such as label smoothing and beam search decoding) that extend the basic implementation without compromising correctness.\"\n        },\n        {\n            \"gold_file_name\": \"mha.py\",\n            \"gold_func_name\": \"MultiHeadAttention.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"MultiHeadAttention.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The implementation of multi-head scaled dot-product attention in the target repository is equivalent to the gold repository. It explicitly uses configuration parameters for d_k and d_v and applies dropout on the attention weights, ensuring adherence to the paper\u2019s design.\"\n        },\n        {\n            \"gold_file_name\": \"utils.py\",\n            \"gold_func_name\": \"PositionalEncoding.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"PositionalEncoding.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The sinusoidal positional encoding in the target repository is implemented correctly. It follows the same formulation as the gold repository while streamlining input scaling, thereby aligning well with the paper's methodology.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 3, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"mha.py\",\n            \"gold_func_name\": \"MultiHeadAttention\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"MultiHeadAttention\",\n            \"severity_level\": \"low\",\n            \"critique\": \"In the gold repository, the MultiHeadAttention module returns both the attention output and the attention weights, which can be useful for interpretability and debugging. In the target repository, the MultiHeadAttention implementation returns only the output, omitting explicit attention weights. This is a design choice that does not affect core functionality but slightly reduces transparency.\"\n        },\n        {\n            \"gold_file_name\": \"test_transformer.py\",\n            \"gold_func_name\": \"test_forward_pass\",\n            \"target_file_name\": \"main.py / overall project\",\n            \"target_func_name\": \"N/A (no dedicated unit test file)\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The gold repository includes a dedicated unit test (test_transformer.py) to verify core model components. The target repository, while providing a complete training and evaluation pipeline, lacks a similar dedicated unit test suite for validating the Transformer model\u2019s internal functions.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 4, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"mha.py\",\n            \"gold_func_name\": \"MultiHeadAttention.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"MultiHeadAttention.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target implementation of MultiHeadAttention follows the same linear projection and scaled dot\u2010product attention mechanism as the gold version, but it only returns the output tensor (omitting the attention weights). This is a minor design difference that does not impact core functionality.\"\n        },\n        {\n            \"gold_file_name\": \"decoder.py\",\n            \"gold_func_name\": \"DecoderLayer.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"DecoderLayer.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target repository correctly implements the decoder layer\u2019s structure (masked self-attention, encoder-decoder attention, and feed-forward network with residual connections and layer normalization), though it uses torch.nn.LayerNorm instead of a custom LayerNorm. This is a minor deviation without affecting correctness.\"\n        },\n        {\n            \"gold_file_name\": \"test_transformer.py\",\n            \"gold_func_name\": \"test_forward_pass\",\n            \"target_file_name\": \"dataset_loader.py\",\n            \"target_func_name\": \"load_data\",\n            \"severity_level\": \"low\",\n            \"critique\": \"Unlike the gold repository which includes a simple test, the target repository introduces a more extensive data handling pipeline (tokenization, batching, and file loading) that aligns with the paper\u2019s experimental procedure. This additional functionality is an extension rather than a deviation.\"\n        },\n        {\n            \"gold_file_name\": \"N/A\",\n            \"gold_func_name\": \"N/A\",\n            \"target_file_name\": \"config.py\",\n            \"target_func_name\": \"load_config / Config.from_dict\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target repository employs a comprehensive configuration system using dataclasses to centralize hyperparameters and experimental settings. This architecture is more advanced than the hard-coded parameters seen in the gold repository and benefits reproducibility without impacting the core algorithm.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 5, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"utils.py / mha.py\",\n            \"gold_func_name\": \"PositionalEncoding / MultiHeadAttention\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"PositionalEncoding / MultiHeadAttention\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target repository reimplements positional encoding and multi\u2010head attention in its model.py rather than separating them into dedicated utility modules as in the gold repository. This design choice does not affect correctness as the implementations mimic the gold approach.\"\n        },\n        {\n            \"gold_file_name\": \"decoder.py / encoder.py\",\n            \"gold_func_name\": \"DecoderLayer / EncoderLayer\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"DecoderLayer / EncoderLayer\",\n            \"severity_level\": \"low\",\n            \"critique\": \"Encoder and decoder layers in the target repository correctly incorporate residual connections, layer normalization, and feed\u2010forward sub-layers following the paper's architecture, even though they are consolidated in a single module rather than split into separate files.\"\n        },\n        {\n            \"gold_file_name\": \"Overall Transformer architecture\",\n            \"gold_func_name\": \"forward / train_step\",\n            \"target_file_name\": \"model.py / trainer.py\",\n            \"target_func_name\": \"forward / train_step\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target repository fully implements key components of the Transformer including learning rate scheduling, label smoothing loss, and beam search decoding. The additional modules for configuration, data loading, training, and evaluation enhance the workflow without deviating from the core methodology.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 6, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"mha.py\",\n            \"gold_func_name\": \"MultiHeadAttention.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"MultiHeadAttention.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target repository reimplements multi-head attention within its model.py module. While it integrates the functionality inline (instead of a separated module like in the gold repository), the logic and computations (linear projections, reshaping, scaled dot-product attention, and output projection) match the gold implementation.\"\n        },\n        {\n            \"gold_file_name\": \"decoder.py\",\n            \"gold_func_name\": \"Decoder.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"TransformerModel.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target repository\u2019s forward pass correctly embeds inputs, applies scaling with sqrt(d_model), adds positional encodings, and passes data through stacked encoder and decoder layers. Although the structure is integrated in a single TransformerModel class rather than separate encoder/decoder modules, it accurately implements the paper\u2019s methodology.\"\n        },\n        {\n            \"gold_file_name\": \"utils.py\",\n            \"gold_func_name\": \"create_look_ahead_mask\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"_generate_subsequent_mask\",\n            \"severity_level\": \"low\",\n            \"critique\": \"Both repositories implement a mechanism to create causal masks for the decoder (look-ahead mask). The target repository achieves this within its model class, ensuring that the auto-regressive property is preserved, which is consistent with the gold repository\u2019s approach.\"\n        },\n        {\n            \"gold_file_name\": \"transformer.py / test_transformer.py\",\n            \"gold_func_name\": \"Transformer forward and testing\",\n            \"target_file_name\": \"trainer.py & evaluation.py\",\n            \"target_func_name\": \"train_step and evaluation routines\",\n            \"severity_level\": \"low\",\n            \"critique\": \"The target repository expands on the gold by providing additional modules for configuration, data loading, training, and evaluation. This added infrastructure (including label smoothing in the train_step method and beam search decoding) does not deviate from the original paper\u2019s methodology and is implemented correctly.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}, {"finish_reason": "stop", "index": 7, "logprobs": null, "message": {"content": "{\n    \"critique_list\": [\n        {\n            \"gold_file_name\": \"decoder.py\",\n            \"gold_func_name\": \"DecoderLayer.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"DecoderLayer.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"In the gold repository the DecoderLayer returns attention weights alongside the decoder output. In the target repository, the corresponding decoder layer in model.py does not output these weights. Although this omission does not affect the core training or inference functionality, it may reduce transparency for debugging or interpretability.\"\n        },\n        {\n            \"gold_file_name\": \"mha.py\",\n            \"gold_func_name\": \"MultiHeadAttention.forward\",\n            \"target_file_name\": \"model.py\",\n            \"target_func_name\": \"MultiHeadAttention.forward\",\n            \"severity_level\": \"low\",\n            \"critique\": \"Both repositories implement multi-head attention with similar operations (linear projections, reshaping, scaling, and dropout). The target repository integrates this within a unified model file rather than splitting components into separate files. This design choice is acceptable and aligns with the paper\u2019s methodology.\"\n        }\n    ],\n    \"score\": 5\n}", "refusal": null, "role": "assistant", "annotations": [], "audio": null, "function_call": null, "tool_calls": null}}], "created": 1746669397, "model": "o3-mini-2025-01-31", "object": "chat.completion", "service_tier": "default", "system_fingerprint": "fp_8810992130", "usage": {"completion_tokens": 28368, "prompt_tokens": 35688, "total_tokens": 64056, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 25472, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}}}, "eval_result": {"score": 5.0, "valid_n": 8, "scroe_lst": [5, 5, 5, 5, 5, 5, 5, 5], "rationale_lst": ["[{\"gold_file_name\": \"mha.py\", \"gold_func_name\": \"MultiHeadAttention.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"MultiHeadAttention.forward\", \"severity_level\": \"low\", \"critique\": \"In the gold implementation, MultiHeadAttention returns both the output and the attention weights for further inspection, whereas the target repository returns only the output. This deviation is minor and does not impact the core training functionality, but may limit interpretability if attention weights are needed.\"}, {\"gold_file_name\": \"transformer.py\", \"gold_func_name\": \"Transformer.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"TransformerModel.forward\", \"severity_level\": \"low\", \"critique\": \"The target repository organizes the Transformer components (encoder, decoder, positional encoding, etc.) in a single module with an integrated training step and additional infrastructure (configuration, dataset loading, training, evaluation). Although structured differently from the gold repository, the implementation faithfully adheres to the paper's core concepts and methodologies.\"}]", "[{\"gold_file_name\": \"mha.py\", \"gold_func_name\": \"forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"MultiHeadAttention.forward\", \"severity_level\": \"low\", \"critique\": \"In the gold repository the MultiHeadAttention function returns both the transformed output and the attention weights, which facilitates inspection of the attention distribution. The target repository\\u2019s version returns only the output, which is a minor deviation that does not affect the overall model correctness but removes one avenue for interpretability.\"}, {\"gold_file_name\": \"decoder.py\", \"gold_func_name\": \"forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"DecoderLayer.forward\", \"severity_level\": \"low\", \"critique\": \"The gold repository separates the encoder and decoder implementations into distinct modules for clarity, while the target repository consolidates most components within a single model.py file. Although this consolidation is a design choice that adds extra training, evaluation, and configuration functionality, it slightly reduces modularity but does not compromise the core architectural implementation.\"}, {\"gold_file_name\": \"transformer.py\", \"gold_func_name\": \"forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"forward\", \"severity_level\": \"low\", \"critique\": \"The target repository faithfully implements the core Transformer components\\u2014positional encoding, multi-head attention, feed-forward networks, residual connections, and layer normalization\\u2014while also adding comprehensive modules for training, evaluation, and configuration management. This complete experimental pipeline extends the gold repository\\u2019s scope and matches the paper\\u2019s methodology without introducing logical errors.\"}]", "[{\"gold_file_name\": \"decoder.py\", \"gold_func_name\": \"DecoderLayer.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"TransformerModel.forward\", \"severity_level\": \"low\", \"critique\": \"The target repository\\u2019s decoder construction and overall forward pass closely mirror the gold repository, incorporating all core sub-layers with residual connections and layer normalization. Additionally, it integrates extra features (such as label smoothing and beam search decoding) that extend the basic implementation without compromising correctness.\"}, {\"gold_file_name\": \"mha.py\", \"gold_func_name\": \"MultiHeadAttention.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"MultiHeadAttention.forward\", \"severity_level\": \"low\", \"critique\": \"The implementation of multi-head scaled dot-product attention in the target repository is equivalent to the gold repository. It explicitly uses configuration parameters for d_k and d_v and applies dropout on the attention weights, ensuring adherence to the paper\\u2019s design.\"}, {\"gold_file_name\": \"utils.py\", \"gold_func_name\": \"PositionalEncoding.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"PositionalEncoding.forward\", \"severity_level\": \"low\", \"critique\": \"The sinusoidal positional encoding in the target repository is implemented correctly. It follows the same formulation as the gold repository while streamlining input scaling, thereby aligning well with the paper's methodology.\"}]", "[{\"gold_file_name\": \"mha.py\", \"gold_func_name\": \"MultiHeadAttention\", \"target_file_name\": \"model.py\", \"target_func_name\": \"MultiHeadAttention\", \"severity_level\": \"low\", \"critique\": \"In the gold repository, the MultiHeadAttention module returns both the attention output and the attention weights, which can be useful for interpretability and debugging. In the target repository, the MultiHeadAttention implementation returns only the output, omitting explicit attention weights. This is a design choice that does not affect core functionality but slightly reduces transparency.\"}, {\"gold_file_name\": \"test_transformer.py\", \"gold_func_name\": \"test_forward_pass\", \"target_file_name\": \"main.py / overall project\", \"target_func_name\": \"N/A (no dedicated unit test file)\", \"severity_level\": \"low\", \"critique\": \"The gold repository includes a dedicated unit test (test_transformer.py) to verify core model components. The target repository, while providing a complete training and evaluation pipeline, lacks a similar dedicated unit test suite for validating the Transformer model\\u2019s internal functions.\"}]", "[{\"gold_file_name\": \"mha.py\", \"gold_func_name\": \"MultiHeadAttention.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"MultiHeadAttention.forward\", \"severity_level\": \"low\", \"critique\": \"The target implementation of MultiHeadAttention follows the same linear projection and scaled dot\\u2010product attention mechanism as the gold version, but it only returns the output tensor (omitting the attention weights). This is a minor design difference that does not impact core functionality.\"}, {\"gold_file_name\": \"decoder.py\", \"gold_func_name\": \"DecoderLayer.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"DecoderLayer.forward\", \"severity_level\": \"low\", \"critique\": \"The target repository correctly implements the decoder layer\\u2019s structure (masked self-attention, encoder-decoder attention, and feed-forward network with residual connections and layer normalization), though it uses torch.nn.LayerNorm instead of a custom LayerNorm. This is a minor deviation without affecting correctness.\"}, {\"gold_file_name\": \"test_transformer.py\", \"gold_func_name\": \"test_forward_pass\", \"target_file_name\": \"dataset_loader.py\", \"target_func_name\": \"load_data\", \"severity_level\": \"low\", \"critique\": \"Unlike the gold repository which includes a simple test, the target repository introduces a more extensive data handling pipeline (tokenization, batching, and file loading) that aligns with the paper\\u2019s experimental procedure. This additional functionality is an extension rather than a deviation.\"}, {\"gold_file_name\": \"N/A\", \"gold_func_name\": \"N/A\", \"target_file_name\": \"config.py\", \"target_func_name\": \"load_config / Config.from_dict\", \"severity_level\": \"low\", \"critique\": \"The target repository employs a comprehensive configuration system using dataclasses to centralize hyperparameters and experimental settings. This architecture is more advanced than the hard-coded parameters seen in the gold repository and benefits reproducibility without impacting the core algorithm.\"}]", "[{\"gold_file_name\": \"utils.py / mha.py\", \"gold_func_name\": \"PositionalEncoding / MultiHeadAttention\", \"target_file_name\": \"model.py\", \"target_func_name\": \"PositionalEncoding / MultiHeadAttention\", \"severity_level\": \"low\", \"critique\": \"The target repository reimplements positional encoding and multi\\u2010head attention in its model.py rather than separating them into dedicated utility modules as in the gold repository. This design choice does not affect correctness as the implementations mimic the gold approach.\"}, {\"gold_file_name\": \"decoder.py / encoder.py\", \"gold_func_name\": \"DecoderLayer / EncoderLayer\", \"target_file_name\": \"model.py\", \"target_func_name\": \"DecoderLayer / EncoderLayer\", \"severity_level\": \"low\", \"critique\": \"Encoder and decoder layers in the target repository correctly incorporate residual connections, layer normalization, and feed\\u2010forward sub-layers following the paper's architecture, even though they are consolidated in a single module rather than split into separate files.\"}, {\"gold_file_name\": \"Overall Transformer architecture\", \"gold_func_name\": \"forward / train_step\", \"target_file_name\": \"model.py / trainer.py\", \"target_func_name\": \"forward / train_step\", \"severity_level\": \"low\", \"critique\": \"The target repository fully implements key components of the Transformer including learning rate scheduling, label smoothing loss, and beam search decoding. The additional modules for configuration, data loading, training, and evaluation enhance the workflow without deviating from the core methodology.\"}]", "[{\"gold_file_name\": \"mha.py\", \"gold_func_name\": \"MultiHeadAttention.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"MultiHeadAttention.forward\", \"severity_level\": \"low\", \"critique\": \"The target repository reimplements multi-head attention within its model.py module. While it integrates the functionality inline (instead of a separated module like in the gold repository), the logic and computations (linear projections, reshaping, scaled dot-product attention, and output projection) match the gold implementation.\"}, {\"gold_file_name\": \"decoder.py\", \"gold_func_name\": \"Decoder.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"TransformerModel.forward\", \"severity_level\": \"low\", \"critique\": \"The target repository\\u2019s forward pass correctly embeds inputs, applies scaling with sqrt(d_model), adds positional encodings, and passes data through stacked encoder and decoder layers. Although the structure is integrated in a single TransformerModel class rather than separate encoder/decoder modules, it accurately implements the paper\\u2019s methodology.\"}, {\"gold_file_name\": \"utils.py\", \"gold_func_name\": \"create_look_ahead_mask\", \"target_file_name\": \"model.py\", \"target_func_name\": \"_generate_subsequent_mask\", \"severity_level\": \"low\", \"critique\": \"Both repositories implement a mechanism to create causal masks for the decoder (look-ahead mask). The target repository achieves this within its model class, ensuring that the auto-regressive property is preserved, which is consistent with the gold repository\\u2019s approach.\"}, {\"gold_file_name\": \"transformer.py / test_transformer.py\", \"gold_func_name\": \"Transformer forward and testing\", \"target_file_name\": \"trainer.py & evaluation.py\", \"target_func_name\": \"train_step and evaluation routines\", \"severity_level\": \"low\", \"critique\": \"The target repository expands on the gold by providing additional modules for configuration, data loading, training, and evaluation. This added infrastructure (including label smoothing in the train_step method and beam search decoding) does not deviate from the original paper\\u2019s methodology and is implemented correctly.\"}]", "[{\"gold_file_name\": \"decoder.py\", \"gold_func_name\": \"DecoderLayer.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"DecoderLayer.forward\", \"severity_level\": \"low\", \"critique\": \"In the gold repository the DecoderLayer returns attention weights alongside the decoder output. In the target repository, the corresponding decoder layer in model.py does not output these weights. Although this omission does not affect the core training or inference functionality, it may reduce transparency for debugging or interpretability.\"}, {\"gold_file_name\": \"mha.py\", \"gold_func_name\": \"MultiHeadAttention.forward\", \"target_file_name\": \"model.py\", \"target_func_name\": \"MultiHeadAttention.forward\", \"severity_level\": \"low\", \"critique\": \"Both repositories implement multi-head attention with similar operations (linear projections, reshaping, scaling, and dropout). The target repository integrates this within a unified model file rather than splitting components into separate files. This design choice is acceptable and aligns with the paper\\u2019s methodology.\"}]"]}}